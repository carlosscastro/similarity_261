{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS - w261 Machine Learning At Scale\n",
    "__Course Lead:__ Dr James G. Shanahan (__email__ Jimi via  James.Shanahan _AT_ gmail.com)\n",
    "\n",
    "## Assignment - HW5\n",
    "\n",
    "\n",
    "---\n",
    "__Name:__  Carlos Castro   \n",
    "__Class:__ MIDS w261 (Section 2, e.g., Fall 2016 Group 1)     \n",
    "__Email:__  carlosscastro@iSchool.Berkeley.edu     \n",
    "__Week:__   5\n",
    "\n",
    "__Due Time:__ 2 Phases. \n",
    "\n",
    "* __HW5 Phase 1__ \n",
    "This can be done on a local machine (with a unit test on the cloud such as AltaScale's PaaS or on AWS) and is due Tuesday, Week 6 by 8AM (West coast time). It will primarily focus on building a unit/systems and for pairwise similarity calculations pipeline (for stripe documents)\n",
    "\n",
    "* __HW5 Phase 2__ \n",
    "This will require the AltaScale cluster and will be due Tuesday, Week 7 by 8AM (West coast time). \n",
    "The focus of  HW5 Phase 2  will be to scale up the unit/systems tests to the Google 5 gram corpus. This will be a group exercise \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.13 | packaged by conda-forge | (default, May  2 2017, 12:48:11) \\n[GCC 4.8.2 20140120 (Red Hat 4.8.2-15)]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   3 chqngh users  534624366 2017-06-18 10:25 hdfs:/user/chqngh/virtualenv/hw5_test2.zip\r\n",
      "-rw-r--r--   3 chqngh users     407514 2017-06-18 19:05 hdfs:/user/chqngh/virtualenv/hw5_test3.zip\r\n"
     ]
    }
   ],
   "source": [
    "# make sure to replace USERNAME and ENV with the same USERNAME and ENV you used to \n",
    "# create the virtualenv(in create_env.sh)\n",
    "USERNAME='chqngh'\n",
    "ENV='hw5_test2'\n",
    "pyBin ='./'+ENV+'/bin/python'\n",
    "pyArchive = \"hdfs:///user/\"+USERNAME+\"/virtualenv/\"+ENV+\".zip#\"+ENV\n",
    "!hdfs dfs -ls \"hdfs:///user/{USERNAME}/virtualenv/\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents <a name=\"TOC\"></a> \n",
    "\n",
    "1.  [HW Instructions](#1)   \n",
    "2.  [HW References](#2)\n",
    "3.  [HW Problems](#3)   \n",
    "       \n",
    "    5.4.  [HW5.4](#5.4)    \n",
    "    5.5.  [HW5.5](#5.5)    \n",
    "    5.6.  [HW5.6](#5.6)    \n",
    "    5.7.  [HW5.7](#5.7)    \n",
    "    5.8.  [HW5.8](#5.8)    \n",
    "    5.9.  [HW5.9](#5.9)    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "# 1 Instructions\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "MIDS UC Berkeley, Machine Learning at Scale   \n",
    "DATSCIW261 ASSIGNMENT #5\n",
    "\n",
    "Version 2017-9-2 \n",
    "\n",
    "\n",
    "### IMPORTANT\n",
    "\n",
    "This homework must be completed in the cloud \n",
    "\n",
    "### === INSTRUCTIONS for SUBMISSIONS ===   \n",
    "Follow the instructions for submissions carefully.\n",
    "\n",
    "Each student has a `HW-<user>` repository for all assignments.   \n",
    "\n",
    "Click this link to enable you to create a github repo within the MIDS261 Classroom:   \n",
    "https://classroom.github.com/assignment-invitations/3b1d6c8e58351209f9dd865537111ff8   \n",
    "and follow the instructions to create a HW repo.\n",
    "\n",
    "Push the following to your HW github repo into the master branch:\n",
    "* Your local HW5 directory. Your repo file structure should look like this:\n",
    "\n",
    "```\n",
    "HW-<user>\n",
    "    --HW3\n",
    "       |__MIDS-W261-HW-03-<Student_id>.ipynb\n",
    "       |__MIDS-W261-HW-03-<Student_id>.pdf\n",
    "       |__some other hw3 file\n",
    "    --HW4\n",
    "       |__MIDS-W261-HW-04-<Student_id>.ipynb\n",
    "       |__MIDS-W261-HW-04-<Student_id>.pdf\n",
    "       |__some other hw4 file\n",
    "    etc..\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\">\n",
    "# 2 Useful References\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "* See async and live lectures for this week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3\">\n",
    "# 3 HW Problems\n",
    "[Back to Table of Contents](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"5.4\"></a> \n",
    "# PHASE 2\n",
    "----------\n",
    "\n",
    "# HW 5.4   \n",
    "## Full-scale experiment on Google N-gram data on the CLOUD\n",
    "__ Once you are happy with your test results __ proceed to generating  your results on the Google n-grams dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.4.0  <a name=\"5.4.0\"></a> Run systems tests on the CLOUD  (PHASE 2)\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Repeat HW5.3.0 (using the same small data sources that were used in HW5.3.0) on ** the cloud** (e.g., AltaScale / AWS/ SoftLayer/ Azure). Make sure all tests give correct results! Good luck out there!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting buildStripes.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile buildStripes.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "import re\n",
    "import mrjob\n",
    "import json\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import itertools\n",
    "import collections\n",
    "# import logging\n",
    "# import time\n",
    "\n",
    "\n",
    "\n",
    "class MRbuildStripes(MRJob):\n",
    "  \n",
    "    #START SUDENT CODE531_STRIPES\n",
    "  \n",
    "    MRJob.SORT_VALUES = True\n",
    "    \n",
    "    #def mapper_init(self):\n",
    "    #    return self.start_time = time.time()\n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        splits = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "\n",
    "        words = splits[0].lower().split()\n",
    "        count = splits[1]\n",
    "\n",
    "        H = {}\n",
    "        for subset in itertools.combinations(sorted(set(words)), 2):\n",
    "            \n",
    "            # Process combinations in sorted order, i.e. \"hello\",\"tomorrow\"\n",
    "            if subset[0] not in H.keys():\n",
    "                H[subset[0]] = {}\n",
    "                H[subset[0]][subset[1]] = count \n",
    "            elif subset[1] not in H[subset[0]]:\n",
    "                H[subset[0]][subset[1]] = count\n",
    "            else:\n",
    "                H[subset[0]][subset[1]] += count\n",
    "\n",
    "            # Obtain combinations in reverse order, to consider them both ways\n",
    "            # TODO: Should refactor this and the block above, shameless copy-paste\n",
    "            if subset[1] not in H.keys():\n",
    "                H[subset[1]] = {}\n",
    "                H[subset[1]][subset[0]] = count \n",
    "            elif subset[0] not in H[subset[1]]:\n",
    "                H[subset[1]][subset[0]] = count\n",
    "            else:\n",
    "                H[subset[1]][subset[0]] += count\n",
    "        for key in H.keys():\n",
    "            #print \"%s\\t%s\" % (key, json.dumps(H[key]))\n",
    "            yield key, H[key]\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        \n",
    "        counter = {}\n",
    "\n",
    "        for value in values:\n",
    "            \n",
    "            for k, v in value.iteritems():\n",
    "                if k in counter:\n",
    "                    counter[k] += int(v)\n",
    "                else:\n",
    "                    counter[k] = int(v)\n",
    "        \n",
    "        yield key, counter\n",
    "        \n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "\n",
    "            MRStep(#mapper_init=self.mapper_init\n",
    "                   #,\n",
    "                   mapper=self.mapper\n",
    "                   ,\n",
    "                   reducer=self.reducer\n",
    "                  )\n",
    "            ]\n",
    "  #END SUDENT CODE531_STRIPES\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "#     start_time = time.time()\n",
    "    MRbuildStripes.run()\n",
    "#     elapsed_time = time.time() - start_time\n",
    "#     mins = elapsed_time/float(60)\n",
    "#     a = \"\"\"Elapsed time: %s seconds\n",
    "#     In minutes: %s mins\"\"\" % (str(elapsed_time), str(mins))\n",
    "#     logging.warning(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting invertedIndex.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile invertedIndex.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "from __future__ import division\n",
    "import collections\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import itertools\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import json\n",
    "# import logging\n",
    "# import time\n",
    "\n",
    "class MRinvertedIndex(MRJob):\n",
    "    \n",
    "    #START SUDENT CODE531_INV_INDEX\n",
    "  \n",
    "    def mapper(self, _, line):\n",
    "        key, stripeJson = line.strip().split('\\t')\n",
    "        key = key.strip(\"\\\"\")\n",
    "        stripe = json.loads(stripeJson)\n",
    "        \n",
    "        for k, v in stripe.iteritems():\n",
    "            yield k, [key, len(stripe)]\n",
    "        \n",
    "    def reducer(self, key, values):\n",
    "\n",
    "        table = {}\n",
    "        for value in values:\n",
    "            table[value[0]] = value[1]\n",
    "            \n",
    "        yield key, table\n",
    "        \n",
    "    #END SUDENT CODE531_INV_INDEX\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "#     start_time = time.time()\n",
    "    MRinvertedIndex.run() \n",
    "#     elapsed_time = time.time() - start_time\n",
    "#     mins = elapsed_time/float(60)\n",
    "#     a = \"\"\"Elapsed time: %s seconds\n",
    "#     In minutes: %s mins\"\"\" % (str(elapsed_time), str(mins))\n",
    "#     logging.warning(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting similarity.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile similarity.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "import collections\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "#import numpy as np\n",
    "import itertools\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import time\n",
    "import logging\n",
    "\n",
    "class MRsimilarity(MRJob):\n",
    "  \n",
    "    #START SUDENT CODE531_SIMILARITY\n",
    "\n",
    "    MRJob.SORT_VALUES = True\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        key, valuesJson = line.strip().split('\\t')\n",
    "        key = key.strip(\"\\\"\")\n",
    "        values = json.loads(valuesJson)\n",
    "\n",
    "        for pair in itertools.combinations(sorted(set(values)), 2):\n",
    "            yield pair, [values[pair[0]], values[pair[1]]]\n",
    "        \n",
    "    def reducer(self, key, values):\n",
    "        intersection = 0\n",
    "        count1 = None\n",
    "        count2 = None\n",
    "        \n",
    "        cosine = 0.0\n",
    "        \n",
    "        # Iterate through the values\n",
    "        for value in values:\n",
    "            # Jaccard, get counts for the intersection, and for each set\n",
    "            intersection += 1\n",
    "            if count1 == None:\n",
    "                count1 = value[0]\n",
    "                count2 = value[1]\n",
    "        \n",
    "            # Cosine\n",
    "            a = 1 / math.sqrt(value[0])\n",
    "            b = 1 / math.sqrt(value[1])\n",
    "            cosine += a * b\n",
    "            \n",
    "        jaccard = float(intersection) / float(count1 + count2 - intersection)\n",
    "        \n",
    "        overlap_coefficient = float(intersection) / min(count1, count2)\n",
    "        \n",
    "        dice_coefficient = float(2 * intersection) / (count1 + count2)\n",
    "        \n",
    "        average = (cosine + jaccard + overlap_coefficient + dice_coefficient) / 4.0\n",
    "        \n",
    "        yield average, [key[0] + ' - ' + key[1], cosine, jaccard, overlap_coefficient, dice_coefficient]\n",
    "            \n",
    "    #END SUDENT CODE531_SIMILARITY\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    MRsimilarity.run()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    mins = elapsed_time/float(60)\n",
    "    a = \"\"\"Elapsed time: %s seconds\n",
    "     In minutes: %s mins\"\"\" % (str(elapsed_time), str(mins))\n",
    "    logging.warning(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt\n",
    "A BILL FOR ESTABLISHING RELIGIOUS\t59\t59\t54\n",
    "A Biography of General George\t92\t90\t74\n",
    "A Case Study in Government\t102\t102\t78\n",
    "A Case Study of Female\t447\t447\t327\n",
    "A Case Study of Limited\t55\t55\t43\n",
    "A Child's Christmas in Wales\t1099\t1061\t866\n",
    "A Circumstantial Narrative of the\t62\t62\t50\n",
    "A City by the Sea\t62\t60\t49\n",
    "A Collection of Fairy Tales\t123\t117\t80\n",
    "A Collection of Forms of\t116\t103\t82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting atlas-boon-systems-test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile atlas-boon-systems-test.txt\n",
    "atlas boon\t50\t50\t50\n",
    "boon cava dipped\t10\t10\t10\n",
    "atlas dipped\t15\t15\t15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build stripes for mini-test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `systems_test_stripes_1': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/buildStripes.chqngh.20170619.033035.406183\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/chqngh/tmp/mrjob/buildStripes.chqngh.20170619.033035.406183/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob9103621092739473141.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1493936954640_1558\n",
      "  Submitted application application_1493936954640_1558\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1493936954640_1558/\n",
      "  Running job: job_1493936954640_1558\n",
      "  Job job_1493936954640_1558 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1493936954640_1558 completed successfully\n",
      "  Output directory: hdfs:///user/chqngh/tmp/mrjob/buildStripes.chqngh.20170619.033035.406183/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=563\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2118\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1042\n",
      "\t\tFILE: Number of bytes written=401082\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1011\n",
      "\t\tHDFS: Number of bytes written=2118\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=27789312\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=12193280\n",
      "\t\tTotal time spent by all map tasks (ms)=18092\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=54276\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4763\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=23815\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=18092\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=4763\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=3020\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=102\n",
      "\t\tInput split bytes=448\n",
      "\t\tMap input records=10\n",
      "\t\tMap output bytes=3024\n",
      "\t\tMap output materialized bytes=1047\n",
      "\t\tMap output records=49\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1891033088\n",
      "\t\tReduce input groups=49\n",
      "\t\tReduce input records=49\n",
      "\t\tReduce output records=28\n",
      "\t\tReduce shuffle bytes=1047\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=98\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7745478656\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/chqngh/tmp/mrjob/buildStripes.chqngh.20170619.033035.406183/output...\n",
      "Removing HDFS temp directory hdfs:///user/chqngh/tmp/mrjob/buildStripes.chqngh.20170619.033035.406183...\n",
      "Removing temp directory /tmp/buildStripes.chqngh.20170619.033035.406183...\n"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "# Make Stripes from ngrams for systems test 1\n",
    "###########################################################################\n",
    "\n",
    "\n",
    "!hdfs dfs -rm -r systems_test_stripes_1\n",
    "!python buildStripes.py -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > systems_test_stripes_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=2\n",
    "### Launched reduce tasks=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"a\"\t{\"limited\":55,\"female\":447,\"general\":92,\"sea\":62,\"in\":1201,\"religious\":59,\"george\":92,\"biography\":92,\"city\":62,\"for\":59,\"tales\":123,\"child's\":1099,\"forms\":116,\"wales\":1099,\"christmas\":1099,\"government\":102,\"collection\":239,\"by\":62,\"case\":604,\"circumstantial\":62,\"fairy\":123,\"of\":895,\"study\":604,\"bill\":59,\"establishing\":59,\"narrative\":62,\"the\":124}\r\n",
      "\"bill\"\t{\"a\":59,\"religious\":59,\"for\":59,\"establishing\":59}\r\n",
      "\"biography\"\t{\"a\":92,\"of\":92,\"george\":92,\"general\":92}\r\n",
      "\"by\"\t{\"a\":62,\"city\":62,\"the\":62,\"sea\":62}\r\n",
      "\"case\"\t{\"a\":604,\"limited\":55,\"government\":102,\"of\":502,\"study\":604,\"female\":447,\"in\":102}\r\n",
      "\"child's\"\t{\"a\":1099,\"wales\":1099,\"christmas\":1099,\"in\":1099}\r\n",
      "\"christmas\"\t{\"a\":1099,\"wales\":1099,\"in\":1099,\"child's\":1099}\r\n",
      "\"circumstantial\"\t{\"a\":62,\"of\":62,\"the\":62,\"narrative\":62}\r\n",
      "\"city\"\t{\"a\":62,\"the\":62,\"by\":62,\"sea\":62}\r\n",
      "\"collection\"\t{\"a\":239,\"forms\":116,\"fairy\":123,\"tales\":123,\"of\":239}\r\n",
      "\"establishing\"\t{\"a\":59,\"bill\":59,\"religious\":59,\"for\":59}\r\n",
      "\"fairy\"\t{\"a\":123,\"of\":123,\"tales\":123,\"collection\":123}\r\n",
      "\"female\"\t{\"a\":447,\"case\":447,\"study\":447,\"of\":447}\r\n",
      "\"for\"\t{\"a\":59,\"bill\":59,\"religious\":59,\"establishing\":59}\r\n",
      "\"forms\"\t{\"a\":116,\"of\":116,\"collection\":116}\r\n",
      "\"general\"\t{\"a\":92,\"of\":92,\"george\":92,\"biography\":92}\r\n",
      "\"george\"\t{\"a\":92,\"of\":92,\"biography\":92,\"general\":92}\r\n",
      "\"government\"\t{\"a\":102,\"case\":102,\"study\":102,\"in\":102}\r\n",
      "\"in\"\t{\"a\":1201,\"case\":102,\"government\":102,\"study\":102,\"child's\":1099,\"wales\":1099,\"christmas\":1099}\r\n",
      "\"limited\"\t{\"a\":55,\"case\":55,\"study\":55,\"of\":55}\r\n",
      "\"narrative\"\t{\"a\":62,\"of\":62,\"the\":62,\"circumstantial\":62}\r\n",
      "\"of\"\t{\"a\":895,\"case\":502,\"circumstantial\":62,\"george\":92,\"limited\":55,\"tales\":123,\"collection\":239,\"the\":62,\"forms\":116,\"female\":447,\"narrative\":62,\"fairy\":123,\"general\":92,\"study\":502,\"biography\":92}\r\n",
      "\"religious\"\t{\"a\":59,\"bill\":59,\"for\":59,\"establishing\":59}\r\n",
      "\"sea\"\t{\"a\":62,\"city\":62,\"the\":62,\"by\":62}\r\n",
      "\"study\"\t{\"a\":604,\"case\":604,\"limited\":55,\"government\":102,\"of\":502,\"female\":447,\"in\":102}\r\n",
      "\"tales\"\t{\"a\":123,\"of\":123,\"fairy\":123,\"collection\":123}\r\n",
      "\"the\"\t{\"a\":124,\"city\":62,\"circumstantial\":62,\"of\":62,\"sea\":62,\"narrative\":62,\"by\":62}\r\n",
      "\"wales\"\t{\"a\":1099,\"in\":1099,\"christmas\":1099,\"child's\":1099}\r\n"
     ]
    }
   ],
   "source": [
    "!cat systems_test_stripes_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `systems_test_stripes_2': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/buildStripes.chqngh.20170619.034715.706358\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/chqngh/tmp/mrjob/buildStripes.chqngh.20170619.034715.706358/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob7826431321457810332.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1493936954640_1570\n",
      "  Submitted application application_1493936954640_1570\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1493936954640_1570/\n",
      "  Running job: job_1493936954640_1570\n",
      "  Job job_1493936954640_1570 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "  Task Id : attempt_1493936954640_1570_m_000000_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@518fd5d8 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1570_m_000001_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@518fd5d8 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1570_m_000000_1, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@569b5010 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "   map 100% reduce 0%\n",
      "  Task Id : attempt_1493936954640_1570_r_000000_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@ed75561 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1570_r_000000_1, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@214a66f5 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1570_r_000000_2, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@13e37288 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "   map 100% reduce 100%\n",
      "  Job job_1493936954640_1570 completed successfully\n",
      "  Output directory: hdfs:///user/chqngh/tmp/mrjob/buildStripes.chqngh.20170619.034715.706358/output\n",
      "Counters: 52\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=101\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=147\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=137\n",
      "\t\tFILE: Number of bytes written=399190\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=475\n",
      "\t\tHDFS: Number of bytes written=147\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tFailed map tasks=3\n",
      "\t\tFailed reduce tasks=3\n",
      "\t\tLaunched map tasks=5\n",
      "\t\tLaunched reduce tasks=4\n",
      "\t\tOther local map tasks=3\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=25457664\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=24358400\n",
      "\t\tTotal time spent by all map tasks (ms)=16574\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=49722\n",
      "\t\tTotal time spent by all reduce tasks (ms)=9515\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=47575\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=16574\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=9515\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2680\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=105\n",
      "\t\tInput split bytes=374\n",
      "\t\tMap input records=3\n",
      "\t\tMap output bytes=204\n",
      "\t\tMap output materialized bytes=171\n",
      "\t\tMap output records=7\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1909981184\n",
      "\t\tReduce input groups=7\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=4\n",
      "\t\tReduce shuffle bytes=171\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=14\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7777882112\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/chqngh/tmp/mrjob/buildStripes.chqngh.20170619.034715.706358/output...\n",
      "Removing HDFS temp directory hdfs:///user/chqngh/tmp/mrjob/buildStripes.chqngh.20170619.034715.706358...\n",
      "Removing temp directory /tmp/buildStripes.chqngh.20170619.034715.706358...\n"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "# Make Stripes from ngrams for systems test 2\n",
    "###########################################################################\n",
    "\n",
    "!hdfs dfs -rm -r systems_test_stripes_2\n",
    "!python buildStripes.py -r hadoop atlas-boon-systems-test.txt \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > systems_test_stripes_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=2\n",
    "### Launched reduce tasks=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"atlas\"\t{\"dipped\":15,\"boon\":50}\r\n",
      "\"boon\"\t{\"atlas\":50,\"dipped\":10,\"cava\":10}\r\n",
      "\"cava\"\t{\"dipped\":10,\"boon\":10}\r\n",
      "\"dipped\"\t{\"atlas\":15,\"boon\":10,\"cava\":10}\r\n"
     ]
    }
   ],
   "source": [
    "!cat systems_test_stripes_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"DocA\"\t{\"X\":20, \"Y\":30, \"Z\":5}\r\n",
      "\"DocB\"\t{\"X\":100, \"Y\":20}\r\n",
      "\"DocC\"\t{\"M\":5, \"N\":20, \"Z\":5, \"Y\":1}\r\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "# Stripes for systems test 3 (given, no need to build stripes)\n",
    "########################################################################\n",
    "\n",
    "with open(\"systems_test_stripes_3\", \"w\") as f:\n",
    "    f.writelines([\n",
    "        '\"DocA\"\\t{\"X\":20, \"Y\":30, \"Z\":5}\\n',\n",
    "        '\"DocB\"\\t{\"X\":100, \"Y\":20}\\n',  \n",
    "        '\"DocC\"\\t{\"M\":5, \"N\":20, \"Z\":5, \"Y\":1}\\n'\n",
    "    ])\n",
    "!cat systems_test_stripes_3  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted indices for mini-test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/invertedIndex.chqngh.20170619.033235.396306\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/chqngh/tmp/mrjob/invertedIndex.chqngh.20170619.033235.396306/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob8640351491209051597.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1493936954640_1560\n",
      "  Submitted application application_1493936954640_1560\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1493936954640_1560/\n",
      "  Running job: job_1493936954640_1560\n",
      "  Job job_1493936954640_1560 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "  Task Id : attempt_1493936954640_1560_r_000000_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@15ede67 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "   map 100% reduce 100%\n",
      "  Job job_1493936954640_1560 completed successfully\n",
      "  Output directory: hdfs:///user/chqngh/tmp/mrjob/invertedIndex.chqngh.20170619.033235.396306/output\n",
      "Counters: 51\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3177\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1904\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1386\n",
      "\t\tFILE: Number of bytes written=400439\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3543\n",
      "\t\tHDFS: Number of bytes written=1904\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tFailed reduce tasks=1\n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=13899264\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=27816960\n",
      "\t\tTotal time spent by all map tasks (ms)=9049\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=27147\n",
      "\t\tTotal time spent by all reduce tasks (ms)=10866\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=54330\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=9049\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=10866\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2910\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=139\n",
      "\t\tInput split bytes=366\n",
      "\t\tMap input records=28\n",
      "\t\tMap output bytes=3150\n",
      "\t\tMap output materialized bytes=1581\n",
      "\t\tMap output records=158\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1908269056\n",
      "\t\tReduce input groups=28\n",
      "\t\tReduce input records=158\n",
      "\t\tReduce output records=28\n",
      "\t\tReduce shuffle bytes=1581\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=316\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7736860672\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/chqngh/tmp/mrjob/invertedIndex.chqngh.20170619.033235.396306/output...\n",
      "Removing HDFS temp directory hdfs:///user/chqngh/tmp/mrjob/invertedIndex.chqngh.20170619.033235.396306...\n",
      "Removing temp directory /tmp/invertedIndex.chqngh.20170619.033235.396306...\n"
     ]
    }
   ],
   "source": [
    "!python invertedIndex.py -r hadoop systems_test_stripes_1 \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > systems_test_index_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=2\n",
    "### Launched reduce tasks=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/invertedIndex.chqngh.20170619.034958.475352\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/chqngh/tmp/mrjob/invertedIndex.chqngh.20170619.034958.475352/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob6150870763797153562.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1493936954640_1572\n",
      "  Submitted application application_1493936954640_1572\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1493936954640_1572/\n",
      "  Running job: job_1493936954640_1572\n",
      "  Job job_1493936954640_1572 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1493936954640_1572 completed successfully\n",
      "  Output directory: hdfs:///user/chqngh/tmp/mrjob/invertedIndex.chqngh.20170619.034958.475352/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=221\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=137\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=141\n",
      "\t\tFILE: Number of bytes written=397787\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=587\n",
      "\t\tHDFS: Number of bytes written=137\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=13435392\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=22558720\n",
      "\t\tTotal time spent by all map tasks (ms)=8747\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=26241\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8812\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=44060\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=8747\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=8812\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2860\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=140\n",
      "\t\tInput split bytes=366\n",
      "\t\tMap input records=4\n",
      "\t\tMap output bytes=196\n",
      "\t\tMap output materialized bytes=183\n",
      "\t\tMap output records=10\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1901142016\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce input records=10\n",
      "\t\tReduce output records=4\n",
      "\t\tReduce shuffle bytes=183\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=20\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7735967744\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/chqngh/tmp/mrjob/invertedIndex.chqngh.20170619.034958.475352/output...\n",
      "Removing HDFS temp directory hdfs:///user/chqngh/tmp/mrjob/invertedIndex.chqngh.20170619.034958.475352...\n",
      "Removing temp directory /tmp/invertedIndex.chqngh.20170619.034958.475352...\n"
     ]
    }
   ],
   "source": [
    "!python invertedIndex.py -r hadoop systems_test_stripes_2 \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > systems_test_index_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"atlas\"\t{\"dipped\":3,\"boon\":3}\r\n",
      "\"boon\"\t{\"atlas\":2,\"dipped\":3,\"cava\":2}\r\n",
      "\"cava\"\t{\"dipped\":3,\"boon\":3}\r\n",
      "\"dipped\"\t{\"atlas\":2,\"boon\":3,\"cava\":2}\r\n"
     ]
    }
   ],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=2\n",
    "### Launched reduce tasks=1\n",
    "!cat systems_test_index_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/invertedIndex.chqngh.20170619.034858.322120\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/chqngh/tmp/mrjob/invertedIndex.chqngh.20170619.034858.322120/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob5260813806124014654.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1493936954640_1571\n",
      "  Submitted application application_1493936954640_1571\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1493936954640_1571/\n",
      "  Running job: job_1493936954640_1571\n",
      "  Job job_1493936954640_1571 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "  Task Id : attempt_1493936954640_1571_r_000000_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@26103129 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "   map 100% reduce 100%\n",
      "  Job job_1493936954640_1571 completed successfully\n",
      "  Output directory: hdfs:///user/chqngh/tmp/mrjob/invertedIndex.chqngh.20170619.034858.322120/output\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=140\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=111\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=92\n",
      "\t\tFILE: Number of bytes written=397689\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=506\n",
      "\t\tHDFS: Number of bytes written=111\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tFailed reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=13301760\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=14105600\n",
      "\t\tTotal time spent by all map tasks (ms)=8660\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=25980\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5510\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=27550\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=8660\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=5510\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2430\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=101\n",
      "\t\tInput split bytes=366\n",
      "\t\tMap input records=3\n",
      "\t\tMap output bytes=135\n",
      "\t\tMap output materialized bytes=125\n",
      "\t\tMap output records=9\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1907417088\n",
      "\t\tReduce input groups=5\n",
      "\t\tReduce input records=9\n",
      "\t\tReduce output records=5\n",
      "\t\tReduce shuffle bytes=125\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=18\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7753158656\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/chqngh/tmp/mrjob/invertedIndex.chqngh.20170619.034858.322120/output...\n",
      "Removing HDFS temp directory hdfs:///user/chqngh/tmp/mrjob/invertedIndex.chqngh.20170619.034858.322120...\n",
      "Removing temp directory /tmp/invertedIndex.chqngh.20170619.034858.322120...\n"
     ]
    }
   ],
   "source": [
    "!python invertedIndex.py -r hadoop systems_test_stripes_3 \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > systems_test_index_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=2\n",
    "### Launched reduce tasks=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Systems test  1  - Inverted Index\n",
      "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "             \"a\" |          bill 4 |     biography 4 |            by 4\n",
      "          \"bill\" |            a 27 |  establishing 4 |           for 4\n",
      "     \"biography\" |            a 27 |       general 4 |        george 4\n",
      "            \"by\" |            a 27 |          city 4 |           sea 4\n",
      "          \"case\" |            a 27 |        female 4 |    government 4\n",
      "       \"child's\" |            a 27 |     christmas 4 |            in 7\n",
      "     \"christmas\" |            a 27 |       child's 4 |            in 7\n",
      "\"circumstantial\" |            a 27 |     narrative 4 |           of 15\n",
      "          \"city\" |            a 27 |            by 4 |           sea 4\n",
      "    \"collection\" |            a 27 |         fairy 4 |         forms 3\n",
      "  \"establishing\" |            a 27 |          bill 4 |           for 4\n",
      "         \"fairy\" |            a 27 |    collection 5 |           of 15\n",
      "        \"female\" |            a 27 |          case 7 |           of 15\n",
      "           \"for\" |            a 27 |          bill 4 |  establishing 4\n",
      "         \"forms\" |            a 27 |    collection 5 |           of 15\n",
      "       \"general\" |            a 27 |     biography 4 |        george 4\n",
      "        \"george\" |            a 27 |     biography 4 |       general 4\n",
      "    \"government\" |            a 27 |          case 7 |            in 7\n",
      "            \"in\" |            a 27 |          case 7 |       child's 4\n",
      "       \"limited\" |            a 27 |          case 7 |           of 15\n",
      "     \"narrative\" |            a 27 |circumstantial 4 |           of 15\n",
      "            \"of\" |            a 27 |     biography 4 |          case 7\n",
      "     \"religious\" |            a 27 |          bill 4 |  establishing 4\n",
      "           \"sea\" |            a 27 |            by 4 |          city 4\n",
      "         \"study\" |            a 27 |          case 7 |        female 4\n",
      "         \"tales\" |            a 27 |    collection 5 |         fairy 4\n",
      "           \"the\" |            a 27 |            by 4 |circumstantial 4\n",
      "         \"wales\" |            a 27 |       child's 4 |     christmas 4\n",
      "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Systems test  2  - Inverted Index\n",
      "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "         \"atlas\" |          boon 3 |        dipped 3 |                \n",
      "          \"boon\" |         atlas 2 |          cava 2 |        dipped 3\n",
      "          \"cava\" |          boon 3 |        dipped 3 |                \n",
      "        \"dipped\" |         atlas 2 |          boon 3 |          cava 2\n",
      "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Systems test  3  - Inverted Index\n",
      "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "             \"M\" |          DocC 4 |                 |                \n",
      "             \"N\" |          DocC 4 |                 |                \n",
      "             \"X\" |          DocA 3 |          DocB 2 |                \n",
      "             \"Y\" |          DocA 3 |          DocB 2 |          DocC 4\n",
      "             \"Z\" |          DocA 3 |          DocC 4 |                \n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "# Pretty print systems tests for generating Inverted Index\n",
    "##########################################################\n",
    "import json\n",
    "\n",
    "for i in range(1,4):\n",
    "    print \"—\"*100\n",
    "    print \"Systems test \",i,\" - Inverted Index\"\n",
    "    print \"—\"*100  \n",
    "    with open(\"systems_test_index_\"+str(i),\"r\") as f:\n",
    "        lines = sorted(f.readlines())\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            word, doc_list = line.split(\"\\t\")\n",
    "            doc_dict = json.loads(doc_list)\n",
    "            stripe=[]\n",
    "            for doc in doc_dict:\n",
    "                stripe.append([doc, doc_dict[doc]])\n",
    "            stripe=sorted(stripe)\n",
    "            stripe.extend([[\"\",\"\"] for _ in xrange(3 - len(stripe))])\n",
    "\n",
    "            print \"{0:>16} |{1:>16} |{2:>16} |{3:>16}\".format(\n",
    "              (word), stripe[0][0]+\" \"+str(stripe[0][1]), stripe[1][0]+\" \"+str(stripe[1][1]), stripe[2][0]+\" \"+str(stripe[2][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarities for mini-test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/similarity.chqngh.20170619.035618.066151\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/chqngh/tmp/mrjob/similarity.chqngh.20170619.035618.066151/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob2573380214472060003.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1493936954640_1576\n",
      "  Submitted application application_1493936954640_1576\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1493936954640_1576/\n",
      "  Running job: job_1493936954640_1576\n",
      "  Job job_1493936954640_1576 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1493936954640_1576 completed successfully\n",
      "  Output directory: hdfs:///user/chqngh/tmp/mrjob/similarity.chqngh.20170619.035618.066151/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2856\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=25050\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=3654\n",
      "\t\tFILE: Number of bytes written=407250\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3210\n",
      "\t\tHDFS: Number of bytes written=25050\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=12530688\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=12032000\n",
      "\t\tTotal time spent by all map tasks (ms)=8158\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=24474\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4700\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=23500\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=8158\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=4700\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2790\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=125\n",
      "\t\tInput split bytes=354\n",
      "\t\tMap input records=28\n",
      "\t\tMap output bytes=17893\n",
      "\t\tMap output materialized bytes=4825\n",
      "\t\tMap output records=673\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1895108608\n",
      "\t\tReduce input groups=378\n",
      "\t\tReduce input records=673\n",
      "\t\tReduce output records=378\n",
      "\t\tReduce shuffle bytes=4825\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=1346\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7730302976\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/chqngh/tmp/mrjob/similarity.chqngh.20170619.035618.066151/output...\n",
      "Removing HDFS temp directory hdfs:///user/chqngh/tmp/mrjob/similarity.chqngh.20170619.035618.066151...\n",
      "Removing temp directory /tmp/similarity.chqngh.20170619.035618.066151...\n",
      "WARNING:root:Elapsed time: 57.1936540604 seconds\n",
      "     In minutes: 0.953227567673 mins\n"
     ]
    }
   ],
   "source": [
    "!python similarity.py -r hadoop systems_test_index_1 \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > systems_test_similarities_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3348415947\t[\"a - bill\",0.2886751346,0.1071428571,0.75,0.1935483871]\r\n",
      "0.3348415947\t[\"a - biography\",0.2886751346,0.1071428571,0.75,0.1935483871]\r\n",
      "0.3348415947\t[\"a - by\",0.2886751346,0.1071428571,0.75,0.1935483871]\r\n",
      "0.4652013821\t[\"a - case\",0.4364357805,0.2142857143,0.8571428571,0.3529411765]\r\n",
      "0.3348415947\t[\"a - child's\",0.2886751346,0.1071428571,0.75,0.1935483871]\r\n"
     ]
    }
   ],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=2\n",
    "### Launched reduce tasks=1\n",
    "!head -5 systems_test_similarities_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/similarity.chqngh.20170619.035227.108392\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/chqngh/tmp/mrjob/similarity.chqngh.20170619.035227.108392/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob8088695988843366310.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1493936954640_1574\n",
      "  Submitted application application_1493936954640_1574\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1493936954640_1574/\n",
      "  Running job: job_1493936954640_1574\n",
      "  Job job_1493936954640_1574 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "  Task Id : attempt_1493936954640_1574_r_000000_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@5acc43c rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "   map 100% reduce 100%\n",
      "  Job job_1493936954640_1574 completed successfully\n",
      "  Output directory: hdfs:///user/chqngh/tmp/mrjob/similarity.chqngh.20170619.035227.108392/output\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=206\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=330\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=122\n",
      "\t\tFILE: Number of bytes written=399073\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=560\n",
      "\t\tHDFS: Number of bytes written=330\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tFailed reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=12593664\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=14632960\n",
      "\t\tTotal time spent by all map tasks (ms)=8199\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=24597\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5716\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=28580\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=8199\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=5716\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2840\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=136\n",
      "\t\tInput split bytes=354\n",
      "\t\tMap input records=4\n",
      "\t\tMap output bytes=196\n",
      "\t\tMap output materialized bytes=180\n",
      "\t\tMap output records=8\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1896341504\n",
      "\t\tReduce input groups=6\n",
      "\t\tReduce input records=8\n",
      "\t\tReduce output records=6\n",
      "\t\tReduce shuffle bytes=180\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=16\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7746990080\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/chqngh/tmp/mrjob/similarity.chqngh.20170619.035227.108392/output...\n",
      "Removing HDFS temp directory hdfs:///user/chqngh/tmp/mrjob/similarity.chqngh.20170619.035227.108392...\n",
      "Removing temp directory /tmp/similarity.chqngh.20170619.035227.108392...\n",
      "WARNING:root:Elapsed time: 60.754365921 seconds\n",
      "     In minutes: 1.01257276535 mins\n"
     ]
    }
   ],
   "source": [
    "!python similarity.py -r hadoop systems_test_index_2 \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > systems_test_similarities_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3895620726\t[\"atlas - boon\",0.4082482905,0.25,0.5,0.4]\r\n",
      "1.0\t[\"atlas - cava\",1.0,1.0,1.0,1.0]\r\n",
      "0.3895620726\t[\"atlas - dipped\",0.4082482905,0.25,0.5,0.4]\r\n",
      "0.3895620726\t[\"boon - cava\",0.4082482905,0.25,0.5,0.4]\r\n",
      "0.625\t[\"boon - dipped\",0.6666666667,0.5,0.6666666667,0.6666666667]\r\n"
     ]
    }
   ],
   "source": [
    "!head -5 systems_test_similarities_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/similarity.chqngh.20170619.035328.211544\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/chqngh/tmp/mrjob/similarity.chqngh.20170619.035328.211544/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob653050329358594709.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1493936954640_1575\n",
      "  Submitted application application_1493936954640_1575\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1493936954640_1575/\n",
      "  Running job: job_1493936954640_1575\n",
      "  Job job_1493936954640_1575 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "  Task Id : attempt_1493936954640_1575_r_000000_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@67baf89d rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "   map 100% reduce 100%\n",
      "  Job job_1493936954640_1575 completed successfully\n",
      "  Output directory: hdfs:///user/chqngh/tmp/mrjob/similarity.chqngh.20170619.035328.211544/output\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=167\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=197\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=74\n",
      "\t\tFILE: Number of bytes written=398948\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=521\n",
      "\t\tHDFS: Number of bytes written=197\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tFailed reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=12331008\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=14359040\n",
      "\t\tTotal time spent by all map tasks (ms)=8028\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=24084\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5609\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=28045\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=8028\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=5609\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2650\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=91\n",
      "\t\tInput split bytes=354\n",
      "\t\tMap input records=5\n",
      "\t\tMap output bytes=115\n",
      "\t\tMap output materialized bytes=106\n",
      "\t\tMap output records=5\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1917251584\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce input records=5\n",
      "\t\tReduce output records=3\n",
      "\t\tReduce shuffle bytes=106\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=10\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7769083904\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/chqngh/tmp/mrjob/similarity.chqngh.20170619.035328.211544/output...\n",
      "Removing HDFS temp directory hdfs:///user/chqngh/tmp/mrjob/similarity.chqngh.20170619.035328.211544...\n",
      "Removing temp directory /tmp/similarity.chqngh.20170619.035328.211544...\n",
      "WARNING:root:Elapsed time: 59.2229130268 seconds\n",
      "     In minutes: 0.987048550447 mins\n"
     ]
    }
   ],
   "source": [
    "!python similarity.py -r hadoop systems_test_index_3 \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > systems_test_similarities_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8207908119\t[\"DocA - DocB\",0.8164965809,0.6666666667,1.0,0.8]\r\n",
      "0.5538613768\t[\"DocA - DocC\",0.5773502692,0.4,0.6666666667,0.5714285714]\r\n",
      "0.346721681\t[\"DocB - DocC\",0.3535533906,0.2,0.5,0.3333333333]\r\n"
     ]
    }
   ],
   "source": [
    "!head -5 systems_test_similarities_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=2\n",
    "### Launched reduce tasks=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Systems test  1  - Similarity measures\n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "        average |           pair |         cosine |        jaccard |        overlap |           dice\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "       0.334842 |       a - bill |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.334842 |  a - biography |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.334842 |         a - by |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.465201 |       a - case |       0.436436 |       0.214286 |       0.857143 |       0.352941\n",
      "       0.334842 |    a - child's |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.334842 |  a - christmas |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.334842 |a - circumstantial |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.334842 |       a - city |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.384281 | a - collection |       0.344265 |       0.142857 |       0.800000 |       0.250000\n",
      "       0.334842 |a - establishing |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.334842 |      a - fairy |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.334842 |     a - female |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.334842 |        a - for |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.273413 |      a - forms |       0.222222 |       0.071429 |       0.666667 |       0.133333\n",
      "       0.334842 |    a - general |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.334842 |     a - george |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.334842 | a - government |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.465201 |         a - in |       0.436436 |       0.214286 |       0.857143 |       0.352941\n",
      "       0.334842 |    a - limited |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.334842 |  a - narrative |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.698916 |         a - of |       0.695666 |       0.500000 |       0.933333 |       0.666667\n",
      "       0.334842 |  a - religious |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.334842 |        a - sea |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.465201 |      a - study |       0.436436 |       0.214286 |       0.857143 |       0.352941\n",
      "       0.334842 |      a - tales |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.465201 |        a - the |       0.436436 |       0.214286 |       0.857143 |       0.352941\n",
      "       0.334842 |      a - wales |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.223214 |bill - biography |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |      bill - by |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |    bill - case |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 | bill - child's |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |bill - christmas |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |bill - circumstantial |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |    bill - city |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.205207 |bill - collection |       0.223607 |       0.125000 |       0.250000 |       0.222222\n",
      "       0.712500 |bill - establishing |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.223214 |   bill - fairy |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |  bill - female |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.712500 |     bill - for |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.268597 |   bill - forms |       0.288675 |       0.166667 |       0.333333 |       0.285714\n",
      "       0.223214 | bill - general |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |  bill - george |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |bill - government |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |      bill - in |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 | bill - limited |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |bill - narrative |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.134980 |      bill - of |       0.129099 |       0.055556 |       0.250000 |       0.105263\n",
      "       0.712500 |bill - religious |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.223214 |     bill - sea |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |   bill - study |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |   bill - tales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |     bill - the |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |   bill - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 | biography - by |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.365956 |biography - case |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.223214 |biography - child's |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |biography - christmas |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.458333 |biography - circumstantial |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.223214 |biography - city |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.419343 |biography - collection |       0.447214 |       0.285714 |       0.500000 |       0.444444\n",
      "       0.223214 |biography - establishing |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.458333 |biography - fairy |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.458333 |biography - female |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.223214 |biography - for |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.553861 |biography - forms |       0.577350 |       0.400000 |       0.666667 |       0.571429\n",
      "       0.712500 |biography - general |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.712500 |biography - george |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.223214 |biography - government |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 | biography - in |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.458333 |biography - limited |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.458333 |biography - narrative |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.410147 | biography - of |       0.387298 |       0.187500 |       0.750000 |       0.315789\n",
      "       0.223214 |biography - religious |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |biography - sea |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.365956 |biography - study |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.458333 |biography - tales |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.365956 |biography - the |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.223214 |biography - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |      by - case |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |   by - child's |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 | by - christmas |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.458333 |by - circumstantial |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.712500 |      by - city |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.205207 |by - collection |       0.223607 |       0.125000 |       0.250000 |       0.222222\n",
      "       0.223214 |by - establishing |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |     by - fairy |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |    by - female |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |       by - for |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.268597 |     by - forms |       0.288675 |       0.166667 |       0.333333 |       0.285714\n",
      "       0.223214 |   by - general |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |    by - george |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |by - government |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |        by - in |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |   by - limited |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.458333 | by - narrative |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.271593 |        by - of |       0.258199 |       0.117647 |       0.500000 |       0.210526\n",
      "       0.223214 | by - religious |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.712500 |       by - sea |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.180200 |     by - study |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |     by - tales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.559350 |       by - the |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.223214 |     by - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.365956 | case - child's |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.365956 |case - christmas |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.365956 |case - circumstantial |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.180200 |    case - city |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.317849 |case - collection |       0.338062 |       0.200000 |       0.400000 |       0.333333\n",
      "       0.180200 |case - establishing |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.365956 |   case - fairy |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.559350 |  case - female |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.180200 |     case - for |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.438276 |   case - forms |       0.436436 |       0.250000 |       0.666667 |       0.400000\n",
      "       0.365956 | case - general |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.365956 |  case - george |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.559350 |case - government |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.389610 |      case - in |       0.428571 |       0.272727 |       0.428571 |       0.428571\n",
      "       0.559350 | case - limited |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.365956 |case - narrative |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.386912 |      case - of |       0.390360 |       0.222222 |       0.571429 |       0.363636\n",
      "       0.180200 |case - religious |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.180200 |     case - sea |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.830357 |   case - study |       0.857143 |       0.750000 |       0.857143 |       0.857143\n",
      "       0.365956 |   case - tales |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.255952 |     case - the |       0.285714 |       0.166667 |       0.285714 |       0.285714\n",
      "       0.365956 |   case - wales |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.712500 |child's - christmas |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.223214 |child's - circumstantial |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 | child's - city |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.205207 |child's - collection |       0.223607 |       0.125000 |       0.250000 |       0.222222\n",
      "       0.223214 |child's - establishing |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |child's - fairy |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |child's - female |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |  child's - for |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.268597 |child's - forms |       0.288675 |       0.166667 |       0.333333 |       0.285714\n",
      "       0.223214 |child's - general |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |child's - george |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.458333 |child's - government |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.559350 |   child's - in |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.223214 |child's - limited |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |child's - narrative |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.134980 |   child's - of |       0.129099 |       0.055556 |       0.250000 |       0.105263\n",
      "       0.223214 |child's - religious |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |  child's - sea |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.365956 |child's - study |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.223214 |child's - tales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |  child's - the |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.712500 |child's - wales |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.223214 |christmas - circumstantial |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |christmas - city |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.205207 |christmas - collection |       0.223607 |       0.125000 |       0.250000 |       0.222222\n",
      "       0.223214 |christmas - establishing |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |christmas - fairy |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |christmas - female |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |christmas - for |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.268597 |christmas - forms |       0.288675 |       0.166667 |       0.333333 |       0.285714\n",
      "       0.223214 |christmas - general |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |christmas - george |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.458333 |christmas - government |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.559350 | christmas - in |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.223214 |christmas - limited |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |christmas - narrative |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.134980 | christmas - of |       0.129099 |       0.055556 |       0.250000 |       0.105263\n",
      "       0.223214 |christmas - religious |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |christmas - sea |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.365956 |christmas - study |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.223214 |christmas - tales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |christmas - the |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.712500 |christmas - wales |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.458333 |circumstantial - city |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.419343 |circumstantial - collection |       0.447214 |       0.285714 |       0.500000 |       0.444444\n",
      "       0.223214 |circumstantial - establishing |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.458333 |circumstantial - fairy |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.458333 |circumstantial - female |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.223214 |circumstantial - for |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.553861 |circumstantial - forms |       0.577350 |       0.400000 |       0.666667 |       0.571429\n",
      "       0.458333 |circumstantial - general |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.458333 |circumstantial - george |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.223214 |circumstantial - government |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |circumstantial - in |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.458333 |circumstantial - limited |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.712500 |circumstantial - narrative |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.410147 |circumstantial - of |       0.387298 |       0.187500 |       0.750000 |       0.315789\n",
      "       0.223214 |circumstantial - religious |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.458333 |circumstantial - sea |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.365956 |circumstantial - study |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.458333 |circumstantial - tales |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.559350 |circumstantial - the |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.223214 |circumstantial - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.205207 |city - collection |       0.223607 |       0.125000 |       0.250000 |       0.222222\n",
      "       0.223214 |city - establishing |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |   city - fairy |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |  city - female |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |     city - for |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.268597 |   city - forms |       0.288675 |       0.166667 |       0.333333 |       0.285714\n",
      "       0.223214 | city - general |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |  city - george |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |city - government |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |      city - in |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 | city - limited |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.458333 |city - narrative |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.271593 |      city - of |       0.258199 |       0.117647 |       0.500000 |       0.210526\n",
      "       0.223214 |city - religious |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.712500 |     city - sea |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.180200 |   city - study |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |   city - tales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.559350 |     city - the |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.223214 |   city - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.205207 |collection - establishing |       0.223607 |       0.125000 |       0.250000 |       0.222222\n",
      "       0.646872 |collection - fairy |       0.670820 |       0.500000 |       0.750000 |       0.666667\n",
      "       0.419343 |collection - female |       0.447214 |       0.285714 |       0.500000 |       0.444444\n",
      "       0.205207 |collection - for |       0.223607 |       0.125000 |       0.250000 |       0.222222\n",
      "       0.504099 |collection - forms |       0.516398 |       0.333333 |       0.666667 |       0.500000\n",
      "       0.419343 |collection - general |       0.447214 |       0.285714 |       0.500000 |       0.444444\n",
      "       0.419343 |collection - george |       0.447214 |       0.285714 |       0.500000 |       0.444444\n",
      "       0.205207 |collection - government |       0.223607 |       0.125000 |       0.250000 |       0.222222\n",
      "       0.156652 |collection - in |       0.169031 |       0.090909 |       0.200000 |       0.166667\n",
      "       0.419343 |collection - limited |       0.447214 |       0.285714 |       0.500000 |       0.444444\n",
      "       0.419343 |collection - narrative |       0.447214 |       0.285714 |       0.500000 |       0.444444\n",
      "       0.477970 |collection - of |       0.461880 |       0.250000 |       0.800000 |       0.400000\n",
      "       0.205207 |collection - religious |       0.223607 |       0.125000 |       0.250000 |       0.222222\n",
      "       0.205207 |collection - sea |       0.223607 |       0.125000 |       0.250000 |       0.222222\n",
      "       0.317849 |collection - study |       0.338062 |       0.200000 |       0.400000 |       0.333333\n",
      "       0.646872 |collection - tales |       0.670820 |       0.500000 |       0.750000 |       0.666667\n",
      "       0.317849 |collection - the |       0.338062 |       0.200000 |       0.400000 |       0.333333\n",
      "       0.205207 |collection - wales |       0.223607 |       0.125000 |       0.250000 |       0.222222\n",
      "       0.223214 |establishing - fairy |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |establishing - female |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.712500 |establishing - for |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.268597 |establishing - forms |       0.288675 |       0.166667 |       0.333333 |       0.285714\n",
      "       0.223214 |establishing - general |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |establishing - george |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |establishing - government |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |establishing - in |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |establishing - limited |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |establishing - narrative |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.134980 |establishing - of |       0.129099 |       0.055556 |       0.250000 |       0.105263\n",
      "       0.712500 |establishing - religious |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.223214 |establishing - sea |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |establishing - study |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |establishing - tales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |establishing - the |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |establishing - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.458333 | fairy - female |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.223214 |    fairy - for |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.868292 |  fairy - forms |       0.866025 |       0.750000 |       1.000000 |       0.857143\n",
      "       0.458333 |fairy - general |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.458333 | fairy - george |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.223214 |fairy - government |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |     fairy - in |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.458333 |fairy - limited |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.458333 |fairy - narrative |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.410147 |     fairy - of |       0.387298 |       0.187500 |       0.750000 |       0.315789\n",
      "       0.223214 |fairy - religious |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |    fairy - sea |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.365956 |  fairy - study |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.712500 |  fairy - tales |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.365956 |    fairy - the |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.223214 |  fairy - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |   female - for |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.553861 | female - forms |       0.577350 |       0.400000 |       0.666667 |       0.571429\n",
      "       0.458333 |female - general |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.458333 |female - george |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.712500 |female - government |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.559350 |    female - in |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       1.000000 |female - limited |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "       0.458333 |female - narrative |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.410147 |    female - of |       0.387298 |       0.187500 |       0.750000 |       0.315789\n",
      "       0.223214 |female - religious |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |   female - sea |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.559350 | female - study |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.458333 | female - tales |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.365956 |   female - the |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.223214 | female - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.268597 |    for - forms |       0.288675 |       0.166667 |       0.333333 |       0.285714\n",
      "       0.223214 |  for - general |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |   for - george |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |for - government |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |       for - in |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |  for - limited |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |for - narrative |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.134980 |       for - of |       0.129099 |       0.055556 |       0.250000 |       0.105263\n",
      "       0.712500 |for - religious |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.223214 |      for - sea |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |    for - study |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |    for - tales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |      for - the |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |    for - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.553861 |forms - general |       0.577350 |       0.400000 |       0.666667 |       0.571429\n",
      "       0.553861 | forms - george |       0.577350 |       0.400000 |       0.666667 |       0.571429\n",
      "       0.268597 |forms - government |       0.288675 |       0.166667 |       0.333333 |       0.285714\n",
      "       0.215666 |     forms - in |       0.218218 |       0.111111 |       0.333333 |       0.200000\n",
      "       0.553861 |forms - limited |       0.577350 |       0.400000 |       0.666667 |       0.571429\n",
      "       0.553861 |forms - narrative |       0.577350 |       0.400000 |       0.666667 |       0.571429\n",
      "       0.328008 |     forms - of |       0.298142 |       0.125000 |       0.666667 |       0.222222\n",
      "       0.268597 |forms - religious |       0.288675 |       0.166667 |       0.333333 |       0.285714\n",
      "       0.268597 |    forms - sea |       0.288675 |       0.166667 |       0.333333 |       0.285714\n",
      "       0.438276 |  forms - study |       0.436436 |       0.250000 |       0.666667 |       0.400000\n",
      "       0.868292 |  forms - tales |       0.866025 |       0.750000 |       1.000000 |       0.857143\n",
      "       0.438276 |    forms - the |       0.436436 |       0.250000 |       0.666667 |       0.400000\n",
      "       0.268597 |  forms - wales |       0.288675 |       0.166667 |       0.333333 |       0.285714\n",
      "       0.712500 |general - george |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.223214 |general - government |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |   general - in |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.458333 |general - limited |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.458333 |general - narrative |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.410147 |   general - of |       0.387298 |       0.187500 |       0.750000 |       0.315789\n",
      "       0.223214 |general - religious |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |  general - sea |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.365956 |general - study |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.458333 |general - tales |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.365956 |  general - the |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.223214 |general - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |george - government |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |    george - in |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.458333 |george - limited |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.458333 |george - narrative |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.410147 |    george - of |       0.387298 |       0.187500 |       0.750000 |       0.315789\n",
      "       0.223214 |george - religious |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |   george - sea |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.365956 | george - study |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.458333 | george - tales |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.365956 |   george - the |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.223214 | george - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.559350 |government - in |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.712500 |government - limited |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.223214 |government - narrative |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.410147 |government - of |       0.387298 |       0.187500 |       0.750000 |       0.315789\n",
      "       0.223214 |government - religious |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |government - sea |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.559350 |government - study |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.223214 |government - tales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |government - the |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.458333 |government - wales |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.559350 |   in - limited |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.180200 | in - narrative |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.287991 |        in - of |       0.292770 |       0.157895 |       0.428571 |       0.272727\n",
      "       0.180200 | in - religious |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.180200 |       in - sea |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.389610 |     in - study |       0.428571 |       0.272727 |       0.428571 |       0.428571\n",
      "       0.180200 |     in - tales |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.126374 |       in - the |       0.142857 |       0.076923 |       0.142857 |       0.142857\n",
      "       0.559350 |     in - wales |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.458333 |limited - narrative |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.410147 |   limited - of |       0.387298 |       0.187500 |       0.750000 |       0.315789\n",
      "       0.223214 |limited - religious |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |  limited - sea |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.559350 |limited - study |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.458333 |limited - tales |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.365956 |  limited - the |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.223214 |limited - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.410147 | narrative - of |       0.387298 |       0.187500 |       0.750000 |       0.315789\n",
      "       0.223214 |narrative - religious |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.458333 |narrative - sea |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.365956 |narrative - study |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.458333 |narrative - tales |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.559350 |narrative - the |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.223214 |narrative - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.134980 | of - religious |       0.129099 |       0.055556 |       0.250000 |       0.105263\n",
      "       0.271593 |       of - sea |       0.258199 |       0.117647 |       0.500000 |       0.210526\n",
      "       0.386912 |     of - study |       0.390360 |       0.222222 |       0.571429 |       0.363636\n",
      "       0.410147 |     of - tales |       0.387298 |       0.187500 |       0.750000 |       0.315789\n",
      "       0.287991 |       of - the |       0.292770 |       0.157895 |       0.428571 |       0.272727\n",
      "       0.134980 |     of - wales |       0.129099 |       0.055556 |       0.250000 |       0.105263\n",
      "       0.223214 |religious - sea |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |religious - study |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |religious - tales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |religious - the |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |religious - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |    sea - study |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |    sea - tales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.559350 |      sea - the |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.223214 |    sea - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.365956 |  study - tales |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.255952 |    study - the |       0.285714 |       0.166667 |       0.285714 |       0.285714\n",
      "       0.365956 |  study - wales |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.365956 |    tales - the |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.223214 |  tales - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |    the - wales |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Systems test  2  - Similarity measures\n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "        average |           pair |         cosine |        jaccard |        overlap |           dice\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "       0.389562 |   atlas - boon |       0.408248 |       0.250000 |       0.500000 |       0.400000\n",
      "       1.000000 |   atlas - cava |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "       0.389562 | atlas - dipped |       0.408248 |       0.250000 |       0.500000 |       0.400000\n",
      "       0.389562 |    boon - cava |       0.408248 |       0.250000 |       0.500000 |       0.400000\n",
      "       0.625000 |  boon - dipped |       0.666667 |       0.500000 |       0.666667 |       0.666667\n",
      "       0.389562 |  cava - dipped |       0.408248 |       0.250000 |       0.500000 |       0.400000\n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Systems test  3  - Similarity measures\n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "        average |           pair |         cosine |        jaccard |        overlap |           dice\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "       0.820791 |    DocA - DocB |       0.816497 |       0.666667 |       1.000000 |       0.800000\n",
      "       0.553861 |    DocA - DocC |       0.577350 |       0.400000 |       0.666667 |       0.571429\n",
      "       0.346722 |    DocB - DocC |       0.353553 |       0.200000 |       0.500000 |       0.333333\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "# Pretty print systems tests\n",
    "############################################\n",
    "\n",
    "import json\n",
    "for i in range(1,4):\n",
    "  print '—'*110\n",
    "  print \"Systems test \",i,\" - Similarity measures\"\n",
    "  print '—'*110\n",
    "  print \"{0:>15} |{1:>15} |{2:>15} |{3:>15} |{4:>15} |{5:>15}\".format(\n",
    "          \"average\", \"pair\", \"cosine\", \"jaccard\", \"overlap\", \"dice\")\n",
    "  print '-'*110\n",
    "\n",
    "  with open(\"systems_test_similarities_\"+str(i),\"r\") as f:\n",
    "      lines = f.readlines()\n",
    "      for line in lines:\n",
    "          line = line.strip()\n",
    "          avg,stripe = line.split(\"\\t\")\n",
    "          stripe = json.loads(stripe)\n",
    "\n",
    "          print \"{0:>15f} |{1:>15} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "              float(avg), stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.4.1 <a name=\"5.4.1\"></a>Full-scale experiment: EDA of Google n-grams dataset (PHASE 2)\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Do some EDA on this dataset using mrjob, e.g., \n",
    "\n",
    "- A. Longest 5-gram (number of characters)\n",
    "- B. Top 10 most frequent words (please use the count information), i.e., unigrams\n",
    "- C. 20 Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency \n",
    "- D. Distribution of 5-gram sizes (character length).  E.g., count (using the count field) up how many times a 5-gram of 50 characters shows up. Plot the data graphically using a histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.4.1 - A. Longest 5-gram (number of characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting longest5gram.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile longest5gram.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import re\n",
    "\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import logging\n",
    "import time\n",
    "\n",
    "class longest5gram(MRJob):\n",
    "    \n",
    "    # START STUDENT CODE 5.4.1.A\n",
    "    \n",
    "    MRJob.SORT_VALUES = True\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super(longest5gram, self).__init__(args)\n",
    "        self.max_count = 0\n",
    "        self.max_grams = []\n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # Split line\n",
    "        splits = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "        words = splits[0].lower().split()\n",
    "        \n",
    "        char_count = 0\n",
    "        \n",
    "        # Count characters\n",
    "        for word in words:\n",
    "            char_count += len(word)\n",
    "        \n",
    "        # Optimization: we track the max count local to the current mapper instance. If records\n",
    "        # have higher count than the max, we output them and update the max. We don't yield\n",
    "        # records that are smaller than the local max. \n",
    "        # Even some non-max records are passed on, the good thing about this is that it is extremely memory efficient\n",
    "        # in that it uses constant memory.\n",
    "        if char_count > self.max_count:\n",
    "            self.max_count = char_count\n",
    "            yield (words), char_count\n",
    "        elif char_count == self.max_count:\n",
    "            yield (words), char_count\n",
    "            \n",
    "    \n",
    "    def combiner(self, ngram, char_counts):\n",
    "        current_max = max(char_counts)\n",
    "        \n",
    "        # Optimization: we track the max count local to the current combiner instance. If records\n",
    "        # have higher count than the max, we output them and update the max. We don't yield\n",
    "        # records that are smaller than the local max, drastically reducing work on shuffling and sorting\n",
    "        # Even some non-max or local max records are passed on, the good thing about this is that it is extremely \n",
    "        # memory efficient in that it uses constant memory (just 1 integer :)\n",
    "        if current_max > self.max_count:\n",
    "            self.max_count = current_max\n",
    "            yield ngram, current_max\n",
    "        elif current_max == self.max_count:\n",
    "            yield ngram, current_max\n",
    "    \n",
    "    def reducer(self, ngram, char_counts):\n",
    "            \n",
    "        current_count = max(char_counts)\n",
    "\n",
    "        # Track in max_grams the n-grams with the max count of words\n",
    "        if current_count > self.max_count:\n",
    "            self.max_count = current_count\n",
    "            self.max_grams = [(current_count, ngram)]\n",
    "        elif current_count == self.max_count:\n",
    "            self.max_grams.append((current_count, ngram))\n",
    "\n",
    "    def reducer_final(self):\n",
    "        # Once\n",
    "        for gram in self.max_grams:\n",
    "            yield gram[0], gram[1]\n",
    "\n",
    "\n",
    "    def steps(self):\n",
    "        \n",
    "        # We need 1 reducer for this approach. However the optimizations in the mappers and combiners\n",
    "        # help us ensure that a small percentage of records get to the reducer\n",
    "        custom_jobconf = {\n",
    "            'stream.num.map.output.key.fields':'1',\n",
    "            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapred.text.key.comparator.options': '-k2,2nr',\n",
    "            'mapred.reduce.tasks': '1'\n",
    "        }\n",
    "\n",
    "        return [\n",
    "                MRStep(jobconf=custom_jobconf,\n",
    "                    mapper=self.mapper,\n",
    "                    reducer=self.reducer,\n",
    "                    combiner = self.combiner,\n",
    "                    reducer_final = self.reducer_final\n",
    "                      )\n",
    "                 ]\n",
    "\n",
    "    # END STUDENT CODE 5.4.1.A\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    longest5gram.run()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    mins = elapsed_time/float(60)\n",
    "    a = \"\"\"Elapsed time: %s seconds\n",
    "    In minutes: %s mins\"\"\" % (str(elapsed_time), str(mins))\n",
    "    logging.warning(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On test data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `systems_test_stripes_5.4.1.a_1': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/longest5gram.chqngh.20170619.035732.257618\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/chqngh/tmp/mrjob/longest5gram.chqngh.20170619.035732.257618/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob1643608451332483976.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1493936954640_1577\n",
      "  Submitted application application_1493936954640_1577\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1493936954640_1577/\n",
      "  Running job: job_1493936954640_1577\n",
      "  Job job_1493936954640_1577 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "  Task Id : attempt_1493936954640_1577_m_000001_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1482a4f8 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1577_m_000000_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1482a4f8 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1577_m_000000_1, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4aaed6d2 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "   map 100% reduce 0%\n",
      "  Task Id : attempt_1493936954640_1577_r_000000_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@794211f1 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "   map 100% reduce 100%\n",
      "  Job job_1493936954640_1577 completed successfully\n",
      "  Output directory: hdfs:///user/chqngh/tmp/mrjob/longest5gram.chqngh.20170619.035732.257618/output\n",
      "Counters: 52\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=563\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=98\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=157\n",
      "\t\tFILE: Number of bytes written=401606\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1011\n",
      "\t\tHDFS: Number of bytes written=98\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tFailed map tasks=3\n",
      "\t\tFailed reduce tasks=1\n",
      "\t\tLaunched map tasks=5\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tOther local map tasks=3\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=28460544\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=14845440\n",
      "\t\tTotal time spent by all map tasks (ms)=18529\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=55587\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5799\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=28995\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=18529\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=5799\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=3120\n",
      "\t\tCombine input records=3\n",
      "\t\tCombine output records=3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=93\n",
      "\t\tInput split bytes=448\n",
      "\t\tMap input records=10\n",
      "\t\tMap output bytes=142\n",
      "\t\tMap output materialized bytes=173\n",
      "\t\tMap output records=3\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1902505984\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce input records=3\n",
      "\t\tReduce output records=2\n",
      "\t\tReduce shuffle bytes=173\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=6\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7735758848\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/chqngh/tmp/mrjob/longest5gram.chqngh.20170619.035732.257618/output...\n",
      "Removing HDFS temp directory hdfs:///user/chqngh/tmp/mrjob/longest5gram.chqngh.20170619.035732.257618...\n",
      "Removing temp directory /tmp/longest5gram.chqngh.20170619.035732.257618...\n",
      "WARNING:root:Elapsed time: 68.4803411961 seconds\n",
      "    In minutes: 1.14133901993 mins\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r systems_test_stripes_5.4.1.a_1\n",
    "!python longest5gram.py -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > systems_test_stripes_5.4.1.a_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=2\n",
    "### Launched reduce tasks=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\t[\"a\",\"bill\",\"for\",\"establishing\",\"religious\"]\r\n",
      "29\t[\"a\",\"circumstantial\",\"narrative\",\"of\",\"the\"]\r\n"
     ]
    }
   ],
   "source": [
    "!cat systems_test_stripes_5.4.1.a_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On full data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 190 items\r\n",
      "-rw-r--r--   3 chqngh users   11444614 2017-06-19 04:07 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-0-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users          0 2017-06-19 04:07 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-1-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11447003 2017-06-19 04:07 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-10-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11484723 2017-06-19 04:07 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-100-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11473190 2017-06-19 04:07 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-101-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11411047 2017-06-19 04:07 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-102-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11479296 2017-06-19 04:07 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-103-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11426686 2017-06-19 04:07 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-104-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11482267 2017-06-19 04:07 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-105-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11466886 2017-06-19 04:07 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-106-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11485960 2017-06-19 04:07 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-107-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11463278 2017-06-19 04:07 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-108-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11446599 2017-06-19 04:07 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-109-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11495017 2017-06-19 04:07 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-11-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11450101 2017-06-19 04:07 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-110-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11456942 2017-06-19 04:07 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-111-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11463851 2017-06-19 04:07 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-112-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11457743 2017-06-19 04:07 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-113-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11480874 2017-06-19 04:07 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-114-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11504547 2017-06-19 04:07 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-115-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11431759 2017-06-19 04:07 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-116-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11451033 2017-06-19 04:07 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-117-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11499348 2017-06-19 04:07 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-118-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11461816 2017-06-19 04:07 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-119-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11443168 2017-06-19 04:07 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-12-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11478681 2017-06-19 04:07 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-120-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11452388 2017-06-19 04:07 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-121-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11461706 2017-06-19 04:07 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-122-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11489973 2017-06-19 04:07 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-123-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11482331 2017-06-19 04:07 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-124-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11462905 2017-06-19 04:07 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-125-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11467767 2017-06-19 04:07 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-126-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11459598 2017-06-19 04:07 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-127-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11443935 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-128-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11433432 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-129-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11485635 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-13-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11457212 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-130-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11472377 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-131-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11493120 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-132-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11441679 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-133-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11470516 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-134-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11456972 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-135-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11492095 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-136-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11446669 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-137-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11466514 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-138-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11459726 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-139-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11442376 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-14-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11460720 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-140-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11483474 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-141-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11447339 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-142-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11526964 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-143-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11435580 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-144-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11471874 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-145-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11432836 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-146-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11459158 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-147-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11454244 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-148-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11451707 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-149-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11452631 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-15-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11473902 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-150-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11469708 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-151-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11455532 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-152-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11501176 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-153-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11515944 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-154-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11464571 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-155-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11470657 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-156-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11445685 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-157-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11472240 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-158-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11476035 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-159-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11525458 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-16-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11452574 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-160-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11464062 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-161-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11486898 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-162-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11477376 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-163-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11505357 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-164-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11462181 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-165-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11466783 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-166-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11462354 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-167-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11506473 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-168-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11472965 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-169-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11509921 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-17-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11469219 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-170-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11467382 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-171-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11415310 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-172-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11457481 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-173-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11496657 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-174-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11470283 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-175-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11441689 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-176-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11484462 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-177-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11494587 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-178-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11430020 2017-06-19 04:08 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-179-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11492843 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-18-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11464793 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-180-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11478471 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-181-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11505360 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-182-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11486325 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-183-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11452312 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-184-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11457259 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-185-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11471607 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-186-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11444970 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-187-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11456589 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-188-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11445694 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-189-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11446280 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-19-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11469040 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-2-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11483940 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-20-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11419380 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-21-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11504055 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-22-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11444833 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-23-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11470920 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-24-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11447510 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-25-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11451162 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-26-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11432861 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-27-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11473738 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-28-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11466285 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-29-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11473472 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-3-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11451427 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-30-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11445890 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-31-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11451034 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-32-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11448837 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-33-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11477127 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-34-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11482525 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-35-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11485001 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-36-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11462556 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-37-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11471356 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-38-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11463027 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-39-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11474462 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-4-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11479067 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-40-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11492731 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-41-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11444143 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-42-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11457891 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-43-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11477565 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-44-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11491534 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-45-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users          0 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-46-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11437493 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-47-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11486342 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-48-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11506157 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-49-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11486767 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-5-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11448291 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-50-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11466332 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-51-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11454748 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-52-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11473706 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-53-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11449750 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-54-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11440722 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-55-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11443428 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-56-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11523191 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-57-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11464003 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-58-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11450319 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-59-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11465378 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-6-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11477145 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-60-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11484900 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-61-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11461230 2017-06-19 04:09 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-62-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11479759 2017-06-19 04:10 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-63-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11445477 2017-06-19 04:10 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-64-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11519941 2017-06-19 04:10 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-65-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11455135 2017-06-19 04:10 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-66-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11463820 2017-06-19 04:10 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-67-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11502857 2017-06-19 04:10 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-68-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11510895 2017-06-19 04:10 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-69-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11459099 2017-06-19 04:10 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-7-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11431260 2017-06-19 04:10 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-70-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11465722 2017-06-19 04:10 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-71-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11495024 2017-06-19 04:10 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-72-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11474806 2017-06-19 04:10 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-73-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11491417 2017-06-19 04:10 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-74-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11484503 2017-06-19 04:10 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-75-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11458149 2017-06-19 04:10 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-76-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11473636 2017-06-19 04:10 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-77-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11479521 2017-06-19 04:10 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-78-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11430938 2017-06-19 04:10 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-79-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11455035 2017-06-19 04:10 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-8-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11464432 2017-06-19 04:10 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-80-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11473302 2017-06-19 04:10 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-81-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11457728 2017-06-19 04:10 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-82-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11510120 2017-06-19 04:10 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-83-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11459989 2017-06-19 04:10 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-84-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11487885 2017-06-19 04:10 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-85-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11492162 2017-06-19 04:10 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-86-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11487948 2017-06-19 04:10 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-87-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11452445 2017-06-19 04:10 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-88-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11507487 2017-06-19 04:10 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-89-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11450336 2017-06-19 04:10 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-9-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11525239 2017-06-19 04:10 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-90-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11468711 2017-06-19 04:10 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-91-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11458184 2017-06-19 04:10 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-92-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11474240 2017-06-19 04:10 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-93-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11471885 2017-06-19 04:10 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-94-filtered.txt\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   3 chqngh users   11488721 2017-06-19 04:10 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-95-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11483342 2017-06-19 04:10 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-96-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11451041 2017-06-19 04:10 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-97-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11464391 2017-06-19 04:10 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-98-filtered.txt\r\n",
      "-rw-r--r--   3 chqngh users   11479936 2017-06-19 04:10 hdfs:/user/chqngh/5grams/filtered-5Grams/googlebooks-eng-all-5gram-20090715-99-filtered.txt\r\n"
     ]
    }
   ],
   "source": [
    "# !hdfs dfs -mkdir hdfs:///user/chqngh/5grams\n",
    "# !hdfs dfs -cp hdfs:///user/cendylin/filtered-5Grams/ hdfs:///user/chqngh/5grams\n",
    "!hdfs dfs -ls hdfs:///user/chqngh/5grams/filtered-5Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `full_stripes_5.4.1.a': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Creating temp directory /tmp/longest5gram.chqngh.20170619.041610.313296\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/chqngh/tmp/mrjob/longest5gram.chqngh.20170619.041610.313296/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob6220024672428482964.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 190\n",
      "  number of splits:190\n",
      "  Submitting tokens for job: job_1493936954640_1581\n",
      "  Submitted application application_1493936954640_1581\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1493936954640_1581/\n",
      "  Running job: job_1493936954640_1581\n",
      "  Job job_1493936954640_1581 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "  Task Id : attempt_1493936954640_1581_m_000133_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@68b5eeb1 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000046_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@49d15392 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000139_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@68b5eeb1 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000045_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@49d15392 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000136_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@68b5eeb1 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000052_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@49d15392 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000044_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@49d15392 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000131_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@68b5eeb1 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000049_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@49d15392 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000134_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@68b5eeb1 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000135_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@68b5eeb1 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000132_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@68b5eeb1 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000137_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@68b5eeb1 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000140_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@48b767ba rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000142_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1707ab21 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000138_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@48b767ba rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000141_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1707ab21 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000143_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1707ab21 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000145_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1707ab21 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000148_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1707ab21 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000146_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1707ab21 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000155_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1707ab21 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000149_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1707ab21 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000147_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1707ab21 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000151_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1707ab21 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000153_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1707ab21 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000154_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1707ab21 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000152_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1707ab21 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000161_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@57cb83cf rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000162_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@57cb83cf rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000166_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@57cb83cf rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000150_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1707ab21 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000169_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@57cb83cf rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000165_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@57cb83cf rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000144_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1707ab21 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000156_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1707ab21 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000160_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@57cb83cf rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000167_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@57cb83cf rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000046_1, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@108674f7 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000133_1, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@108674f7 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000168_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@57cb83cf rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000139_1, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@2ce00bd8 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000052_1, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@f1c735d rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000045_1, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@2461a304 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000136_1, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@f1c735d rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000131_1, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@7ff05c79 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000064_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1b066d2a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000053_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@766feacc rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000047_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@766feacc rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Task Id : attempt_1493936954640_1581_m_000051_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1b066d2a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000050_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@766feacc rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000055_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1b066d2a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000048_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@766feacc rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000058_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1b066d2a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000060_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1b066d2a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000057_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1b066d2a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000061_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1b066d2a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000059_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1b066d2a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000056_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1b066d2a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000054_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1b066d2a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000069_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1b066d2a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000044_1, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@643a768a rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000072_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1b066d2a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000164_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1a6be3f8 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000076_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1b066d2a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000084_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1b066d2a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000070_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1b066d2a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000086_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1b066d2a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000080_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1b066d2a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000081_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1b066d2a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000079_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1b066d2a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000157_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1a6be3f8 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000062_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1b066d2a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000077_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1b066d2a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000075_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1b066d2a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000082_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1b066d2a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000074_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1b066d2a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000066_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1b066d2a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000063_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1b066d2a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1581_m_000065_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1b066d2a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Task Id : attempt_1493936954640_1581_m_000085_0, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1b066d2a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000071_0, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1b066d2a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000073_0, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1b066d2a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000087_0, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1b066d2a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000163_0, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1a6be3f8 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000068_0, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1b066d2a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000158_0, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1a6be3f8 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000083_0, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1b066d2a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000078_0, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1b066d2a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000049_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@5f764ef4 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000067_0, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1b066d2a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000159_0, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1a6be3f8 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000132_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@2b2c6a34 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000134_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@231d751c rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000141_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@10622159 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000135_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@40118c73 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "   map 2% reduce 0%\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000140_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@40856c77 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000137_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@307bbc78 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000138_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@10622159 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000148_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1ea89261 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000143_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@187f8393 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000142_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@187f8393 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000155_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@20c443ad rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000145_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@39e0b04b rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000146_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@39e0b04b rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000153_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6cbfae51 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000151_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6169081a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000147_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@464252d8 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000149_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@310daf05 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000161_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6221b2ab rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\r\n",
      "\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Task Id : attempt_1493936954640_1581_m_000154_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@42d8235d rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "   map 0% reduce 0%\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000162_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@2e3ccbde rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000166_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@29e93710 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000152_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@752df508 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000150_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@75f61daa rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000144_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@28da7b90 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000165_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@264e5c6e rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000169_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1b89b848 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000156_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@545aa9bb rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000046_2, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@ef8b454 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000160_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@16e0687c rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000168_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@14bcbf74 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000167_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@615ff0f5 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000133_2, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@5294e3c2 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000139_2, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@babbfcc rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000045_2, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@3f2a7108 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000052_2, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@c59c6cf rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "   map 1% reduce 0%\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000136_2, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4718a92d rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000053_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@5591956d rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000131_2, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@5285dc39 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000064_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6e8e1578 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000047_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@68966139 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000051_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6622ce55 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000048_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@7f4b3704 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\r\n",
      "\r\n",
      "   map 3% reduce 0%\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000060_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@316e20f4 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000058_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@316e20f4 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000057_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@ae533c8 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000050_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@ae533c8 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000055_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@ae533c8 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\r\n",
      "\r\n",
      "   map 4% reduce 0%\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000054_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@5d9ed111 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Task Id : attempt_1493936954640_1581_m_000061_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1c0844e2 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000059_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@b241a1d rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000069_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6083286b rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000056_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@b241a1d rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\r\n",
      "\r\n",
      "   map 5% reduce 0%\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000164_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4566b51e rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000076_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@5b06b5e0 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "   map 11% reduce 0%\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000072_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1bc0fdd3 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000044_2, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@65434dea rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "   map 13% reduce 0%\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000081_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@a9b890 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000080_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@35210b26 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\r\n",
      "\r\n",
      "   map 16% reduce 0%\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000070_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@62377afa rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000084_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6b7366b3 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000062_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@69f31f11 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "   map 19% reduce 0%\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000157_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6637b135 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\r\n",
      "\r\n",
      "   map 21% reduce 0%\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000065_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@7850780 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000074_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@7b5edf3a rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\r\n",
      "\r\n",
      "   map 23% reduce 0%\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000068_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@559e9c2f rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000083_1, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@5e2b5213 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000132_2, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@34a64880 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000148_2, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@5e00b82f rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000135_2, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@20225c14 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000151_2, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@486d4c04 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000146_2, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6c6940a8 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000152_2, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@50c04846 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000149_2, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4c746a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000162_2, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@21c31c7a rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\r\n",
      "\r\n",
      "   map 24% reduce 0%\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000166_2, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@2c3655f rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000165_2, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@2048d83f rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000144_2, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@2efbf0d1 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1581_m_000156_2, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@166fca9a rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\r\n",
      "\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   map 100% reduce 100%\n",
      "  Job job_1493936954640_1581 failed with state FAILED due to: Task failed task_1493936954640_1581_m_000046\n",
      "Job failed as tasks failed. failedMaps:1 failedReduces:0\n",
      "\n",
      "  Job not successful!\n",
      "  Streaming Command Failed!\n",
      "Counters: 40\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=504392531\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=5896832\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=504400167\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=132\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tFailed map tasks=171\n",
      "\t\tKilled map tasks=145\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=296\n",
      "\t\tOther local map tasks=126\n",
      "\t\tRack-local map tasks=170\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=7071203328\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=0\n",
      "\t\tTotal time spent by all map tasks (ms)=4603648\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=13810944\n",
      "\t\tTotal time spent by all reduce tasks (ms)=0\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=4603648\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=134800\n",
      "\t\tCombine input records=637\n",
      "\t\tCombine output records=258\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=7185\n",
      "\t\tInput split bytes=7636\n",
      "\t\tMap input records=13727878\n",
      "\t\tMap output bytes=38555\n",
      "\t\tMap output materialized bytes=15014\n",
      "\t\tMap output records=637\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tPhysical memory (bytes) snapshot=35668660224\n",
      "\t\tSpilled Records=258\n",
      "\t\tTotal committed heap usage (bytes)=68975329280\n",
      "\t\tVirtual memory (bytes) snapshot=96724676608\n",
      "Scanning logs for probable cause of failure...\n",
      "Probable cause of failure:\n",
      "\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@2c3655f rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "Step 1 of 1 failed: Command '['/opt/hadoop/bin/hadoop', 'jar', '/opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar', '-files', 'hdfs:///user/chqngh/tmp/mrjob/longest5gram.chqngh.20170619.041610.313296/files/longest5gram.py#longest5gram.py,hdfs:///user/chqngh/tmp/mrjob/longest5gram.chqngh.20170619.041610.313296/files/setup-wrapper.sh#setup-wrapper.sh', '-archives', 'hdfs:///user/chqngh/tmp/mrjob/longest5gram.chqngh.20170619.041610.313296/files/mrjob.tar.gz#mrjob.tar.gz,hdfs:///user/chqngh/virtualenv/hw5_test2.zip#hw5_test2', '-D', u'mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator', '-D', u'mapred.reduce.tasks=1', '-D', u'mapred.text.key.comparator.options=-k2,2nr', '-D', 'mapred.text.key.partitioner.options=-k1,1', '-D', u'mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator', '-D', u'mapreduce.job.reduces=1', '-D', u'mapreduce.partition.keycomparator.options=-k2,2nr', '-D', 'mapreduce.partition.keypartitioner.options=-k1,1', '-D', u'stream.num.map.output.key.fields=1', '-partitioner', 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner', '-input', 'hdfs:///user/chqngh/5grams/filtered-5Grams/', '-output', 'hdfs:///user/chqngh/tmp/mrjob/longest5gram.chqngh.20170619.041610.313296/output', '-mapper', 'sh -ex setup-wrapper.sh ./hw5_test2/bin/python longest5gram.py --step-num=0 --mapper', '-combiner', 'sh -ex setup-wrapper.sh ./hw5_test2/bin/python longest5gram.py --step-num=0 --combiner', '-reducer', 'sh -ex setup-wrapper.sh ./hw5_test2/bin/python longest5gram.py --step-num=0 --reducer']' returned non-zero exit status 256\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r full_stripes_5.4.1.a\n",
    "# !python longest5gram.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > full_stripes_5.4.1.a\n",
    "!python longest5gram.py -r hadoop hdfs:///user/chqngh/5grams/filtered-5Grams/ \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > full_stripes_5.4.1.a    \n",
    "    \n",
    "#     --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=2\n",
    "### Launched reduce tasks=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cat full_stripes_5.4.1.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report Stats: \n",
    "## example: \n",
    "## Longest 5grams MR stats\n",
    "\n",
    "    ec2_instance_type: m3.xlarge\n",
    "    num_ec2_instances: 15\n",
    "\n",
    "__Step 1:__  \n",
    "\n",
    "    RUNNING for 107.0s ~= 2 minutes  \n",
    "    Reduce tasks = 16 \n",
    "    \n",
    "__Step 2:__   \n",
    "\n",
    "    RUNNING for 108.8s ~= 2 minutes\n",
    "    Reduce tasks = 1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.4.1 - B. Top 10 most frequent words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mostFrequentWords.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mostFrequentWords.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import re\n",
    "\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import logging\n",
    "import time\n",
    "\n",
    "class mostFrequentWords(MRJob):\n",
    "\n",
    "    # START STUDENT CODE 5.4.1.B\n",
    "            \n",
    "    MRJob.SORT_VALUES = True\n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # Split line\n",
    "        splits = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "        words = splits[0].lower().split()\n",
    "        count = int(splits[1])\n",
    "        \n",
    "        for word in words:\n",
    "            yield word, count\n",
    "            \n",
    "    \n",
    "    def combiner(self, word, counts):\n",
    "        total = sum(count for count in counts)\n",
    "        yield word, total\n",
    "    \n",
    "    def reducer(self, word, counts):\n",
    "        total = sum(count for count in counts)\n",
    "        yield total, word\n",
    "    \n",
    "    def max_reducer(self, count, words):\n",
    "        for word in words:\n",
    "            yield word, count\n",
    "\n",
    "    def steps(self):\n",
    "        \n",
    "        custom_jobconf = {\n",
    "            'stream.num.map.output.key.fields':'2',\n",
    "            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapred.text.key.comparator.options': '-k1,1nr',\n",
    "            'mapred.reduce.tasks': '1'\n",
    "        }\n",
    "\n",
    "        return [\n",
    "                MRStep(mapper=self.mapper,\n",
    "                    reducer=self.reducer,\n",
    "                    combiner = self.combiner),\n",
    "                MRStep(jobconf=custom_jobconf,\n",
    "                       reducer=self.max_reducer)\n",
    "                 ]\n",
    "\n",
    "    # END STUDENT CODE 5.4.1.B\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    mostFrequentWords.run()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    mins = elapsed_time/float(60)\n",
    "    a = \"\"\"Elapsed time: %s seconds\n",
    "    In minutes: %s mins\"\"\" % (str(elapsed_time), str(mins))\n",
    "    logging.warning(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On the test data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `systems_test_stripes_5.4.1.b_1': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/mostFrequentWords.chqngh.20170618.191017.874889\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/chqngh/tmp/mrjob/mostFrequentWords.chqngh.20170618.191017.874889/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob2295853456664206911.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1493936954640_1449\n",
      "  Submitted application application_1493936954640_1449\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1493936954640_1449/\n",
      "  Running job: job_1493936954640_1449\n",
      "  Job job_1493936954640_1449 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "  Task Id : attempt_1493936954640_1449_m_000001_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@48143cee rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1449_m_000000_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@48143cee rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1493936954640_1449 completed successfully\n",
      "  Output directory: hdfs:///user/chqngh/tmp/mrjob/mostFrequentWords.chqngh.20170618.191017.874889/step-output/0000\n",
      "Counters: 51\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=563\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=357\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=430\n",
      "\t\tFILE: Number of bytes written=400151\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1021\n",
      "\t\tHDFS: Number of bytes written=357\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tFailed map tasks=2\n",
      "\t\tLaunched map tasks=4\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tOther local map tasks=2\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=20382720\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=19814400\n",
      "\t\tTotal time spent by all map tasks (ms)=13270\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=39810\n",
      "\t\tTotal time spent by all reduce tasks (ms)=7740\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=38700\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=13270\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=7740\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=3120\n",
      "\t\tCombine input records=50\n",
      "\t\tCombine output records=31\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=114\n",
      "\t\tInput split bytes=458\n",
      "\t\tMap input records=10\n",
      "\t\tMap output bytes=602\n",
      "\t\tMap output materialized bytes=458\n",
      "\t\tMap output records=50\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1892925440\n",
      "\t\tReduce input groups=28\n",
      "\t\tReduce input records=31\n",
      "\t\tReduce output records=28\n",
      "\t\tReduce shuffle bytes=458\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=62\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7722655744\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob1334858266337110976.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1493936954640_1451\n",
      "  Submitted application application_1493936954640_1451\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1493936954640_1451/\n",
      "  Running job: job_1493936954640_1451\n",
      "  Job job_1493936954640_1451 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1493936954640_1451 completed successfully\n",
      "  Output directory: hdfs:///user/chqngh/tmp/mrjob/mostFrequentWords.chqngh.20170618.191017.874889/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=536\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=357\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=396\n",
      "\t\tFILE: Number of bytes written=399616\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=908\n",
      "\t\tHDFS: Number of bytes written=357\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=20278272\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=9656320\n",
      "\t\tTotal time spent by all map tasks (ms)=13202\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=39606\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3772\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=18860\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=13202\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3772\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2550\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=104\n",
      "\t\tInput split bytes=372\n",
      "\t\tMap input records=28\n",
      "\t\tMap output bytes=385\n",
      "\t\tMap output materialized bytes=437\n",
      "\t\tMap output records=28\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1908695040\n",
      "\t\tReduce input groups=28\n",
      "\t\tReduce input records=28\n",
      "\t\tReduce output records=28\n",
      "\t\tReduce shuffle bytes=437\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=56\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7761215488\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/chqngh/tmp/mrjob/mostFrequentWords.chqngh.20170618.191017.874889/output...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing HDFS temp directory hdfs:///user/chqngh/tmp/mrjob/mostFrequentWords.chqngh.20170618.191017.874889...\n",
      "Removing temp directory /tmp/mostFrequentWords.chqngh.20170618.191017.874889...\n",
      "WARNING:root:Elapsed time: 97.3480660915 seconds\n",
      "    In minutes: 1.62246776819 mins\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r systems_test_stripes_5.4.1.b_1\n",
    "!python mostFrequentWords.py -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > systems_test_stripes_5.4.1.b_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=2\n",
    "### Launched reduce tasks=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"a\"\t2217\r\n",
      "\"in\"\t1201\r\n",
      "\"wales\"\t1099\r\n",
      "\"christmas\"\t1099\r\n",
      "\"child's\"\t1099\r\n"
     ]
    }
   ],
   "source": [
    "!head -5 systems_test_stripes_5.4.1.b_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On the full data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `full_mostFrequentWords_5.4.1.b': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Creating temp directory /tmp/mostFrequentWords.chqngh.20170619.051516.848830\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/chqngh/tmp/mrjob/mostFrequentWords.chqngh.20170619.051516.848830/files/...\n",
      "STDERR: put: `hdfs:///user/chqngh/tmp/mrjob/mostFrequentWords.chqngh.20170619.051516.848830/files/{pyArchive}': No such file or directory\n",
      "Traceback (most recent call last):\n",
      "  File \"mostFrequentWords.py\", line 63, in <module>\n",
      "    mostFrequentWords.run()\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/job.py\", line 429, in run\n",
      "    mr_job.execute()\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/job.py\", line 447, in execute\n",
      "    super(MRJob, self).execute()\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/launch.py\", line 158, in execute\n",
      "    self.run_job()\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/launch.py\", line 228, in run_job\n",
      "    runner.run()\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/runner.py\", line 481, in run\n",
      "    self._run()\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/hadoop.py\", line 335, in _run\n",
      "    self._upload_local_files_to_hdfs()\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/hadoop.py\", line 366, in _upload_local_files_to_hdfs\n",
      "    self._upload_to_hdfs(path, uri)\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/hadoop.py\", line 370, in _upload_to_hdfs\n",
      "    self.fs._put(path, target)\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/fs/hadoop.py\", line 317, in _put\n",
      "    self.invoke_hadoop(['fs', '-put', local_path, target])\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/fs/hadoop.py\", line 179, in invoke_hadoop\n",
      "    raise CalledProcessError(proc.returncode, args)\n",
      "subprocess.CalledProcessError: Command '['/opt/hadoop/bin/hadoop', 'fs', '-put', '{pyArchive}', 'hdfs:///user/chqngh/tmp/mrjob/mostFrequentWords.chqngh.20170619.051516.848830/files/{pyArchive}']' returned non-zero exit status 1\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r full_mostFrequentWords_5.4.1.b\n",
    "!python mostFrequentWords.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > full_mostFrequentWords_5.4.1.b\n",
    "# pscript = 'mostFrequentWords.py'\n",
    "# inpFileName = 'hdfs:///user/cendylin/filtered-5Grams/'\n",
    "# outFileName = 'full_mostFrequentWords_5.4.1.b'\n",
    "# runPyScript(pscript, inpFileName, outFileName)\n",
    "# !head -5 {outFileName}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=2\n",
    "### Launched reduce tasks=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"the\"\t5490815394\r\n",
      "\"of\"\t3698583299\r\n",
      "\"to\"\t2227866570\r\n",
      "\"in\"\t1421312776\r\n",
      "\"a\"\t1361123022\r\n"
     ]
    }
   ],
   "source": [
    "!head -5 full_mostFrequentWords_5.4.1.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most frequent words MR stats\n",
    "    \n",
    "    ec2_instance_type: m3.xlarge\n",
    "    num_ec2_instances: 15\n",
    "    \n",
    "__Step 1:__   \n",
    "\n",
    "    RUNNING for 590.7s ~= 10 minutes   \n",
    "    Launched map tasks=191  \n",
    "    Launched reduce tasks=57   \n",
    "\n",
    "__Step 2:__  \n",
    "\n",
    "    RUNNING for 76.6s   \n",
    "    Launched map tasks=110\n",
    "    Launched reduce tasks=16  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.4.1 - C. 20 Most/Least densely appearing words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mostLeastDenseWords.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mostLeastDenseWords.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import division\n",
    "import re\n",
    "import numpy as np\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import time\n",
    "import logging\n",
    "\n",
    "class mostLeastDenseWords(MRJob):\n",
    "    \n",
    "    # START STUDENT CODE 5.4.1.C\n",
    "           \n",
    "    MRJob.SORT_VALUES = True\n",
    "        \n",
    "    def __init__(self, args):\n",
    "        super(mostLeastDenseWords, self).__init__(args)\n",
    "        self.total_word_count = None\n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # Split line\n",
    "        splits = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "        words = splits[0].lower().split()\n",
    "        count = int(splits[1])\n",
    "        \n",
    "        for word in words:\n",
    "            yield \"*\", count\n",
    "            yield word, count\n",
    "            \n",
    "    \n",
    "    def combiner(self, word, counts):\n",
    "        total = sum(count for count in counts)\n",
    "        yield word, total\n",
    "    \n",
    "    def reducer(self, word, counts):\n",
    "    \n",
    "        total = sum(count for count in counts)\n",
    "        \n",
    "        if word == \"*\":\n",
    "            self.total_word_count = total\n",
    "        else:\n",
    "            yield float(total) / float(self.total_word_count), word\n",
    "    \n",
    "    def max_reducer(self, count, words):\n",
    "        for word in words:\n",
    "            yield word, count\n",
    "\n",
    "    def steps(self):\n",
    "        \n",
    "        custom_jobconf = {\n",
    "            'stream.num.map.output.key.fields':'2',\n",
    "            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapred.text.key.comparator.options': '-g -k1,1nr',\n",
    "            'mapred.reduce.tasks': '1'\n",
    "        }\n",
    "\n",
    "        return [\n",
    "                MRStep(\n",
    "                    mapper=self.mapper,\n",
    "                    reducer=self.reducer,\n",
    "                    combiner = self.combiner),\n",
    "                MRStep(jobconf=custom_jobconf,\n",
    "                       reducer=self.max_reducer)\n",
    "                 ]\n",
    "    \n",
    "    # END STUDENT CODE 5.4.1.C\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    mostLeastDenseWords.run()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    mins = elapsed_time/float(60)\n",
    "    a = \"\"\"Elapsed time: %s seconds\n",
    "    In minutes: %s mins\"\"\" % (str(elapsed_time), str(mins))\n",
    "    logging.warning(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On the test data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `systems_test_stripes_5.4.1.c_1': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/mostLeastDenseWords.chqngh.20170619.051530.346587\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/chqngh/tmp/mrjob/mostLeastDenseWords.chqngh.20170619.051530.346587/files/...\n",
      "STDERR: put: `hdfs:///user/chqngh/tmp/mrjob/mostLeastDenseWords.chqngh.20170619.051530.346587/files/{pyArchive}': No such file or directory\n",
      "Traceback (most recent call last):\n",
      "  File \"mostLeastDenseWords.py\", line 74, in <module>\n",
      "    mostLeastDenseWords.run()\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/job.py\", line 429, in run\n",
      "    mr_job.execute()\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/job.py\", line 447, in execute\n",
      "    super(MRJob, self).execute()\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/launch.py\", line 158, in execute\n",
      "    self.run_job()\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/launch.py\", line 228, in run_job\n",
      "    runner.run()\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/runner.py\", line 481, in run\n",
      "    self._run()\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/hadoop.py\", line 335, in _run\n",
      "    self._upload_local_files_to_hdfs()\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/hadoop.py\", line 366, in _upload_local_files_to_hdfs\n",
      "    self._upload_to_hdfs(path, uri)\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/hadoop.py\", line 370, in _upload_to_hdfs\n",
      "    self.fs._put(path, target)\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/fs/hadoop.py\", line 317, in _put\n",
      "    self.invoke_hadoop(['fs', '-put', local_path, target])\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/fs/hadoop.py\", line 179, in invoke_hadoop\n",
      "    raise CalledProcessError(proc.returncode, args)\n",
      "subprocess.CalledProcessError: Command '['/opt/hadoop/bin/hadoop', 'fs', '-put', '{pyArchive}', 'hdfs:///user/chqngh/tmp/mrjob/mostLeastDenseWords.chqngh.20170619.051530.346587/files/{pyArchive}']' returned non-zero exit status 1\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r systems_test_stripes_5.4.1.c_1\n",
    "!python mostLeastDenseWords.py -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > systems_test_stripes_5.4.1.c_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=2\n",
    "### Launched reduce tasks=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"a\"\t0.20000000000000001\r\n",
      "\"in\"\t0.1083446098331078\r\n",
      "\"wales\"\t0.099142986017140278\r\n",
      "\"christmas\"\t0.099142986017140278\r\n",
      "\"child's\"\t0.099142986017140278\r\n",
      "\"of\"\t0.091204330175913395\r\n",
      "\"study\"\t0.054488046910239063\r\n",
      "\"case\"\t0.054488046910239063\r\n",
      "\"female\"\t0.04032476319350474\r\n",
      "\"collection\"\t0.021560667568786648\r\n",
      "\"the\"\t0.011186287776274244\r\n",
      "\"tales\"\t0.011096075778078484\r\n",
      "\"fairy\"\t0.011096075778078484\r\n",
      "\"forms\"\t0.010464591790708164\r\n",
      "\"government\"\t0.0092016238159675235\r\n",
      "\"george\"\t0.0082995038340099234\r\n",
      "\"general\"\t0.0082995038340099234\r\n",
      "\"biography\"\t0.0082995038340099234\r\n",
      "\"city\"\t0.0055931438881371221\r\n",
      "\"circumstantial\"\t0.0055931438881371221\r\n",
      "\"by\"\t0.0055931438881371221\r\n",
      "\"sea\"\t0.0055931438881371221\r\n",
      "\"narrative\"\t0.0055931438881371221\r\n",
      "\"religious\"\t0.0053225078935498424\r\n",
      "\"establishing\"\t0.0053225078935498424\r\n",
      "\"for\"\t0.0053225078935498424\r\n",
      "\"bill\"\t0.0053225078935498424\r\n",
      "\"limited\"\t0.0049616599007668016\r\n"
     ]
    }
   ],
   "source": [
    "!cat systems_test_stripes_5.4.1.c_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On the full data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `full_mostLeastDenseWords_5.4.1.c': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Creating temp directory /tmp/mostLeastDenseWords.chqngh.20170619.051546.617526\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/chqngh/tmp/mrjob/mostLeastDenseWords.chqngh.20170619.051546.617526/files/...\n",
      "STDERR: put: `hdfs:///user/chqngh/tmp/mrjob/mostLeastDenseWords.chqngh.20170619.051546.617526/files/{pyArchive}': No such file or directory\n",
      "Traceback (most recent call last):\n",
      "  File \"mostLeastDenseWords.py\", line 74, in <module>\n",
      "    mostLeastDenseWords.run()\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/job.py\", line 429, in run\n",
      "    mr_job.execute()\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/job.py\", line 447, in execute\n",
      "    super(MRJob, self).execute()\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/launch.py\", line 158, in execute\n",
      "    self.run_job()\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/launch.py\", line 228, in run_job\n",
      "    runner.run()\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/runner.py\", line 481, in run\n",
      "    self._run()\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/hadoop.py\", line 335, in _run\n",
      "    self._upload_local_files_to_hdfs()\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/hadoop.py\", line 366, in _upload_local_files_to_hdfs\n",
      "    self._upload_to_hdfs(path, uri)\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/hadoop.py\", line 370, in _upload_to_hdfs\n",
      "    self.fs._put(path, target)\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/fs/hadoop.py\", line 317, in _put\n",
      "    self.invoke_hadoop(['fs', '-put', local_path, target])\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/fs/hadoop.py\", line 179, in invoke_hadoop\n",
      "    raise CalledProcessError(proc.returncode, args)\n",
      "subprocess.CalledProcessError: Command '['/opt/hadoop/bin/hadoop', 'fs', '-put', '{pyArchive}', 'hdfs:///user/chqngh/tmp/mrjob/mostLeastDenseWords.chqngh.20170619.051546.617526/files/{pyArchive}']' returned non-zero exit status 1\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r full_mostLeastDenseWords_5.4.1.c\n",
    "!python mostLeastDenseWords.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > full_mostLeastDenseWords_5.4.1.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=192\n",
    "### Launched reduce tasks=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Highest frequency words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"particulars\"\t9.999991202155478e-06\r\n",
      "\"venn\"\t9.9998902747203268e-08\r\n",
      "\"shortage\"\t9.9992645246223945e-06\r\n",
      "\"melville's\"\t9.9978717260173162e-08\r\n",
      "\"heine\"\t9.9978717260173162e-08\r\n",
      "\"gloat\"\t9.9978717260173162e-08\r\n",
      "\"exxon\"\t9.9978717260173162e-08\r\n",
      "\"anachronistic\"\t9.9978717260173162e-08\r\n",
      "\"trail\"\t9.9970239355620498e-06\r\n",
      "\"letzten\"\t9.9958531773143042e-08\r\n",
      "\"playgrounds\"\t9.9958531773143042e-08\r\n",
      "\"breathings\"\t9.9958531773143042e-08\r\n",
      "\"exceptionable\"\t9.9958531773143042e-08\r\n",
      "\"formalize\"\t9.9958531773143042e-08\r\n",
      "\"hates\"\t9.9942383383518938e-07\r\n",
      "\"coastwise\"\t9.9938346286112922e-08\r\n",
      "\"dividers\"\t9.9938346286112922e-08\r\n",
      "\"assents\"\t9.9938346286112922e-08\r\n",
      "\"residents\"\t9.9936529592280203e-06\r\n",
      "\"smiths\"\t9.9918160799082802e-08\r\n"
     ]
    }
   ],
   "source": [
    "!head -20 full_mostLeastDenseWords_5.4.1.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowest frequency words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"continued\"\t0.00010249356653279443\r\n",
      "\"twenty\"\t0.0001022841017338829\r\n",
      "\"heat\"\t0.00010214243998590554\r\n",
      "\"test\"\t0.00010213036906466152\r\n",
      "\"lines\"\t0.00010208735379180034\r\n",
      "\"towards\"\t0.00010163723761651575\r\n",
      "\"difficulty\"\t0.00010160754476509445\r\n",
      "\"income\"\t0.00010148736037531712\r\n",
      "\"mass\"\t0.00010128009579449188\r\n",
      "\"especially\"\t0.00010121792449443911\r\n",
      "\"kingdom\"\t0.00010088728621688579\r\n",
      "\"religion\"\t0.00010079972157414914\r\n",
      "\"former\"\t0.00010066784978738138\r\n",
      "\"obtain\"\t0.00010047905492718869\r\n",
      "\"aspects\"\t0.00010047713730592082\r\n",
      "\"parties\"\t0.0001004460920268685\r\n",
      "\"outside\"\t0.00010036181761851775\r\n",
      "\"mouth\"\t0.00010033606093706733\r\n",
      "\"remember\"\t0.00010031351374805469\r\n",
      "\"treated\"\t0.00010018121806605929\r\n"
     ]
    }
   ],
   "source": [
    "!tail -20 full_mostLeastDenseWords_5.4.1.c\n",
    "#TODO revert order probably"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word density MR stats\n",
    "\n",
    "    ec2_instance_type: m3.xlarge\n",
    "    num_ec2_instances: 15\n",
    "    \n",
    "__Step 1:__ \n",
    "\n",
    "    RUNNING for 649.2s  ~= 10 minutes      \n",
    "    Launched map tasks=190   \n",
    "    Launched reduce tasks=57     \n",
    "\n",
    "__Step 2:__  \n",
    "\n",
    "    RUNNING for 74.4s  ~= 1 minute    \n",
    "    Launched map tasks=110   \n",
    "    Launched reduce tasks=20   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.4.1 - D. Distribution of 5-gram sizes (character length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting distribution.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile distribution.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import time\n",
    "import logging\n",
    "\n",
    "class distribution(MRJob):\n",
    "    \n",
    "    # START STUDENT CODE 5.4.1.D\n",
    "    \n",
    "    MRJob.SORT_VALUES = True\n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # Split line\n",
    "        splits = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "        words = splits[0].lower().split()\n",
    "        \n",
    "        char_count = 0\n",
    "        \n",
    "        # Count characters\n",
    "        for word in words:\n",
    "            char_count += len(word)\n",
    "        \n",
    "        yield char_count, 1\n",
    "            \n",
    "    \n",
    "    def combiner(self, ngram_size, counts):\n",
    "        yield ngram_size, sum(count for count in counts)\n",
    "    \n",
    "    def reducer(self, ngram_size, counts):\n",
    "        yield ngram_size, sum(count for count in counts)\n",
    "\n",
    "    def steps(self):\n",
    "        \n",
    "        custom_jobconf = {\n",
    "            'stream.num.map.output.key.fields':'2',\n",
    "            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapred.text.key.comparator.options': '-k1,1n',\n",
    "            'mapred.reduce.tasks': '1'\n",
    "        }\n",
    "\n",
    "        return [\n",
    "                MRStep(\n",
    "                    jobconf=custom_jobconf,\n",
    "                    mapper=self.mapper,\n",
    "                    reducer=self.reducer,\n",
    "                    combiner = self.combiner)\n",
    "        ]\n",
    "\n",
    "    # END STUDENT CODE 5.4.1.D\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    distribution.run()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    mins = elapsed_time/float(60)\n",
    "    a = \"\"\"Elapsed time: %s seconds\n",
    "    In minutes: %s mins\"\"\" % (str(elapsed_time), str(mins))\n",
    "    logging.warning(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__On the test data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `5.3distributions_test/part-00000': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/distribution.chqngh.20170619.051600.175358\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/chqngh/tmp/mrjob/distribution.chqngh.20170619.051600.175358/files/...\n",
      "STDERR: put: `hdfs:///user/chqngh/tmp/mrjob/distribution.chqngh.20170619.051600.175358/files/{pyArchive}': No such file or directory\n",
      "Traceback (most recent call last):\n",
      "  File \"distribution.py\", line 59, in <module>\n",
      "    distribution.run()\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/job.py\", line 429, in run\n",
      "    mr_job.execute()\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/job.py\", line 447, in execute\n",
      "    super(MRJob, self).execute()\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/launch.py\", line 158, in execute\n",
      "    self.run_job()\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/launch.py\", line 228, in run_job\n",
      "    runner.run()\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/runner.py\", line 481, in run\n",
      "    self._run()\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/hadoop.py\", line 335, in _run\n",
      "    self._upload_local_files_to_hdfs()\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/hadoop.py\", line 366, in _upload_local_files_to_hdfs\n",
      "    self._upload_to_hdfs(path, uri)\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/hadoop.py\", line 370, in _upload_to_hdfs\n",
      "    self.fs._put(path, target)\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/fs/hadoop.py\", line 317, in _put\n",
      "    self.invoke_hadoop(['fs', '-put', local_path, target])\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/fs/hadoop.py\", line 179, in invoke_hadoop\n",
      "    raise CalledProcessError(proc.returncode, args)\n",
      "subprocess.CalledProcessError: Command '['/opt/hadoop/bin/hadoop', 'fs', '-put', '{pyArchive}', 'hdfs:///user/chqngh/tmp/mrjob/distribution.chqngh.20170619.051600.175358/files/{pyArchive}']' returned non-zero exit status 1\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r 5.3distributions_test/part-00000\n",
    "!python distribution.py -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > 5.3distributions_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=2\n",
    "### Launched reduce tasks=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\t1\r\n",
      "18\t1\r\n",
      "19\t1\r\n",
      "20\t1\r\n",
      "22\t1\r\n",
      "23\t1\r\n",
      "24\t1\r\n",
      "25\t1\r\n",
      "29\t2\r\n"
     ]
    }
   ],
   "source": [
    "!cat 5.3distributions_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Histogram 10-line test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+MAAAGuCAYAAAD73ddLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUtsGEqhaaFwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6645418e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "\n",
    "results_A = []\n",
    "for line in open(\"5.3distributions_test\").readlines():\n",
    "    line = line.strip()\n",
    "    X,Y = line.split(\"\\t\")\n",
    "    results_A.append([int(X),int(Y)])\n",
    "\n",
    "items = (np.array(results_A)[::-1].T)\n",
    "fig = pl.figure(figsize=(17,7))\n",
    "ax = pl.subplot(111)\n",
    "width=0.8\n",
    "ax.bar(range(len(items[0])), items[1], width=width)\n",
    "ax.set_xticks(np.arange(len(items[0])) + width/2)\n",
    "ax.set_xticklabels(items[0], rotation=90)\n",
    "ax.invert_xaxis()\n",
    "\n",
    "\n",
    "pl.title(\"Distributions of 5 Gram lengths in 10-line test\")\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On the full data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `full_distribution_5.4.1.d': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Creating temp directory /tmp/distribution.chqngh.20170619.051617.101194\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/chqngh/tmp/mrjob/distribution.chqngh.20170619.051617.101194/files/...\n",
      "STDERR: put: `hdfs:///user/chqngh/tmp/mrjob/distribution.chqngh.20170619.051617.101194/files/{pyArchive}': No such file or directory\n",
      "Traceback (most recent call last):\n",
      "  File \"distribution.py\", line 59, in <module>\n",
      "    distribution.run()\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/job.py\", line 429, in run\n",
      "    mr_job.execute()\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/job.py\", line 447, in execute\n",
      "    super(MRJob, self).execute()\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/launch.py\", line 158, in execute\n",
      "    self.run_job()\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/launch.py\", line 228, in run_job\n",
      "    runner.run()\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/runner.py\", line 481, in run\n",
      "    self._run()\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/hadoop.py\", line 335, in _run\n",
      "    self._upload_local_files_to_hdfs()\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/hadoop.py\", line 366, in _upload_local_files_to_hdfs\n",
      "    self._upload_to_hdfs(path, uri)\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/hadoop.py\", line 370, in _upload_to_hdfs\n",
      "    self.fs._put(path, target)\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/fs/hadoop.py\", line 317, in _put\n",
      "    self.invoke_hadoop(['fs', '-put', local_path, target])\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/fs/hadoop.py\", line 179, in invoke_hadoop\n",
      "    raise CalledProcessError(proc.returncode, args)\n",
      "subprocess.CalledProcessError: Command '['/opt/hadoop/bin/hadoop', 'fs', '-put', '{pyArchive}', 'hdfs:///user/chqngh/tmp/mrjob/distribution.chqngh.20170619.051617.101194/files/{pyArchive}']' returned non-zero exit status 1\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r full_distribution_5.4.1.d\n",
    "!python distribution.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > full_distribution_5.4.1.d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=2\n",
    "### Launched reduce tasks=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution MRJob stats\n",
    "\n",
    "__Step 1:__ \n",
    "\n",
    "    RUNNING for 157.8s ~= 2.6 minutes  \n",
    "    Launched map tasks=191  \n",
    "    Launched reduce tasks=16   \n",
    "    \n",
    "__Step 2:__  \n",
    "\n",
    "    RUNNING for 115.0s ~= 2 minutes   \n",
    "    Launched map tasks=139\n",
    "\tLaunched reduce tasks=1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA/oAAAG1CAYAAABJd48xAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAACAEfn/AYLyy+6w1VfAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2ccd1c76d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "\n",
    "results_A = []\n",
    "for line in open(\"full_distribution_5.4.1.d\").readlines():\n",
    "    line = line.strip()\n",
    "    X,Y = line.split(\"\\t\")\n",
    "    results_A.append([int(X),int(Y)])\n",
    "\n",
    "items = (np.array(results_A)[::-1].T)\n",
    "fig = pl.figure(figsize=(17,7))\n",
    "ax = pl.subplot(111)\n",
    "width=0.8\n",
    "ax.bar(range(len(items[0])), items[1], width=width)\n",
    "ax.set_xticks(np.arange(len(items[0])) + width/2)\n",
    "ax.set_xticklabels(items[0], rotation=90)\n",
    "ax.invert_xaxis()\n",
    "\n",
    "\n",
    "pl.title(\"Distributions of 5 Gram lengths\")\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO revert order of counts in chart\n",
    "### NH - done, used ax.invert_xaxis()\n",
    "### http://matplotlib.org/devdocs/api/_as_gen/matplotlib.axes.Axes.invert_xaxis.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.4.2 <a name=\"5.4.2\"></a>OPTIONAL Question: log-log plots (PHASE 2)\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Plot the log-log plot of the frequency distributuion of unigrams. Does it follow power law distribution?\n",
    "\n",
    "For more background see:\n",
    "- https://en.wikipedia.org/wiki/Log%E2%80%93log_plot\n",
    "- https://en.wikipedia.org/wiki/Power_law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.5  <a name=\"5.5\"></a> Synonym detection over 2Gig of Data with extra Preprocessing steps (HW5.3 plus some preprocessing)   (Phase 2)\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "For the remainder of this assignment please feel free to eliminate stop words from your analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">There is also a corpus of stopwords, that is, high-frequency words like \"the\", \"to\" and \"also\" that we sometimes want to filter out of a document before further processing. Stopwords usually have little lexical content, and their presence in a text fails to distinguish it from other texts. Python's nltk comes with a prebuilt list of stopwords (see below). Using this stopword list filter out these tokens from your analysis and rerun the experiments in 5.5 and disucuss the results of using a stopword list and without using a stopword list.\n",
    "\n",
    "> from nltk.corpus import stopwords\n",
    " stopwords.words('english')\n",
    "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: A large subset of the Google n-grams dataset as was described above\n",
    "\n",
    "For each HW 5.4 -5.5.1 Please unit test and system test your code with respect \n",
    "to SYSTEMS TEST DATASET and show the results. \n",
    "Please compute the expected answer by hand and show your hand calculations for the \n",
    "SYSTEMS TEST DATASET. Then show the results you get with your system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the assignment we will focus on developing methods for detecting synonyms, using the Google 5-grams dataset. At a high level:\n",
    "\n",
    "\n",
    "1. remove stopwords\n",
    "2. get 10,0000 most frequent\n",
    "3. get 1000 (9001-10000) features\n",
    "3. build stripes\n",
    "\n",
    "To accomplish this you must script two main tasks using MRJob:\n",
    "\n",
    "\n",
    "__TASK (1)__ Build stripes for the most frequent 10,000 words using cooccurence information based on\n",
    "the words ranked from 9001,-10,000 as a basis/vocabulary (drop stopword-like terms),\n",
    "and output to a file in your bucket on s3 (bigram analysis, though the words are non-contiguous).\n",
    "\n",
    "\n",
    "__TASK (2)__ Using two (symmetric) comparison methods of your choice \n",
    "(e.g., correlations, distances, similarities), pairwise compare \n",
    "all stripes (vectors), and output to a file in your bucket on s3.\n",
    "\n",
    "#### Design notes for TASK (1)\n",
    "For this task you will be able to modify the pattern we used in HW 3.2\n",
    "(feel free to use the solution as reference). To total the word counts \n",
    "across the 5-grams, output the support from the mappers using the total \n",
    "order inversion pattern:\n",
    "\n",
    "<*word,count>\n",
    "\n",
    "to ensure that the support arrives before the cooccurrences.\n",
    "\n",
    "In addition to ensuring the determination of the total word counts,\n",
    "the mapper must also output co-occurrence counts for the pairs of\n",
    "words inside of each 5-gram. Treat these words as a basket,\n",
    "as we have in HW 3, but count all stripes or pairs in both orders,\n",
    "i.e., count both orderings: (word1,word2), and (word2,word1), to preserve\n",
    "symmetry in our output for TASK (2).\n",
    "\n",
    "#### Design notes for _TASK (2)_\n",
    "For this task you will have to determine a method of comparison.\n",
    "Here are a few that you might consider:\n",
    "\n",
    "- Jaccard\n",
    "- Cosine similarity\n",
    "- Spearman correlation\n",
    "- Euclidean distance\n",
    "- Taxicab (Manhattan) distance\n",
    "- Shortest path graph distance (a graph, because our data is symmetric!)\n",
    "- Pearson correlation\n",
    "- Kendall correlation\n",
    "\n",
    "However, be cautioned that some comparison methods are more difficult to\n",
    "parallelize than others, and do not perform more associations than is necessary, \n",
    "since your choice of association will be symmetric.\n",
    "\n",
    "Please use the inverted index (discussed in live session #5) based pattern to compute the pairwise (term-by-term) similarity matrix. \n",
    "\n",
    "Please report the size of the cluster used and the amount of time it takes to run for the index construction task and for the synonym calculation task. How many pairs need to be processed (HINT: use the posting list length to calculate directly)? Report your  Cluster configuration!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example MR stats: (report times!)\n",
    "    took ~11 minutes on 5 m3.xlarge nodes\n",
    "    Data-local map tasks=188\n",
    "\tLaunched map tasks=190\n",
    "\tLaunched reduce tasks=15\n",
    "\tOther local map tasks=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# START STUDENT CODE 5.5\n",
    "# ADD OR REMOVE CELLS AS NEEDED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Frequency ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting frequencies5_5.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile frequencies5_5.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import re\n",
    "\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import time\n",
    "import logging\n",
    "\n",
    "class frequencies(MRJob):\n",
    "\n",
    "    # START STUDENT CODE 5.4.1.B\n",
    "    \n",
    "    MRJob.SORT_VALUES = True\n",
    "        \n",
    "    def __init__(self, args):\n",
    "        super(frequencies, self).__init__(args)\n",
    "        #self.min_rank = 9001\n",
    "        #self.max_rank = 10000 \n",
    "        self.current_rank = 0\n",
    "\n",
    "    def configure_options(self): \n",
    "        super(frequencies, self).configure_options() \n",
    "        self.add_passthrough_option('--min_rank', dest='min_rank', type='int', default=9001) \n",
    "        self.add_passthrough_option('--max_rank', dest='max_rank', type='int', default=10000) \n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # Split line\n",
    "        splits = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "        words = splits[0].lower().split()\n",
    "        count = int(splits[1])\n",
    "        \n",
    "        for word in words:\n",
    "            yield word, count\n",
    "            \n",
    "    \n",
    "    def combiner(self, word, counts):\n",
    "        total = sum(count for count in counts)\n",
    "        yield word, total\n",
    "    \n",
    "    def reducer(self, word, counts):\n",
    "        total = sum(count for count in counts)\n",
    "        yield total, word\n",
    "    \n",
    "    def max_reducer(self, count, words):\n",
    "        \n",
    "        # Words come in frequency descending order here\n",
    "        # Only yield the words that are within the min and max frequency ranking desired\n",
    "        \n",
    "        for word in words:\n",
    "            self.current_rank += 1\n",
    "            \n",
    "            if self.current_rank >= self.options.min_rank and self.current_rank <= self.options.max_rank:\n",
    "                yield word, count\n",
    "\n",
    "    def steps(self):\n",
    "        \n",
    "        custom_jobconf = {\n",
    "            'stream.num.map.output.key.fields':'2',\n",
    "            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapred.text.key.comparator.options': '-k1,1nr',\n",
    "            'mapred.reduce.tasks': '1'\n",
    "        }\n",
    "\n",
    "        return [\n",
    "                MRStep(mapper=self.mapper,\n",
    "                    reducer=self.reducer,\n",
    "                    combiner = self.combiner),\n",
    "                MRStep(jobconf=custom_jobconf,\n",
    "                       reducer=self.max_reducer)\n",
    "                 ]\n",
    "\n",
    "    # END STUDENT CODE 5.4.1.B\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    frequencies.run()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    mins = elapsed_time/float(60)\n",
    "    a = \"\"\"Elapsed time: %s seconds\n",
    "    In minutes: %s mins\"\"\" % (str(elapsed_time), str(mins))\n",
    "    logging.warning(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency ranking on 10-line test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `frequencies_test5.5': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/frequencies5_5.chqngh.20170619.052015.986383\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/chqngh/tmp/mrjob/frequencies5_5.chqngh.20170619.052015.986383/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob5271340902699786319.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1493936954640_1597\n",
      "  Submitted application application_1493936954640_1597\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1493936954640_1597/\n",
      "  Running job: job_1493936954640_1597\n",
      "  Job job_1493936954640_1597 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1493936954640_1597 completed successfully\n",
      "  Output directory: hdfs:///user/chqngh/tmp/mrjob/frequencies5_5.chqngh.20170619.052015.986383/step-output/0000\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=563\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=357\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=430\n",
      "\t\tFILE: Number of bytes written=401444\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1015\n",
      "\t\tHDFS: Number of bytes written=357\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=25743360\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=14709760\n",
      "\t\tTotal time spent by all map tasks (ms)=16760\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=50280\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5746\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=28730\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=16760\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=5746\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2990\n",
      "\t\tCombine input records=50\n",
      "\t\tCombine output records=31\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=89\n",
      "\t\tInput split bytes=452\n",
      "\t\tMap input records=10\n",
      "\t\tMap output bytes=602\n",
      "\t\tMap output materialized bytes=458\n",
      "\t\tMap output records=50\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1912217600\n",
      "\t\tReduce input groups=28\n",
      "\t\tReduce input records=31\n",
      "\t\tReduce output records=28\n",
      "\t\tReduce shuffle bytes=458\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=62\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7743225856\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob3796656124648989014.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1493936954640_1599\n",
      "  Submitted application application_1493936954640_1599\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1493936954640_1599/\n",
      "  Running job: job_1493936954640_1599\n",
      "  Job job_1493936954640_1599 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "  Job job_1493936954640_1599 failed with state FAILED due to: Application application_1493936954640_1599 failed 2 times due to AM Container for appattempt_1493936954640_1599_000002 exited with  exitCode: -1000\n",
      "For more detailed output, check application tracking page:http://rm-ia.s3s.altiscale.com:8188/applicationhistory/app/application_1493936954640_1599Then, click on links to logs of each attempt.\n",
      "Diagnostics: Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@26ce4706 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "Failing this attempt. Failing the application.\n",
      "  Job not successful!\n",
      "  Streaming Command Failed!\n",
      "Attempting to fetch counters from logs...\n",
      "No counters found\n",
      "Scanning logs for probable cause of failure...\n",
      "Step 2 of 2 failed: Command '['/opt/hadoop/bin/hadoop', 'jar', '/opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar', '-files', 'hdfs:///user/chqngh/tmp/mrjob/frequencies5_5.chqngh.20170619.052015.986383/files/frequencies5_5.py#frequencies5_5.py,hdfs:///user/chqngh/tmp/mrjob/frequencies5_5.chqngh.20170619.052015.986383/files/setup-wrapper.sh#setup-wrapper.sh', '-archives', 'hdfs:///user/chqngh/tmp/mrjob/frequencies5_5.chqngh.20170619.052015.986383/files/mrjob.tar.gz#mrjob.tar.gz,hdfs:///user/chqngh/virtualenv/hw5_test2.zip#hw5_test2', '-D', u'mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator', '-D', u'mapred.reduce.tasks=1', '-D', u'mapred.text.key.comparator.options=-k1,1nr', '-D', 'mapred.text.key.partitioner.options=-k1,1', '-D', u'mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator', '-D', u'mapreduce.job.reduces=1', '-D', u'mapreduce.partition.keycomparator.options=-k1,1nr', '-D', 'mapreduce.partition.keypartitioner.options=-k1,1', '-D', u'stream.num.map.output.key.fields=2', '-partitioner', 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner', '-input', 'hdfs:///user/chqngh/tmp/mrjob/frequencies5_5.chqngh.20170619.052015.986383/step-output/0000', '-output', 'hdfs:///user/chqngh/tmp/mrjob/frequencies5_5.chqngh.20170619.052015.986383/output', '-mapper', 'cat', '-reducer', 'sh -ex setup-wrapper.sh ./hw5_test2/bin/python frequencies5_5.py --step-num=1 --reducer --max_rank 4 --min_rank 2']' returned non-zero exit status 256\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r frequencies_test5.5\n",
    "!python frequencies5_5.py --min_rank 2 --max_rank 4 -r \\\n",
    "    hadoop googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > frequencies_test5.5\n",
    "\n",
    "# pscript = 'distribution.py'\n",
    "# inpFileName = 'hdfs:///user/cendylin/filtered-5Grams/'\n",
    "# outFileName = 'frequencies_test5.5'\n",
    "# min_rank = 2\n",
    "# max_rank = 4\n",
    "# runPyScript(pscript, inpFileName, outFileName, min_rank, max_rank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"in\"\t1201\r\n",
      "\"wales\"\t1099\r\n",
      "\"christmas\"\t1099\r\n"
     ]
    }
   ],
   "source": [
    "!cat frequencies_test5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/sh/tmqpc4o0xswhkvz/AACUifrl6wrMrlK6a3X3lZ9Ea?dl=0 -o googlebooks-eng-all-5gram-20090715-0-filtered.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency ranking on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `frequencies5.5': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Creating temp directory /tmp/frequencies5_5.chqngh.20170619.051717.476687\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/chqngh/tmp/mrjob/frequencies5_5.chqngh.20170619.051717.476687/files/...\n",
      "STDERR: put: `hdfs:///user/chqngh/tmp/mrjob/frequencies5_5.chqngh.20170619.051717.476687/files/{pyArchive}': No such file or directory\n",
      "Traceback (most recent call last):\n",
      "  File \"frequencies5_5.py\", line 81, in <module>\n",
      "    frequencies.run()\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/job.py\", line 429, in run\n",
      "    mr_job.execute()\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/job.py\", line 447, in execute\n",
      "    super(MRJob, self).execute()\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/launch.py\", line 158, in execute\n",
      "    self.run_job()\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/launch.py\", line 228, in run_job\n",
      "    runner.run()\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/runner.py\", line 481, in run\n",
      "    self._run()\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/hadoop.py\", line 335, in _run\n",
      "    self._upload_local_files_to_hdfs()\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/hadoop.py\", line 366, in _upload_local_files_to_hdfs\n",
      "    self._upload_to_hdfs(path, uri)\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/hadoop.py\", line 370, in _upload_to_hdfs\n",
      "    self.fs._put(path, target)\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/fs/hadoop.py\", line 317, in _put\n",
      "    self.invoke_hadoop(['fs', '-put', local_path, target])\n",
      "  File \"/home/chqngh/.conda/envs/hw5_test2/lib/python2.7/site-packages/mrjob/fs/hadoop.py\", line 179, in invoke_hadoop\n",
      "    raise CalledProcessError(proc.returncode, args)\n",
      "subprocess.CalledProcessError: Command '['/opt/hadoop/bin/hadoop', 'fs', '-put', '{pyArchive}', 'hdfs:///user/chqngh/tmp/mrjob/frequencies5_5.chqngh.20170619.051717.476687/files/{pyArchive}']' returned non-zero exit status 1\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r frequencies5.5\n",
    "!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > frequencies5.5\n",
    "#!python frequencies5_5.py -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered.txt > frequencies5.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cat frequencies5.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stripes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stripes5_5.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile stripes5_5.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "import re\n",
    "import mrjob\n",
    "import json\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import itertools\n",
    "import collections\n",
    "import time\n",
    "import logging\n",
    "\n",
    "class stripes(MRJob):\n",
    "  \n",
    "    #START SUDENT CODE531_STRIPES\n",
    "  \n",
    "    MRJob.SORT_VALUES = True\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        self.valid_words = set()\n",
    "        super(stripes, self).__init__(args)\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        # Load the left table in the init so all mappers get this info\n",
    "        with open(\"frequencies5.5\", 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                x = line.strip().split(\"\\t\")\n",
    "                self.valid_words.add(x[0].strip(\"\\\"\"))\n",
    "            \n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        splits = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "\n",
    "        words = splits[0].lower().split()\n",
    "        count = splits[1]\n",
    "\n",
    "        H = {}\n",
    "        for subset in itertools.combinations(sorted(set(words)), 2):\n",
    "            \n",
    "            if subset[0] in self.valid_words and subset[1] in self.valid_words:\n",
    "\n",
    "                # Process combinations in sorted order, i.e. \"hello\",\"tomorrow\"\n",
    "                if subset[0] not in H.keys():\n",
    "                    H[subset[0]] = {}\n",
    "                    H[subset[0]][subset[1]] = count \n",
    "                elif subset[1] not in H[subset[0]]:\n",
    "                    H[subset[0]][subset[1]] = count\n",
    "                else:\n",
    "                    H[subset[0]][subset[1]] += count\n",
    "\n",
    "                # Obtain combinations in reverse order, to consider them both ways\n",
    "                # TODO: Should refactor this and the block above, shameless copy-paste\n",
    "                if subset[1] not in H.keys():\n",
    "                    H[subset[1]] = {}\n",
    "                    H[subset[1]][subset[0]] = count \n",
    "                elif subset[0] not in H[subset[1]]:\n",
    "                    H[subset[1]][subset[0]] = count\n",
    "                else:\n",
    "                    H[subset[1]][subset[0]] += count\n",
    "        for key in H.keys():\n",
    "            #print \"%s\\t%s\" % (key, json.dumps(H[key]))\n",
    "            yield key, H[key]\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        \n",
    "        counter = {}\n",
    "\n",
    "        for value in values:\n",
    "            \n",
    "            for k, v in value.iteritems():\n",
    "                if k in counter:\n",
    "                    counter[k] += int(v)\n",
    "                else:\n",
    "                    counter[k] = int(v)\n",
    "        \n",
    "        yield key, counter\n",
    "\n",
    "  #END SUDENT CODE531_STRIPES\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    stripes.run()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    mins = elapsed_time/float(60)\n",
    "    a = \"\"\"Elapsed time: %s seconds\n",
    "    In minutes: %s mins\"\"\" % (str(elapsed_time), str(mins))\n",
    "    logging.warning(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `stripes5.5': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/stripes5_5.chqngh.20170619.052128.042034\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/chqngh/tmp/mrjob/stripes5_5.chqngh.20170619.052128.042034/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob3349796141722926923.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1493936954640_1600\n",
      "  Submitted application application_1493936954640_1600\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1493936954640_1600/\n",
      "  Running job: job_1493936954640_1600\n",
      "  Job job_1493936954640_1600 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "  Job job_1493936954640_1600 failed with state FAILED due to: Application application_1493936954640_1600 failed 2 times due to AM Container for appattempt_1493936954640_1600_000002 exited with  exitCode: -1000\n",
      "For more detailed output, check application tracking page:http://rm-ia.s3s.altiscale.com:8188/applicationhistory/app/application_1493936954640_1600Then, click on links to logs of each attempt.\n",
      "Diagnostics: Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@2b5b3c94 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "Failing this attempt. Failing the application.\n",
      "  Job not successful!\n",
      "  Streaming Command Failed!\n",
      "Attempting to fetch counters from logs...\n",
      "No counters found\n",
      "Scanning logs for probable cause of failure...\n",
      "Step 1 of 1 failed: Command '['/opt/hadoop/bin/hadoop', 'jar', '/opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar', '-files', 'hdfs:///user/chqngh/tmp/mrjob/stripes5_5.chqngh.20170619.052128.042034/files/setup-wrapper.sh#setup-wrapper.sh,hdfs:///user/chqngh/tmp/mrjob/stripes5_5.chqngh.20170619.052128.042034/files/frequencies5.5#frequencies5.5,hdfs:///user/chqngh/tmp/mrjob/stripes5_5.chqngh.20170619.052128.042034/files/stripes5_5.py#stripes5_5.py', '-archives', 'hdfs:///user/chqngh/tmp/mrjob/stripes5_5.chqngh.20170619.052128.042034/files/mrjob.tar.gz#mrjob.tar.gz,hdfs:///user/chqngh/virtualenv/hw5_test2.zip#hw5_test2', '-D', 'mapred.text.key.partitioner.options=-k1,1', '-D', 'mapreduce.partition.keypartitioner.options=-k1,1', '-D', 'stream.num.map.output.key.fields=2', '-partitioner', 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner', '-input', 'hdfs:///user/chqngh/tmp/mrjob/stripes5_5.chqngh.20170619.052128.042034/files/googlebooks-eng-all-5gram-20090715-0-filtered.txt', '-output', 'hdfs:///user/chqngh/tmp/mrjob/stripes5_5.chqngh.20170619.052128.042034/output', '-mapper', 'sh -ex setup-wrapper.sh ./hw5_test2/bin/python stripes5_5.py --step-num=0 --mapper', '-reducer', 'sh -ex setup-wrapper.sh ./hw5_test2/bin/python stripes5_5.py --step-num=0 --reducer']' returned non-zero exit status 256\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r stripes5.5\n",
    "# !python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python stripes5_5.py --file=frequencies5.5 -r \\\n",
    "    hadoop googlebooks-eng-all-5gram-20090715-0-filtered.txt \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > stripes5.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cat stripes5.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting index5_5.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile index5_5.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "from __future__ import division\n",
    "import collections\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import itertools\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "\n",
    "class index(MRJob):\n",
    "    \n",
    "    #START SUDENT CODE531_INV_INDEX\n",
    "  \n",
    "    def mapper(self, _, line):\n",
    "        key, stripeJson = line.strip().split('\\t')\n",
    "        key = key.strip(\"\\\"\")\n",
    "        stripe = json.loads(stripeJson)\n",
    "        \n",
    "        for k, v in stripe.iteritems():\n",
    "            yield k, [key, len(stripe)]\n",
    "        \n",
    "    def reducer(self, key, values):\n",
    "\n",
    "        table = {}\n",
    "        for value in values:\n",
    "            table[value[0]] = value[1]\n",
    "            \n",
    "        yield key, table\n",
    "        \n",
    "    #END SUDENT CODE531_INV_INDEX\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    index.run() \n",
    "    elapsed_time = time.time() - start_time\n",
    "    mins = elapsed_time/float(60)\n",
    "    a = \"\"\"Elapsed time: %s seconds\n",
    "    In minutes: %s mins\"\"\" % (str(elapsed_time), str(mins))\n",
    "    logging.warning(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `index5.5': No such file or directory\n",
      "python: can't open file 'index5_5.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r index5.5\n",
    "!python index5_5.py -r hadoop stripes5.5 \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > index5.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat index5.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting similarity5_5.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile similarity5_5.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "import collections\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "#import numpy as np\n",
    "import itertools\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import time\n",
    "import logging\n",
    "\n",
    "class similarity(MRJob):\n",
    "  \n",
    "    #START SUDENT CODE531_SIMILARITY\n",
    "\n",
    "    MRJob.SORT_VALUES = True\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        key, valuesJson = line.strip().split('\\t')\n",
    "        key = key.strip(\"\\\"\")\n",
    "        values = json.loads(valuesJson)\n",
    "\n",
    "        for pair in itertools.combinations(sorted(set(values)), 2):\n",
    "            yield pair, [values[pair[0]], values[pair[1]]]\n",
    "        \n",
    "    def reducer(self, key, values):\n",
    "        intersection = 0\n",
    "        count1 = None\n",
    "        count2 = None\n",
    "        \n",
    "        cosine = 0.0\n",
    "        \n",
    "        # Iterate through the values\n",
    "        for value in values:\n",
    "            # Jaccard, get counts for the intersection, and for each set\n",
    "            intersection += 1\n",
    "            if count1 == None:\n",
    "                count1 = value[0]\n",
    "                count2 = value[1]\n",
    "        \n",
    "            # Cosine\n",
    "            a = 1 / math.sqrt(value[0])\n",
    "            b = 1 / math.sqrt(value[1])\n",
    "            cosine += a * b\n",
    "            \n",
    "        jaccard = float(intersection) / float(count1 + count2 - intersection)\n",
    "        \n",
    "        overlap_coefficient = float(intersection) / min(count1, count2)\n",
    "        \n",
    "        dice_coefficient = float(2 * intersection) / (count1 + count2)\n",
    "        \n",
    "        average = (cosine + jaccard + overlap_coefficient + dice_coefficient) / 4.0\n",
    "        \n",
    "        yield average, [key[0] + ' - ' + key[1], cosine, jaccard, overlap_coefficient, dice_coefficient, average]\n",
    "    \n",
    "    \n",
    "    def max_reducer(self, average, records):\n",
    "        for record in records:\n",
    "            yield average, record\n",
    "\n",
    "    def steps(self):\n",
    "        \n",
    "        custom_jobconf = {\n",
    "            'stream.num.map.output.key.fields':'2',\n",
    "            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapred.text.key.comparator.options': '-k1,1nr',\n",
    "            'mapred.reduce.tasks': '1'\n",
    "        }\n",
    "\n",
    "        return [\n",
    "                MRStep(mapper=self.mapper,\n",
    "                    reducer=self.reducer),\n",
    "                MRStep(jobconf=custom_jobconf,\n",
    "                       reducer=self.max_reducer)\n",
    "                 ]\n",
    "    \n",
    "    #END SUDENT CODE531_SIMILARITY\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    similarity.run()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    mins = elapsed_time/float(60)\n",
    "    a = \"\"\"Elapsed time: %s seconds\n",
    "    In minutes: %s mins\"\"\" % (str(elapsed_time), str(mins))\n",
    "    logging.warning(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `similarity5.5': No such file or directory\n",
      "python: can't open file 'similarity5_5.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r similarity5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python similarity5_5.py -r hadoop index5.5 \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > similarity5.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat similarity5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "sortedSims = []\n",
    "with open(\"similarity5.5\",\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "\n",
    "        line = line.strip()\n",
    "        avg,lisst = line.split(\"\\t\")\n",
    "        lisst = json.loads(lisst)\n",
    "        lisst.append(avg)\n",
    "        sortedSims.append(lisst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir sims2\n",
    "!head -1000 similarity5.5 > sims2/top1000sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> l\n",
      "\n",
      "Packages:\n",
      "  [ ] abc................. Australian Broadcasting Commission 2006\n",
      "  [ ] alpino.............. Alpino Dutch Treebank\n",
      "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
      "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
      "  [ ] basque_grammars..... Grammars for Basque\n",
      "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
      "                           Extraction Systems in Biology)\n",
      "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
      "  [ ] book_grammars....... Grammars from NLTK Book\n",
      "  [ ] brown............... Brown Corpus\n",
      "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
      "  [ ] cess_cat............ CESS-CAT Treebank\n",
      "  [ ] cess_esp............ CESS-ESP Treebank\n",
      "  [ ] chat80.............. Chat-80 Data Files\n",
      "  [ ] city_database....... City Database\n",
      "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
      "  [ ] comparative_sentences Comparative Sentence Dataset\n",
      "  [ ] comtrans............ ComTrans Corpus Sample\n",
      "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
      "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
      "Hit Enter to continue: \n",
      "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
      "                           and Basque Subset)\n",
      "  [ ] crubadan............ Crubadan Corpus\n",
      "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
      "  [ ] dolch............... Dolch Word List\n",
      "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
      "                           Corpus\n",
      "  [ ] floresta............ Portuguese Treebank\n",
      "  [ ] framenet_v15........ FrameNet 1.5\n",
      "  [ ] framenet_v17........ FrameNet 1.7\n",
      "  [ ] gazetteers.......... Gazeteer Lists\n",
      "  [ ] genesis............. Genesis Corpus\n",
      "  [ ] gutenberg........... Project Gutenberg Selections\n",
      "  [ ] hmm_treebank_pos_tagger Treebank Part of Speech Tagger (HMM)\n",
      "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
      "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
      "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
      "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
      "                           ChaSen format)\n",
      "  [ ] kimmo............... PC-KIMMO Data Files\n",
      "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
      "Hit Enter to continue: \n",
      "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
      "                           for parser comparison\n",
      "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
      "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
      "                           part-of-speech tags\n",
      "  [ ] machado............. Machado de Assis -- Obra Completa\n",
      "  [ ] masc_tagged......... MASC Tagged Corpus\n",
      "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
      "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
      "  [ ] moses_sample........ Moses Sample Models\n",
      "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
      "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
      "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
      "                           2015) subset of the Paraphrase Database.\n",
      "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
      "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
      "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
      "  [ ] nps_chat............ NPS Chat\n",
      "  [ ] omw................. Open Multilingual Wordnet\n",
      "  [ ] opinion_lexicon..... Opinion Lexicon\n",
      "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
      "Hit Enter to continue: \n",
      "  [ ] paradigms........... Paradigm Corpus\n",
      "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
      "                           Evaluation Shared Task\n",
      "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
      "                           character properties in Perl\n",
      "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
      "  [ ] pl196x.............. Polish language of the XX century sixties\n",
      "  [ ] porter_test......... Porter Stemmer Test Files\n",
      "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
      "  [ ] problem_reports..... Problem Report Corpus\n",
      "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
      "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
      "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
      "  [ ] pros_cons........... Pros and Cons\n",
      "  [ ] ptb................. Penn Treebank\n",
      "  [ ] punkt............... Punkt Tokenizer Models\n",
      "  [ ] qc.................. Experimental Data for Question Classification\n",
      "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
      "                           version\n",
      "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
      "                           Portuguesa)\n",
      "Hit Enter to continue: \n",
      "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
      "  [ ] sample_grammars..... Sample Grammars\n",
      "  [ ] semcor.............. SemCor 3.0\n",
      "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
      "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
      "  [ ] sentiwordnet........ SentiWordNet\n",
      "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
      "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
      "  [ ] smultron............ SMULTRON Corpus Sample\n",
      "  [ ] snowball_data....... Snowball Data\n",
      "  [ ] spanish_grammars.... Grammars for Spanish\n",
      "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
      "  [ ] stopwords........... Stopwords Corpus\n",
      "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
      "  [ ] swadesh............. Swadesh Wordlists\n",
      "  [ ] switchboard......... Switchboard Corpus Sample\n",
      "  [ ] tagsets............. Help on Tagsets\n",
      "  [ ] timit............... TIMIT Corpus Sample\n",
      "  [ ] toolbox............. Toolbox Sample Files\n",
      "  [ ] treebank............ Penn Treebank Sample\n",
      "  [ ] twitter_samples..... Twitter Samples\n",
      "Hit Enter to continue: \n",
      "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
      "                           (Unicode Version)\n",
      "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
      "  [ ] unicode_samples..... Unicode Samples\n",
      "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
      "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
      "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
      "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
      "  [ ] webtext............. Web Text Corpus\n",
      "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
      "  [ ] word2vec_sample..... Word2Vec Sample\n",
      "  [ ] wordnet............. WordNet\n",
      "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
      "  [ ] words............... Word Lists\n",
      "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
      "                           English Prose\n",
      "\n",
      "Collections:\n",
      "  [ ] all-corpora......... All the corpora\n",
      "  [ ] all-nltk............ All packages available on nltk_data gh-pages\n",
      "                           branch\n",
      "  [ ] all................. All packages\n",
      "Hit Enter to continue: \n",
      "  [ ] book................ Everything used in the NLTK Book\n",
      "  [ ] popular............. Popular packages\n",
      "  [ ] third-party......... Third-party data packages\n",
      "\n",
      "([*] marks installed packages)\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> \n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> wordnet\n",
      "Command 'wordnet' unrecognized\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> download\n",
      "Command 'download' unrecognized\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> d\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> wordnet\n",
      "    Downloading package wordnet to /root/nltk_data...\n",
      "      Unzipping corpora/wordnet.zip.\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-fec50ed38a60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda/lib/python2.7/site-packages/nltk/downloader.pyc\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error)\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;31m# function should make a new copy of self to use?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdownload_dir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interactive_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python2.7/site-packages/nltk/downloader.pyc\u001b[0m in \u001b[0;36m_interactive_download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    974\u001b[0m                 \u001b[0mDownloaderGUI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTclError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m                 \u001b[0mDownloaderShell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    977\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m             \u001b[0mDownloaderShell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python2.7/site-packages/nltk/downloader.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    996\u001b[0m             self._simple_interactive_menu(\n\u001b[1;32m    997\u001b[0m                 'd) Download', 'l) List', ' u) Update', 'c) Config', 'h) Help', 'q) Quit')\n\u001b[0;32m--> 998\u001b[0;31m             \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Downloader> '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    999\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m             \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.pyc\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.pyc\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    722\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# END STUDENT CODE 5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top/Bottom 20 results - Similarity measures - sorted by cosine\n",
      "(From the entire data set)\n",
      "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "                          pair |         cosine |        jaccard |        overlap |           dice |        average\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "            needful - vanished |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "          clusters - ingenuity |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         intolerable - parting |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      " infinitesimal - probabilities |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "             graceful - relish |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "              cyrus - preamble |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "       inhibition - inhibitors |       0.707107 |       0.500000 |       1.000000 |       0.666667 |       0.718443\n",
      "       conditioned - serotonin |       0.707107 |       0.500000 |       1.000000 |       0.666667 |       0.718443\n",
      "               mock - vanished |       0.707107 |       0.500000 |       1.000000 |       0.666667 |       0.718443\n",
      "                mock - needful |       0.707107 |       0.500000 |       1.000000 |       0.666667 |       0.718443\n",
      "               parium - suorum |       0.666667 |       0.500000 |       0.666667 |       0.666667 |       0.625000\n",
      "               legale - suorum |       0.666667 |       0.500000 |       0.666667 |       0.666667 |       0.625000\n",
      "               legale - parium |       0.666667 |       0.500000 |       0.666667 |       0.666667 |       0.625000\n",
      "             judicium - suorum |       0.666667 |       0.500000 |       0.666667 |       0.666667 |       0.625000\n",
      "             judicium - parium |       0.666667 |       0.500000 |       0.666667 |       0.666667 |       0.625000\n",
      "             judicium - legale |       0.666667 |       0.500000 |       0.666667 |       0.666667 |       0.625000\n",
      "          falsehood - insulted |       0.577350 |       0.333333 |       1.000000 |       0.500000 |       0.602671\n",
      "             coolidge - hoover |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "               calvin - hoover |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "             calvin - coolidge |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "       inhibition - inhibitors |       0.707107 |       0.500000 |       1.000000 |       0.666667 |       0.718443\n",
      "       conditioned - serotonin |       0.707107 |       0.500000 |       1.000000 |       0.666667 |       0.718443\n",
      "               mock - vanished |       0.707107 |       0.500000 |       1.000000 |       0.666667 |       0.718443\n",
      "                mock - needful |       0.707107 |       0.500000 |       1.000000 |       0.666667 |       0.718443\n",
      "               parium - suorum |       0.666667 |       0.500000 |       0.666667 |       0.666667 |       0.625000\n",
      "               legale - suorum |       0.666667 |       0.500000 |       0.666667 |       0.666667 |       0.625000\n",
      "               legale - parium |       0.666667 |       0.500000 |       0.666667 |       0.666667 |       0.625000\n",
      "             judicium - suorum |       0.666667 |       0.500000 |       0.666667 |       0.666667 |       0.625000\n",
      "             judicium - parium |       0.666667 |       0.500000 |       0.666667 |       0.666667 |       0.625000\n",
      "             judicium - legale |       0.666667 |       0.500000 |       0.666667 |       0.666667 |       0.625000\n",
      "          falsehood - insulted |       0.577350 |       0.333333 |       1.000000 |       0.500000 |       0.602671\n",
      "             coolidge - hoover |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "               calvin - hoover |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "             calvin - coolidge |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           anciennes - recueil |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "              anciennes - lois |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           malaysian - mohamad |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "            mahathir - mohamad |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "          mahathir - malaysian |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "                lois - recueil |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n"
     ]
    }
   ],
   "source": [
    "print \"\\nTop/Bottom 20 results - Similarity measures - sorted by cosine\"\n",
    "print \"(From the entire data set)\"\n",
    "print '—'*117\n",
    "print \"{0:>30} |{1:>15} |{2:>15} |{3:>15} |{4:>15} |{5:>15}\".format(\n",
    "        \"pair\", \"cosine\", \"jaccard\", \"overlap\", \"dice\", \"average\")\n",
    "print '-'*117\n",
    "\n",
    "for stripe in sortedSims[:20]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )\n",
    "\n",
    "print '—'*117\n",
    "\n",
    "for stripe in sortedSims[-20:]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Top/Bottom 20 results - Similarity measures - sorted by cosine\n",
    "(From the entire data set)\n",
    "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "                          pair |         cosine |        jaccard |        overlap |           dice |        average\n",
    "---------------------------------------------------------------------------------------------------------------------\n",
    "                   cons - pros |       0.894427 |       0.800000 |       1.000000 |       0.888889 |       0.895829\n",
    "            forties - twenties |       0.816497 |       0.666667 |       1.000000 |       0.800000 |       0.820791\n",
    "                    own - time |       0.809510 |       0.670563 |       0.921168 |       0.802799 |       0.801010\n",
    "                 little - time |       0.784197 |       0.630621 |       0.926101 |       0.773473 |       0.778598\n",
    "                  found - time |       0.783434 |       0.636364 |       0.883788 |       0.777778 |       0.770341\n",
    "                 nova - scotia |       0.774597 |       0.600000 |       1.000000 |       0.750000 |       0.781149\n",
    "                   hong - kong |       0.769800 |       0.615385 |       0.888889 |       0.761905 |       0.758995\n",
    "                   life - time |       0.769666 |       0.608789 |       0.925081 |       0.756829 |       0.765091\n",
    "                  time - world |       0.755476 |       0.585049 |       0.937500 |       0.738209 |       0.754058\n",
    "                  means - time |       0.752181 |       0.587117 |       0.902597 |       0.739854 |       0.745437\n",
    "                   form - time |       0.749943 |       0.588418 |       0.876733 |       0.740885 |       0.738995\n",
    "       infarction - myocardial |       0.748331 |       0.560000 |       1.000000 |       0.717949 |       0.756570\n",
    "                 people - time |       0.745788 |       0.573577 |       0.923875 |       0.729010 |       0.743063\n",
    "                 angeles - los |       0.745499 |       0.586207 |       0.850000 |       0.739130 |       0.730209\n",
    "                  little - own |       0.739343 |       0.585834 |       0.767296 |       0.738834 |       0.707827\n",
    "                    life - own |       0.737053 |       0.582217 |       0.778502 |       0.735951 |       0.708430\n",
    "          anterior - posterior |       0.733388 |       0.576471 |       0.790323 |       0.731343 |       0.707881\n",
    "                  power - time |       0.719611 |       0.533623 |       0.933586 |       0.695898 |       0.720680\n",
    "              dearly - install |       0.707107 |       0.500000 |       1.000000 |       0.666667 |       0.718443\n",
    "                   found - own |       0.704802 |       0.544134 |       0.710949 |       0.704776 |       0.666165\n",
    "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "           arrival - essential |       0.008258 |       0.004098 |       0.009615 |       0.008163 |       0.007534\n",
    "         governments - surface |       0.008251 |       0.003534 |       0.014706 |       0.007042 |       0.008383\n",
    "                king - lesions |       0.008178 |       0.003106 |       0.017857 |       0.006192 |       0.008833\n",
    "              clinical - stood |       0.008178 |       0.003831 |       0.011905 |       0.007634 |       0.007887\n",
    "               till - validity |       0.008172 |       0.003367 |       0.015625 |       0.006711 |       0.008469\n",
    "            evidence - started |       0.008159 |       0.003802 |       0.012048 |       0.007576 |       0.007896\n",
    "               forces - record |       0.008152 |       0.003876 |       0.011364 |       0.007722 |       0.007778\n",
    "               primary - stone |       0.008146 |       0.004065 |       0.009091 |       0.008097 |       0.007350\n",
    "             beneath - federal |       0.008134 |       0.004082 |       0.008403 |       0.008130 |       0.007187\n",
    "                factors - rose |       0.008113 |       0.004032 |       0.009346 |       0.008032 |       0.007381\n",
    "           evening - functions |       0.008069 |       0.004049 |       0.008333 |       0.008065 |       0.007129\n",
    "                   bone - told |       0.008061 |       0.003704 |       0.012346 |       0.007380 |       0.007873\n",
    "             building - occurs |       0.008002 |       0.003891 |       0.010309 |       0.007752 |       0.007489\n",
    "                 company - fig |       0.007913 |       0.003257 |       0.015152 |       0.006494 |       0.008204\n",
    "               chronic - north |       0.007803 |       0.003268 |       0.014493 |       0.006515 |       0.008020\n",
    "             evaluation - king |       0.007650 |       0.003030 |       0.015625 |       0.006042 |       0.008087\n",
    "             resulting - stood |       0.007650 |       0.003663 |       0.010417 |       0.007299 |       0.007257\n",
    "                 agent - round |       0.007515 |       0.003289 |       0.012821 |       0.006557 |       0.007546\n",
    "         afterwards - analysis |       0.007387 |       0.003521 |       0.010204 |       0.007018 |       0.007032\n",
    "            posterior - spirit |       0.007156 |       0.002660 |       0.016129 |       0.005305 |       0.007812"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.6  <a name=\"5.6\"></a> Evaluation of synonyms that your discovered\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "\n",
    "In this part of the assignment you will evaluate the success of you synonym detector (developed in response to HW5.4).\n",
    "Take the top 1,000 closest/most similar/correlative pairs of words as determined by your measure in HW5.4, and use the synonyms function in the accompanying python code:\n",
    "\n",
    "nltk_synonyms.py\n",
    "\n",
    "Note: This will require installing the python nltk package:\n",
    "\n",
    "http://www.nltk.org/install.html\n",
    "\n",
    "and downloading its data with nltk.download().\n",
    "\n",
    "For each (word1,word2) pair, check to see if word1 is in the list, \n",
    "synonyms(word2), and vice-versa. If one of the two is a synonym of the other, \n",
    "then consider this pair a 'hit', and then report the precision, recall, and F1 measure  of \n",
    "your detector across your 1,000 best guesses. Report the macro averages of these measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Calculate performance measures:\n",
    "$$Precision (P) = \\frac{TP}{TP + FP} $$  \n",
    "$$Recall (R) = \\frac{TP}{TP + FN} $$  \n",
    "$$F1 = \\frac{2 * ( precision * recall )}{precision + recall}$$\n",
    "\n",
    "\n",
    "We calculate Precision by counting the number of hits and dividing by the number of occurances in our top1000 (opportunities)   \n",
    "We calculate Recall by counting the number of hits, and dividing by the number of synonyms in wordnet (syns)\n",
    "\n",
    "\n",
    "Other diagnostic measures not implemented here:  https://en.wikipedia.org/wiki/F1_score#Diagnostic_Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Number of Hits: 0 out of top 26\n",
      "Number of words without synonyms: 22\n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Precision\t0.0\n",
      "Recall\t\t0.0\n",
      "F1\t\t0.0\n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Words without synonyms:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[] parium\n",
      "[] suorum\n",
      "[] legale\n",
      "[] suorum\n",
      "[] legale\n",
      "[] parium\n",
      "[] judicium\n",
      "[] suorum\n",
      "[] judicium\n",
      "[] parium\n",
      "[] judicium\n",
      "[] legale\n",
      "[] anciennes\n",
      "[] recueil\n",
      "[] anciennes\n",
      "[] lois\n",
      "[] mohamad\n",
      "[] mahathir\n",
      "[] mohamad\n",
      "[] mahathir\n",
      "[] lois\n",
      "[] recueil\n"
     ]
    }
   ],
   "source": [
    "''' Performance measures '''\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "\n",
    "#print all the synset element of an element\n",
    "def synonyms(string):\n",
    "    syndict = {}\n",
    "    for i,j in enumerate(wn.synsets(string)):\n",
    "        syns = j.lemma_names()\n",
    "        for syn in syns:\n",
    "            syndict.setdefault(syn,1)\n",
    "    return syndict.keys()\n",
    "hits = []\n",
    "\n",
    "TP = 0\n",
    "FP = 0\n",
    "\n",
    "TOTAL = 0\n",
    "flag = False # so we don't double count, but at the same time don't miss hits\n",
    "start_time = time.time()\n",
    "top1000sims = []\n",
    "with open(\"sims2/top1000sims\",\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "\n",
    "        line = line.strip()\n",
    "        avg,lisst = line.split(\"\\t\")\n",
    "        lisst = json.loads(lisst)\n",
    "        lisst.append(avg)\n",
    "        top1000sims.append(lisst)\n",
    "    \n",
    "\n",
    "measures = {}\n",
    "not_in_wordnet = []\n",
    "\n",
    "for line in top1000sims:\n",
    "    TOTAL += 1\n",
    "\n",
    "    pair = line[0]\n",
    "    words = pair.split(\" - \")\n",
    "    \n",
    "    for word in words:\n",
    "        if word not in measures:\n",
    "            measures[word] = {\"syns\":0,\"opps\": 0,\"hits\":0}\n",
    "        measures[word][\"opps\"] += 1 \n",
    "    \n",
    "    syns0 = synonyms(words[0])\n",
    "    measures[words[1]][\"syns\"] = len(syns0)\n",
    "    if len(syns0) == 0:\n",
    "        not_in_wordnet.append(words[0])\n",
    "        \n",
    "    if words[1] in syns0:\n",
    "        TP += 1\n",
    "        hits.append(line)\n",
    "        flag = True\n",
    "        measures[words[1]][\"hits\"] += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "    syns1 = synonyms(words[1]) \n",
    "    measures[words[0]][\"syns\"] = len(syns1)\n",
    "    if len(syns1) == 0:\n",
    "        not_in_wordnet.append(words[1])\n",
    "\n",
    "    if words[0] in syns1:\n",
    "        if flag == False:\n",
    "            TP += 1\n",
    "            hits.append(line)\n",
    "            measures[words[0]][\"hits\"] += 1\n",
    "            \n",
    "    flag = False    \n",
    "\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for key in measures:\n",
    "    p,r,f = 0,0,0\n",
    "    if measures[key][\"hits\"] > 0 and measures[key][\"syns\"] > 0:\n",
    "        p = measures[key][\"hits\"]/measures[key][\"opps\"]\n",
    "        r = measures[key][\"hits\"]/measures[key][\"syns\"]\n",
    "        f = 2 * (p*r)/(p+r)\n",
    "    \n",
    "    # For calculating measures, only take into account words that have synonyms in wordnet\n",
    "    if measures[key][\"syns\"] > 0:\n",
    "        precision.append(p)\n",
    "        recall.append(r)\n",
    "        f1.append(f)\n",
    "\n",
    "    \n",
    "# Take the mean of each measure    \n",
    "print \"—\"*110    \n",
    "print \"Number of Hits:\",TP, \"out of top\",TOTAL\n",
    "print \"Number of words without synonyms:\",len(not_in_wordnet)\n",
    "print \"—\"*110 \n",
    "print \"Precision\\t\", np.mean(precision)\n",
    "print \"Recall\\t\\t\", np.mean(recall)\n",
    "print \"F1\\t\\t\", np.mean(f1)\n",
    "print \"—\"*110  \n",
    "\n",
    "print \"Words without synonyms:\"\n",
    "print \"-\"*100\n",
    "\n",
    "for word in not_in_wordnet:\n",
    "    print synonyms(word),word\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "mins = elapsed_time/float(60)\n",
    "a = \"\"\"Elapsed time: %s seconds\n",
    "In minutes: %s mins\"\"\" % (str(elapsed_time), str(mins))\n",
    "logging.warning(a)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "Number of Hits: 31 out of top 1000\n",
    "Number of words without synonyms: 67\n",
    "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "Precision\t0.0280214404967\n",
    "Recall\t\t0.0178598869579\n",
    "F1\t\t0.013965517619\n",
    "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "Words without synonyms:\n",
    "----------------------------------------------------------------------------------------------------\n",
    "[] scotia\n",
    "[] hong\n",
    "[] kong\n",
    "[] angeles\n",
    "[] los\n",
    "[] nor\n",
    "[] themselves\n",
    "[] \n",
    "......."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.7  <a name=\"5.7\"></a> OPTIONAL: using different vocabulary subsets\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "\n",
    "Repeat HW5 using vocabulary words ranked from 8001,-10,000;  7001,-10,000; 6001,-10,000; 5001,-10,000; 3001,-10,000; and 1001,-10,000;\n",
    "Dont forget to report you Cluster configuration.\n",
    "\n",
    "Generate the following graphs:\n",
    "-- vocabulary size (X-Axis) versus CPU time for indexing\n",
    "-- vocabulary size (X-Axis) versus number of pairs processed\n",
    "-- vocabulary size (X-Axis) versus F1 measure, Precision, Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7: Vocabulary subset 8001-10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `frequencies5.5': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/frequencies5_5.chqngh.20170619.052208.461478\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/chqngh/tmp/mrjob/frequencies5_5.chqngh.20170619.052208.461478/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob6237865832262829260.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1493936954640_1601\n",
      "  Submitted application application_1493936954640_1601\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1493936954640_1601/\n",
      "  Running job: job_1493936954640_1601\n",
      "  Job job_1493936954640_1601 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "  Task Id : attempt_1493936954640_1601_m_000001_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:322)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:535)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1719)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n",
      "\n",
      "Container killed by the ApplicationMaster.\n",
      "Container killed on request. Exit code is 143\n",
      "Container exited with a non-zero exit code 143\n",
      "\n",
      "  Task Id : attempt_1493936954640_1601_m_000000_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:322)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:535)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1719)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n",
      "\n",
      "  Task Id : attempt_1493936954640_1601_m_000001_1, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@5a44c9f2 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1601_m_000000_1, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@14197f5b rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1601_m_000001_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:322)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:535)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1719)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n",
      "\n",
      "  Task Id : attempt_1493936954640_1601_m_000000_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:322)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:535)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1719)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n",
      "\n",
      "   map 100% reduce 100%\n",
      "  Job job_1493936954640_1601 failed with state FAILED due to: Task failed task_1493936954640_1601_m_000001\n",
      "Job failed as tasks failed. failedMaps:1 failedReduces:0\n",
      "\n",
      "  Job not successful!\n",
      "  Streaming Command Failed!\n",
      "Counters: 17\n",
      "\tJob Counters \n",
      "\t\tFailed map tasks=7\n",
      "\t\tKilled map tasks=1\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=8\n",
      "\t\tOther local map tasks=6\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=41326080\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=0\n",
      "\t\tTotal time spent by all map tasks (ms)=26905\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=80715\n",
      "\t\tTotal time spent by all reduce tasks (ms)=0\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=26905\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "Scanning logs for probable cause of failure...\n",
      "Probable cause of failure:\n",
      "\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:322)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:535)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1719)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n",
      "\n",
      "Step 1 of 2 failed: Command '['/opt/hadoop/bin/hadoop', 'jar', '/opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar', '-files', 'hdfs:///user/chqngh/tmp/mrjob/frequencies5_5.chqngh.20170619.052208.461478/files/frequencies5_5.py#frequencies5_5.py,hdfs:///user/chqngh/tmp/mrjob/frequencies5_5.chqngh.20170619.052208.461478/files/setup-wrapper.sh#setup-wrapper.sh', '-archives', 'hdfs:///user/chqngh/tmp/mrjob/frequencies5_5.chqngh.20170619.052208.461478/files/mrjob.tar.gz#mrjob.tar.gz,hdfs:///user/chqngh/virtualenv/hw5_test2.zip#hw5_test2', '-D', 'mapred.text.key.partitioner.options=-k1,1', '-D', 'mapreduce.partition.keypartitioner.options=-k1,1', '-D', 'stream.num.map.output.key.fields=2', '-partitioner', 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner', '-input', 'hdfs:///user/chqngh/tmp/mrjob/frequencies5_5.chqngh.20170619.052208.461478/files/googlebooks-eng-all-5gram-20090715-0-filtered.txt', '-output', 'hdfs:///user/chqngh/tmp/mrjob/frequencies5_5.chqngh.20170619.052208.461478/step-output/0000', '-mapper', 'sh -ex setup-wrapper.sh ./hw5_test2/bin/python frequencies5_5.py --step-num=0 --mapper --max_rank 10000 --min_rank 8001', '-combiner', 'sh -ex setup-wrapper.sh ./hw5_test2/bin/python frequencies5_5.py --step-num=0 --combiner --max_rank 10000 --min_rank 8001', '-reducer', 'sh -ex setup-wrapper.sh ./hw5_test2/bin/python frequencies5_5.py --step-num=0 --reducer --max_rank 10000 --min_rank 8001']' returned non-zero exit status 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `stripes5.5': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/stripes5_5.chqngh.20170619.052312.526502\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/chqngh/tmp/mrjob/stripes5_5.chqngh.20170619.052312.526502/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob1560699653353409827.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1493936954640_1603\n",
      "  Submitted application application_1493936954640_1603\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1493936954640_1603/\n",
      "  Running job: job_1493936954640_1603\n",
      "  Job job_1493936954640_1603 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "  Task Id : attempt_1493936954640_1603_m_000001_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@45c8cd92 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1603_m_000000_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@45c8cd92 rejected from java.util.concurrent.ThreadPoolExecutor@1c533e8d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 335]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1603_m_000000_1001, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:322)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:535)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1719)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n",
      "\n",
      "Container killed by the ApplicationMaster.\n",
      "Container killed on request. Exit code is 143\n",
      "Container exited with a non-zero exit code 143\n",
      "\n",
      "  Task Id : attempt_1493936954640_1603_m_000001_1001, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:322)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:535)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1719)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n",
      "\n",
      "  Task Id : attempt_1493936954640_1603_m_000000_1002, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:322)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:535)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1719)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n",
      "\n",
      "Container killed by the ApplicationMaster.\n",
      "Container killed on request. Exit code is 143\n",
      "Container exited with a non-zero exit code 143\n",
      "\n",
      "  Task Id : attempt_1493936954640_1603_m_000001_1002, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@22b36c22 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "   map 100% reduce 100%\n",
      "  Job job_1493936954640_1603 failed with state FAILED due to: Task failed task_1493936954640_1603_m_000000\n",
      "Job failed as tasks failed. failedMaps:1 failedReduces:0\n",
      "\n",
      "  Job not successful!\n",
      "  Streaming Command Failed!\n",
      "Counters: 17\n",
      "\tJob Counters \n",
      "\t\tFailed map tasks=7\n",
      "\t\tKilled map tasks=1\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=8\n",
      "\t\tOther local map tasks=6\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=32632320\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=0\n",
      "\t\tTotal time spent by all map tasks (ms)=21245\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=63735\n",
      "\t\tTotal time spent by all reduce tasks (ms)=0\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=21245\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "Scanning logs for probable cause of failure...\n",
      "Probable cause of failure:\n",
      "\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@22b36c22 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "Step 1 of 1 failed: Command '['/opt/hadoop/bin/hadoop', 'jar', '/opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar', '-files', 'hdfs:///user/chqngh/tmp/mrjob/stripes5_5.chqngh.20170619.052312.526502/files/setup-wrapper.sh#setup-wrapper.sh,hdfs:///user/chqngh/tmp/mrjob/stripes5_5.chqngh.20170619.052312.526502/files/frequencies5.5#frequencies5.5,hdfs:///user/chqngh/tmp/mrjob/stripes5_5.chqngh.20170619.052312.526502/files/stripes5_5.py#stripes5_5.py', '-archives', 'hdfs:///user/chqngh/tmp/mrjob/stripes5_5.chqngh.20170619.052312.526502/files/mrjob.tar.gz#mrjob.tar.gz,hdfs:///user/chqngh/virtualenv/hw5_test2.zip#hw5_test2', '-D', 'mapred.text.key.partitioner.options=-k1,1', '-D', 'mapreduce.partition.keypartitioner.options=-k1,1', '-D', 'stream.num.map.output.key.fields=2', '-partitioner', 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner', '-input', 'hdfs:///user/chqngh/tmp/mrjob/stripes5_5.chqngh.20170619.052312.526502/files/googlebooks-eng-all-5gram-20090715-0-filtered.txt', '-output', 'hdfs:///user/chqngh/tmp/mrjob/stripes5_5.chqngh.20170619.052312.526502/output', '-mapper', 'sh -ex setup-wrapper.sh ./hw5_test2/bin/python stripes5_5.py --step-num=0 --mapper', '-reducer', 'sh -ex setup-wrapper.sh ./hw5_test2/bin/python stripes5_5.py --step-num=0 --reducer']' returned non-zero exit status 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `index5.5': No such file or directory\n",
      "python: can't open file 'index5_5.py': [Errno 2] No such file or directory\n",
      "rm: `similarity5.5': No such file or directory\n",
      "python: can't open file 'similarity5_5.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r frequencies5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5\n",
    "!python frequencies5_5.py --min_rank 8001 --max_rank 10000 -r hadoop \\\n",
    "    googlebooks-eng-all-5gram-20090715-0-filtered.txt \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > frequencies5.5\n",
    "\n",
    "!hdfs dfs -rm -r stripes5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python stripes5_5.py --file=frequencies5.5 -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered.txt \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > stripes5.5\n",
    "\n",
    "!hdfs dfs -rm -r index5.5\n",
    "!python index5_5.py -r hadoop stripes5.5 \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > index5.5\n",
    "\n",
    "!hdfs dfs -rm -r similarity5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python similarity5_5.py -r hadoop index5.5 \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > similarity5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "sortedSims = []\n",
    "with open(\"similarity5.5\",\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "\n",
    "        line = line.strip()\n",
    "        avg,lisst = line.split(\"\\t\")\n",
    "        lisst = json.loads(lisst)\n",
    "        lisst.append(avg)\n",
    "        sortedSims.append(lisst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory `sims2': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir sims2\n",
    "!head -1000 similarity5.5 > sims2/top1000sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top/Bottom 20 results - Similarity measures - sorted by cosine\n",
      "(From the entire data set)\n",
      "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "                          pair |         cosine |        jaccard |        overlap |           dice |        average\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "           iliac - replacement |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         replacement - rupture |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           hampshire - hungary |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "       greenhouse - tomography |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           grazing - livestock |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            gracious - privacy |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           gracious - pendulum |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "                   genre - que |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "        precaution - resultant |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         preamble - toleration |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           preamble - sweeping |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "               plunged - slain |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            planting - players |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "             pilgrim - proudly |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            pendulum - privacy |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            offender - schemes |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "          newborn - saturation |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         sweeping - toleration |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            needful - vanished |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "                 nazis - swift |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "               alpha - epsilon |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "        maxillary - neuropathy |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "     subordination - vengeance |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "              seas - wandering |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "              pious - syllable |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "       neuropathy - trigeminal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "        maxillary - trigeminal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           malaysian - mohamad |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "            mahathir - mohamad |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "          mahathir - malaysian |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "                lois - recueil |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "            ingenuity - wretch |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           infected - infusion |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "               hers - paternal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           fossil - resembling |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "          sensory - trigeminal |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "           maxillary - sensory |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "        breathed - persistence |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "            diabetic - sensory |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "                anne - prussia |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n"
     ]
    }
   ],
   "source": [
    "print \"\\nTop/Bottom 20 results - Similarity measures - sorted by cosine\"\n",
    "print \"(From the entire data set)\"\n",
    "print '—'*117\n",
    "print \"{0:>30} |{1:>15} |{2:>15} |{3:>15} |{4:>15} |{5:>15}\".format(\n",
    "        \"pair\", \"cosine\", \"jaccard\", \"overlap\", \"dice\", \"average\")\n",
    "print '-'*117\n",
    "\n",
    "for stripe in sortedSims[:20]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )\n",
    "\n",
    "print '—'*117\n",
    "\n",
    "for stripe in sortedSims[-20:]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7: Vocabulary subset 7001-10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r frequencies5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5\n",
    "!python frequencies5_5.py --min_rank 7001 --max_rank 10000 -r hadoop \\\n",
    "    googlebooks-eng-all-5gram-20090715-0-filtered.txt \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > frequencies5.5\n",
    "\n",
    "!hdfs dfs -rm -r stripes5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python stripes5_5.py --file=frequencies5.5 -r hadoop \\\n",
    "    googlebooks-eng-all-5gram-20090715-0-filtered.txt \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > stripes5.5\n",
    "\n",
    "!hdfs dfs -rm -r index5.5\n",
    "!python index5_5.py -r hadoop stripes5.5 \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > index5.5\n",
    "\n",
    "!hdfs dfs -rm -r similarity5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python similarity5_5.py -r hadoop index5.5 \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > similarity5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "sortedSims = []\n",
    "with open(\"similarity5.5\",\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "\n",
    "        line = line.strip()\n",
    "        avg,lisst = line.split(\"\\t\")\n",
    "        lisst = json.loads(lisst)\n",
    "        lisst.append(avg)\n",
    "        sortedSims.append(lisst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory `sims2': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir sims2\n",
    "!head -1000 similarity5.5 > sims2/top1000sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top/Bottom 20 results - Similarity measures - sorted by cosine\n",
      "(From the entire data set)\n",
      "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "                          pair |         cosine |        jaccard |        overlap |           dice |        average\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "           iliac - replacement |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         replacement - rupture |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           hampshire - hungary |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "       greenhouse - tomography |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           grazing - livestock |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            gracious - privacy |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           gracious - pendulum |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "                   genre - que |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "        precaution - resultant |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         preamble - toleration |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           preamble - sweeping |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "               plunged - slain |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            planting - players |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "             pilgrim - proudly |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            pendulum - privacy |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            offender - schemes |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "          newborn - saturation |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         sweeping - toleration |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            needful - vanished |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "                 nazis - swift |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "               alpha - epsilon |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "        maxillary - neuropathy |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "     subordination - vengeance |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "              seas - wandering |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "              pious - syllable |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "       neuropathy - trigeminal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "        maxillary - trigeminal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           malaysian - mohamad |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "            mahathir - mohamad |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "          mahathir - malaysian |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "                lois - recueil |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "            ingenuity - wretch |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           infected - infusion |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "               hers - paternal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           fossil - resembling |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "          sensory - trigeminal |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "           maxillary - sensory |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "        breathed - persistence |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "            diabetic - sensory |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "                anne - prussia |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n"
     ]
    }
   ],
   "source": [
    "print \"\\nTop/Bottom 20 results - Similarity measures - sorted by cosine\"\n",
    "print \"(From the entire data set)\"\n",
    "print '—'*117\n",
    "print \"{0:>30} |{1:>15} |{2:>15} |{3:>15} |{4:>15} |{5:>15}\".format(\n",
    "        \"pair\", \"cosine\", \"jaccard\", \"overlap\", \"dice\", \"average\")\n",
    "print '-'*117\n",
    "\n",
    "for stripe in sortedSims[:20]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )\n",
    "\n",
    "print '—'*117\n",
    "\n",
    "for stripe in sortedSims[-20:]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7: Vocabulary subset 6001-10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r frequencies5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5\n",
    "!python frequencies5_5.py --min_rank 6001 --max_rank 10000 -r hadoop \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    googlebooks-eng-all-5gram-20090715-0-filtered.txt > frequencies5.5\n",
    "\n",
    "!hdfs dfs -rm -r stripes5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python stripes5_5.py --file=frequencies5.5 -r hadoop \\\n",
    "    googlebooks-eng-all-5gram-20090715-0-filtered.txt \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > stripes5.5\n",
    "\n",
    "!hdfs dfs -rm -r index5.5\n",
    "!python index5_5.py -r hadoop stripes5.5 \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > index5.5\n",
    "\n",
    "!hdfs dfs -rm -r similarity5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python similarity5_5.py -r hadoop index5.5 \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > similarity5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "sortedSims = []\n",
    "with open(\"similarity5.5\",\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "\n",
    "        line = line.strip()\n",
    "        avg,lisst = line.split(\"\\t\")\n",
    "        lisst = json.loads(lisst)\n",
    "        lisst.append(avg)\n",
    "        sortedSims.append(lisst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory `sims2': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir sims2\n",
    "!head -1000 similarity5.5 > sims2/top1000sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top/Bottom 20 results - Similarity measures - sorted by cosine\n",
      "(From the entire data set)\n",
      "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "                          pair |         cosine |        jaccard |        overlap |           dice |        average\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "           iliac - replacement |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         replacement - rupture |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           hampshire - hungary |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "       greenhouse - tomography |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           grazing - livestock |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            gracious - privacy |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           gracious - pendulum |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "                   genre - que |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "        precaution - resultant |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         preamble - toleration |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           preamble - sweeping |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "               plunged - slain |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            planting - players |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "             pilgrim - proudly |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            pendulum - privacy |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            offender - schemes |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "          newborn - saturation |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         sweeping - toleration |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            needful - vanished |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "                 nazis - swift |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "               alpha - epsilon |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "        maxillary - neuropathy |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "     subordination - vengeance |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "              seas - wandering |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "              pious - syllable |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "       neuropathy - trigeminal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "        maxillary - trigeminal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           malaysian - mohamad |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "            mahathir - mohamad |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "          mahathir - malaysian |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "                lois - recueil |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "            ingenuity - wretch |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           infected - infusion |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "               hers - paternal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           fossil - resembling |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "          sensory - trigeminal |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "           maxillary - sensory |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "        breathed - persistence |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "            diabetic - sensory |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "                anne - prussia |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n"
     ]
    }
   ],
   "source": [
    "print \"\\nTop/Bottom 20 results - Similarity measures - sorted by cosine\"\n",
    "print \"(From the entire data set)\"\n",
    "print '—'*117\n",
    "print \"{0:>30} |{1:>15} |{2:>15} |{3:>15} |{4:>15} |{5:>15}\".format(\n",
    "        \"pair\", \"cosine\", \"jaccard\", \"overlap\", \"dice\", \"average\")\n",
    "print '-'*117\n",
    "\n",
    "for stripe in sortedSims[:20]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )\n",
    "\n",
    "print '—'*117\n",
    "\n",
    "for stripe in sortedSims[-20:]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7: Vocabulary subset 5001-10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r frequencies5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5\n",
    "!python frequencies5_5.py --min_rank 5001 --max_rank 10000 -r hadoop \\\n",
    "    googlebooks-eng-all-5gram-20090715-0-filtered.txt \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > frequencies5.5\n",
    "\n",
    "!hdfs dfs -rm -r stripes5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python stripes5_5.py --file=frequencies5.5 -r hadoop \\\n",
    "    googlebooks-eng-all-5gram-20090715-0-filtered.txt \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > stripes5.5\n",
    "\n",
    "!hdfs dfs -rm -r index5.5\n",
    "!python index5_5.py -r hadoop stripes5.5 \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > index5.5\n",
    "\n",
    "!hdfs dfs -rm -r similarity5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python similarity5_5.py -r hadoop index5.5 \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > similarity5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "sortedSims = []\n",
    "with open(\"similarity5.5\",\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "\n",
    "        line = line.strip()\n",
    "        avg,lisst = line.split(\"\\t\")\n",
    "        lisst = json.loads(lisst)\n",
    "        lisst.append(avg)\n",
    "        sortedSims.append(lisst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory `sims2': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir sims2\n",
    "!head -1000 similarity5.5 > sims2/top1000sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top/Bottom 20 results - Similarity measures - sorted by cosine\n",
      "(From the entire data set)\n",
      "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "                          pair |         cosine |        jaccard |        overlap |           dice |        average\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "           iliac - replacement |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         replacement - rupture |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           hampshire - hungary |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "       greenhouse - tomography |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           grazing - livestock |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            gracious - privacy |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           gracious - pendulum |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "                   genre - que |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "        precaution - resultant |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         preamble - toleration |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           preamble - sweeping |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "               plunged - slain |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            planting - players |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "             pilgrim - proudly |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            pendulum - privacy |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            offender - schemes |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "          newborn - saturation |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         sweeping - toleration |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            needful - vanished |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "                 nazis - swift |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "               alpha - epsilon |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "        maxillary - neuropathy |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "     subordination - vengeance |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "              seas - wandering |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "              pious - syllable |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "       neuropathy - trigeminal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "        maxillary - trigeminal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           malaysian - mohamad |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "            mahathir - mohamad |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "          mahathir - malaysian |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "                lois - recueil |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "            ingenuity - wretch |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           infected - infusion |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "               hers - paternal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           fossil - resembling |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "          sensory - trigeminal |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "           maxillary - sensory |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "        breathed - persistence |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "            diabetic - sensory |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "                anne - prussia |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n"
     ]
    }
   ],
   "source": [
    "print \"\\nTop/Bottom 20 results - Similarity measures - sorted by cosine\"\n",
    "print \"(From the entire data set)\"\n",
    "print '—'*117\n",
    "print \"{0:>30} |{1:>15} |{2:>15} |{3:>15} |{4:>15} |{5:>15}\".format(\n",
    "        \"pair\", \"cosine\", \"jaccard\", \"overlap\", \"dice\", \"average\")\n",
    "print '-'*117\n",
    "\n",
    "for stripe in sortedSims[:20]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )\n",
    "\n",
    "print '—'*117\n",
    "\n",
    "for stripe in sortedSims[-20:]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7: Vocabulary subset 4001-10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r frequencies5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5\n",
    "!python frequencies5_5.py --min_rank 4001 --max_rank 10000 -r \\\n",
    "    hadoop googlebooks-eng-all-5gram-20090715-0-filtered.txt \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > frequencies5.5\n",
    "\n",
    "!hdfs dfs -rm -r stripes5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python stripes5_5.py --file=frequencies5.5 -r \\\n",
    "    hadoop googlebooks-eng-all-5gram-20090715-0-filtered.txt \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > stripes5.5\n",
    "\n",
    "!hdfs dfs -rm -r index5.5\n",
    "!python index5_5.py -r hadoop stripes5.5 \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > index5.5\n",
    "\n",
    "!hdfs dfs -rm -r similarity5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python similarity5_5.py -r hadoop index5.5 \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > similarity5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "sortedSims = []\n",
    "with open(\"similarity5.5\",\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "\n",
    "        line = line.strip()\n",
    "        avg,lisst = line.split(\"\\t\")\n",
    "        lisst = json.loads(lisst)\n",
    "        lisst.append(avg)\n",
    "        sortedSims.append(lisst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory `sims2': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir sims2\n",
    "!head -1000 similarity5.5 > sims2/top1000sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top/Bottom 20 results - Similarity measures - sorted by cosine\n",
      "(From the entire data set)\n",
      "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "                          pair |         cosine |        jaccard |        overlap |           dice |        average\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "           iliac - replacement |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         replacement - rupture |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           hampshire - hungary |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "       greenhouse - tomography |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           grazing - livestock |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            gracious - privacy |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           gracious - pendulum |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "                   genre - que |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "        precaution - resultant |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         preamble - toleration |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           preamble - sweeping |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "               plunged - slain |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            planting - players |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "             pilgrim - proudly |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            pendulum - privacy |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            offender - schemes |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "          newborn - saturation |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         sweeping - toleration |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            needful - vanished |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "                 nazis - swift |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "               alpha - epsilon |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "        maxillary - neuropathy |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "     subordination - vengeance |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "              seas - wandering |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "              pious - syllable |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "       neuropathy - trigeminal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "        maxillary - trigeminal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           malaysian - mohamad |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "            mahathir - mohamad |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "          mahathir - malaysian |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "                lois - recueil |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "            ingenuity - wretch |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           infected - infusion |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "               hers - paternal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           fossil - resembling |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "          sensory - trigeminal |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "           maxillary - sensory |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "        breathed - persistence |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "            diabetic - sensory |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "                anne - prussia |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n"
     ]
    }
   ],
   "source": [
    "print \"\\nTop/Bottom 20 results - Similarity measures - sorted by cosine\"\n",
    "print \"(From the entire data set)\"\n",
    "print '—'*117\n",
    "print \"{0:>30} |{1:>15} |{2:>15} |{3:>15} |{4:>15} |{5:>15}\".format(\n",
    "        \"pair\", \"cosine\", \"jaccard\", \"overlap\", \"dice\", \"average\")\n",
    "print '-'*117\n",
    "\n",
    "for stripe in sortedSims[:20]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )\n",
    "\n",
    "print '—'*117\n",
    "\n",
    "for stripe in sortedSims[-20:]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7: Vocabulary subset 3001-10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r frequencies5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5\n",
    "!python frequencies5_5.py --min_rank 3001 --max_rank 10000 -r \\\n",
    "    hadoop googlebooks-eng-all-5gram-20090715-0-filtered.txt \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > frequencies5.5\n",
    "\n",
    "!hdfs dfs -rm -r stripes5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python stripes5_5.py --file=frequencies5.5 -r \\\n",
    "    hadoop googlebooks-eng-all-5gram-20090715-0-filtered.txt \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > stripes5.5\n",
    "\n",
    "!hdfs dfs -rm -r index5.5\n",
    "!python index5_5.py -r hadoop stripes5.5 \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > index5.5\n",
    "\n",
    "!hdfs dfs -rm -r similarity5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python similarity5_5.py -r hadoop index5.5 \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > similarity5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "sortedSims = []\n",
    "with open(\"similarity5.5\",\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "\n",
    "        line = line.strip()\n",
    "        avg,lisst = line.split(\"\\t\")\n",
    "        lisst = json.loads(lisst)\n",
    "        lisst.append(avg)\n",
    "        sortedSims.append(lisst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory `sims2': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir sims2\n",
    "!head -1000 similarity5.5 > sims2/top1000sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top/Bottom 20 results - Similarity measures - sorted by cosine\n",
      "(From the entire data set)\n",
      "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "                          pair |         cosine |        jaccard |        overlap |           dice |        average\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "           iliac - replacement |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         replacement - rupture |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           hampshire - hungary |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "       greenhouse - tomography |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           grazing - livestock |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            gracious - privacy |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           gracious - pendulum |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "                   genre - que |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "        precaution - resultant |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         preamble - toleration |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           preamble - sweeping |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "               plunged - slain |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            planting - players |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "             pilgrim - proudly |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            pendulum - privacy |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            offender - schemes |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "          newborn - saturation |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         sweeping - toleration |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            needful - vanished |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "                 nazis - swift |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "               alpha - epsilon |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "        maxillary - neuropathy |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "     subordination - vengeance |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "              seas - wandering |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "              pious - syllable |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "       neuropathy - trigeminal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "        maxillary - trigeminal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           malaysian - mohamad |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "            mahathir - mohamad |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "          mahathir - malaysian |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "                lois - recueil |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "            ingenuity - wretch |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           infected - infusion |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "               hers - paternal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           fossil - resembling |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "          sensory - trigeminal |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "           maxillary - sensory |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "        breathed - persistence |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "            diabetic - sensory |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "                anne - prussia |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n"
     ]
    }
   ],
   "source": [
    "print \"\\nTop/Bottom 20 results - Similarity measures - sorted by cosine\"\n",
    "print \"(From the entire data set)\"\n",
    "print '—'*117\n",
    "print \"{0:>30} |{1:>15} |{2:>15} |{3:>15} |{4:>15} |{5:>15}\".format(\n",
    "        \"pair\", \"cosine\", \"jaccard\", \"overlap\", \"dice\", \"average\")\n",
    "print '-'*117\n",
    "\n",
    "for stripe in sortedSims[:20]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )\n",
    "\n",
    "print '—'*117\n",
    "\n",
    "for stripe in sortedSims[-20:]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7: Vocabulary subset 2001-10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r frequencies5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5\n",
    "!python frequencies5_5.py --min_rank 2001 --max_rank 10000 -r \\\n",
    "    hadoop googlebooks-eng-all-5gram-20090715-0-filtered.txt \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > frequencies5.5\n",
    "\n",
    "!hdfs dfs -rm -r stripes5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python stripes5_5.py --file=frequencies5.5 -r hadoop \\\n",
    "    googlebooks-eng-all-5gram-20090715-0-filtered.txt \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > stripes5.5\n",
    "\n",
    "!hdfs dfs -rm -r index5.5\n",
    "!python index5_5.py -r hadoop stripes5.5 \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > index5.5\n",
    "\n",
    "!hdfs dfs -rm -r similarity5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python similarity5_5.py -r hadoop index5.5 \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > similarity5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "sortedSims = []\n",
    "with open(\"similarity5.5\",\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "\n",
    "        line = line.strip()\n",
    "        avg,lisst = line.split(\"\\t\")\n",
    "        lisst = json.loads(lisst)\n",
    "        lisst.append(avg)\n",
    "        sortedSims.append(lisst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory `sims2': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir sims2\n",
    "!head -1000 similarity5.5 > sims2/top1000sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top/Bottom 20 results - Similarity measures - sorted by cosine\n",
      "(From the entire data set)\n",
      "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "                          pair |         cosine |        jaccard |        overlap |           dice |        average\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "           iliac - replacement |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         replacement - rupture |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           hampshire - hungary |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "       greenhouse - tomography |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           grazing - livestock |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            gracious - privacy |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           gracious - pendulum |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "                   genre - que |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "        precaution - resultant |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         preamble - toleration |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           preamble - sweeping |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "               plunged - slain |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            planting - players |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "             pilgrim - proudly |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            pendulum - privacy |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            offender - schemes |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "          newborn - saturation |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         sweeping - toleration |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            needful - vanished |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "                 nazis - swift |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "               alpha - epsilon |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "        maxillary - neuropathy |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "     subordination - vengeance |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "              seas - wandering |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "              pious - syllable |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "       neuropathy - trigeminal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "        maxillary - trigeminal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           malaysian - mohamad |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "            mahathir - mohamad |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "          mahathir - malaysian |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "                lois - recueil |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "            ingenuity - wretch |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           infected - infusion |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "               hers - paternal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           fossil - resembling |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "          sensory - trigeminal |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "           maxillary - sensory |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "        breathed - persistence |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "            diabetic - sensory |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "                anne - prussia |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n"
     ]
    }
   ],
   "source": [
    "print \"\\nTop/Bottom 20 results - Similarity measures - sorted by cosine\"\n",
    "print \"(From the entire data set)\"\n",
    "print '—'*117\n",
    "print \"{0:>30} |{1:>15} |{2:>15} |{3:>15} |{4:>15} |{5:>15}\".format(\n",
    "        \"pair\", \"cosine\", \"jaccard\", \"overlap\", \"dice\", \"average\")\n",
    "print '-'*117\n",
    "\n",
    "for stripe in sortedSims[:20]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )\n",
    "\n",
    "print '—'*117\n",
    "\n",
    "for stripe in sortedSims[-20:]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7: Vocabulary subset 1001-10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r frequencies5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5\n",
    "!python frequencies5_5.py --min_rank 1001 --max_rank 10000 -r hadoop \\\n",
    "    googlebooks-eng-all-5gram-20090715-0-filtered.txt \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > frequencies5.5\n",
    "\n",
    "!hdfs dfs -rm -r stripes5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python stripes5_5.py --file=frequencies5.5 -r hadoop \\\n",
    "    googlebooks-eng-all-5gram-20090715-0-filtered.txt \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > stripes5.5\n",
    "\n",
    "!hdfs dfs -rm -r index5.5\n",
    "!python index5_5.py -r hadoop stripes5.5 \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > index5.5\n",
    "\n",
    "!hdfs dfs -rm -r similarity5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python similarity5_5.py -r hadoop index5.5 \\\n",
    "    --python-archive={pyArchive} --python-bin={pyBin} \\\n",
    "    > similarity5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "sortedSims = []\n",
    "with open(\"similarity5.5\",\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "\n",
    "        line = line.strip()\n",
    "        avg,lisst = line.split(\"\\t\")\n",
    "        lisst = json.loads(lisst)\n",
    "        lisst.append(avg)\n",
    "        sortedSims.append(lisst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory `sims2': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir sims2\n",
    "!head -1000 similarity5.5 > sims2/top1000sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top/Bottom 20 results - Similarity measures - sorted by cosine\n",
      "(From the entire data set)\n",
      "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "                          pair |         cosine |        jaccard |        overlap |           dice |        average\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "           iliac - replacement |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         replacement - rupture |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           hampshire - hungary |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "       greenhouse - tomography |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           grazing - livestock |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            gracious - privacy |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           gracious - pendulum |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "                   genre - que |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "        precaution - resultant |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         preamble - toleration |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           preamble - sweeping |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "               plunged - slain |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            planting - players |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "             pilgrim - proudly |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            pendulum - privacy |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            offender - schemes |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "          newborn - saturation |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         sweeping - toleration |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            needful - vanished |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "                 nazis - swift |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "               alpha - epsilon |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "        maxillary - neuropathy |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "     subordination - vengeance |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "              seas - wandering |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "              pious - syllable |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "       neuropathy - trigeminal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "        maxillary - trigeminal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           malaysian - mohamad |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "            mahathir - mohamad |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "          mahathir - malaysian |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "                lois - recueil |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "            ingenuity - wretch |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           infected - infusion |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "               hers - paternal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           fossil - resembling |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "          sensory - trigeminal |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "           maxillary - sensory |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "        breathed - persistence |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "            diabetic - sensory |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "                anne - prussia |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n"
     ]
    }
   ],
   "source": [
    "print \"\\nTop/Bottom 20 results - Similarity measures - sorted by cosine\"\n",
    "print \"(From the entire data set)\"\n",
    "print '—'*117\n",
    "print \"{0:>30} |{1:>15} |{2:>15} |{3:>15} |{4:>15} |{5:>15}\".format(\n",
    "        \"pair\", \"cosine\", \"jaccard\", \"overlap\", \"dice\", \"average\")\n",
    "print '-'*117\n",
    "\n",
    "for stripe in sortedSims[:20]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )\n",
    "\n",
    "print '—'*117\n",
    "\n",
    "for stripe in sortedSims[-20:]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.8  <a name=\"5.8\"></a> OPTIONAL: filter stopwords\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "There is also a corpus of stopwords, that is, high-frequency words like \"the\", \"to\" and \"also\" that we sometimes want to filter out of a document before further processing. Stopwords usually have little lexical content, and their presence in a text fails to distinguish it from other texts. Python's nltk comes with a prebuilt list of stopwords (see below). Using this stopword list filter out these tokens from your analysis and rerun the experiments in 5.5 and disucuss the results of using a stopword list and without using a stopword list.\n",
    "\n",
    "> from nltk.corpus import stopwords\n",
    ">> stopwords.words('english')\n",
    "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.9 <a name=\"5.9\"></a> OPTIONAL \n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "There are many good ways to build our synonym detectors, so for this optional homework, \n",
    "measure co-occurrence by (left/right/all) consecutive words only, \n",
    "or make stripes according to word co-occurrences with the accompanying \n",
    "2-, 3-, or 4-grams (note here that your output will no longer \n",
    "be interpretable as a network) inside of the 5-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.10 <a name=\"5.10\"></a> OPTIONAL \n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Once again, benchmark your top 10,000 associations (as in 5.5), this time for your\n",
    "results from 5.6. Has your detector improved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "511px",
    "width": "251px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
