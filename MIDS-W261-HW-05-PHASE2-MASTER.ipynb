{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS - w261 Machine Learning At Scale\n",
    "__Course Lead:__ Dr James G. Shanahan (__email__ Jimi via  James.Shanahan _AT_ gmail.com)\n",
    "\n",
    "## Assignment - HW5\n",
    "\n",
    "\n",
    "---\n",
    "__Name:__  Carlos Castro   \n",
    "__Class:__ MIDS w261 (Section 2, e.g., Fall 2016 Group 1)     \n",
    "__Email:__  carlosscastro@iSchool.Berkeley.edu     \n",
    "__Week:__   5\n",
    "\n",
    "__Due Time:__ 2 Phases. \n",
    "\n",
    "* __HW5 Phase 1__ \n",
    "This can be done on a local machine (with a unit test on the cloud such as AltaScale's PaaS or on AWS) and is due Tuesday, Week 6 by 8AM (West coast time). It will primarily focus on building a unit/systems and for pairwise similarity calculations pipeline (for stripe documents)\n",
    "\n",
    "* __HW5 Phase 2__ \n",
    "This will require the AltaScale cluster and will be due Tuesday, Week 7 by 8AM (West coast time). \n",
    "The focus of  HW5 Phase 2  will be to scale up the unit/systems tests to the Google 5 gram corpus. This will be a group exercise \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.13 | packaged by conda-forge | (default, May  2 2017, 12:48:11) \\n[GCC 4.8.2 20140120 (Red Hat 4.8.2-15)]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents <a name=\"TOC\"></a> \n",
    "\n",
    "1.  [HW Instructions](#1)   \n",
    "2.  [HW References](#2)\n",
    "3.  [HW Problems](#3)   \n",
    "       \n",
    "    5.4.  [HW5.4](#5.4)    \n",
    "    5.5.  [HW5.5](#5.5)    \n",
    "    5.6.  [HW5.6](#5.6)    \n",
    "    5.7.  [HW5.7](#5.7)    \n",
    "    5.8.  [HW5.8](#5.8)    \n",
    "    5.9.  [HW5.9](#5.9)    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "# 1 Instructions\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "MIDS UC Berkeley, Machine Learning at Scale   \n",
    "DATSCIW261 ASSIGNMENT #5\n",
    "\n",
    "Version 2017-9-2 \n",
    "\n",
    "\n",
    "### IMPORTANT\n",
    "\n",
    "This homework must be completed in the cloud \n",
    "\n",
    "### === INSTRUCTIONS for SUBMISSIONS ===   \n",
    "Follow the instructions for submissions carefully.\n",
    "\n",
    "Each student has a `HW-<user>` repository for all assignments.   \n",
    "\n",
    "Click this link to enable you to create a github repo within the MIDS261 Classroom:   \n",
    "https://classroom.github.com/assignment-invitations/3b1d6c8e58351209f9dd865537111ff8   \n",
    "and follow the instructions to create a HW repo.\n",
    "\n",
    "Push the following to your HW github repo into the master branch:\n",
    "* Your local HW5 directory. Your repo file structure should look like this:\n",
    "\n",
    "```\n",
    "HW-<user>\n",
    "    --HW3\n",
    "       |__MIDS-W261-HW-03-<Student_id>.ipynb\n",
    "       |__MIDS-W261-HW-03-<Student_id>.pdf\n",
    "       |__some other hw3 file\n",
    "    --HW4\n",
    "       |__MIDS-W261-HW-04-<Student_id>.ipynb\n",
    "       |__MIDS-W261-HW-04-<Student_id>.pdf\n",
    "       |__some other hw4 file\n",
    "    etc..\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\">\n",
    "# 2 Useful References\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "* See async and live lectures for this week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3\">\n",
    "# 3 HW Problems\n",
    "[Back to Table of Contents](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"5.4\"></a> \n",
    "# PHASE 2\n",
    "----------\n",
    "\n",
    "# HW 5.4   \n",
    "## Full-scale experiment on Google N-gram data on the CLOUD\n",
    "__ Once you are happy with your test results __ proceed to generating  your results on the Google n-grams dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.4.0  <a name=\"5.4.0\"></a> Run systems tests on the CLOUD  (PHASE 2)\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Repeat HW5.3.0 (using the same small data sources that were used in HW5.3.0) on ** the cloud** (e.g., AltaScale / AWS/ SoftLayer/ Azure). Make sure all tests give correct results! Good luck out there!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting buildStripes.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile buildStripes.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "import re\n",
    "import mrjob\n",
    "import json\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import itertools\n",
    "import collections\n",
    "import logging\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "class MRbuildStripes(MRJob):\n",
    "  \n",
    "    #START SUDENT CODE531_STRIPES\n",
    "  \n",
    "    MRJob.SORT_VALUES = True\n",
    "    \n",
    "    #def mapper_init(self):\n",
    "    #    return self.start_time = time.time()\n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        splits = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "\n",
    "        words = splits[0].lower().split()\n",
    "        count = splits[1]\n",
    "\n",
    "        H = {}\n",
    "        for subset in itertools.combinations(sorted(set(words)), 2):\n",
    "            \n",
    "            # Process combinations in sorted order, i.e. \"hello\",\"tomorrow\"\n",
    "            if subset[0] not in H.keys():\n",
    "                H[subset[0]] = {}\n",
    "                H[subset[0]][subset[1]] = count \n",
    "            elif subset[1] not in H[subset[0]]:\n",
    "                H[subset[0]][subset[1]] = count\n",
    "            else:\n",
    "                H[subset[0]][subset[1]] += count\n",
    "\n",
    "            # Obtain combinations in reverse order, to consider them both ways\n",
    "            # TODO: Should refactor this and the block above, shameless copy-paste\n",
    "            if subset[1] not in H.keys():\n",
    "                H[subset[1]] = {}\n",
    "                H[subset[1]][subset[0]] = count \n",
    "            elif subset[0] not in H[subset[1]]:\n",
    "                H[subset[1]][subset[0]] = count\n",
    "            else:\n",
    "                H[subset[1]][subset[0]] += count\n",
    "        for key in H.keys():\n",
    "            #print \"%s\\t%s\" % (key, json.dumps(H[key]))\n",
    "            yield key, H[key]\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        \n",
    "        counter = {}\n",
    "\n",
    "        for value in values:\n",
    "            \n",
    "            for k, v in value.iteritems():\n",
    "                if k in counter:\n",
    "                    counter[k] += int(v)\n",
    "                else:\n",
    "                    counter[k] = int(v)\n",
    "        \n",
    "        yield key, counter\n",
    "        \n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "\n",
    "            MRStep(#mapper_init=self.mapper_init\n",
    "                   #,\n",
    "                   mapper=self.mapper\n",
    "                   ,\n",
    "                   reducer=self.reducer\n",
    "                  )\n",
    "            ]\n",
    "  #END SUDENT CODE531_STRIPES\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    MRbuildStripes.run()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    mins = elapsed_time/float(60)\n",
    "    a = \"\"\"Elapsed time: %s seconds\n",
    "    In minutes: %s mins\"\"\" % (str(elapsed_time), str(mins))\n",
    "    logging.warning(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting invertedIndex.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile invertedIndex.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "from __future__ import division\n",
    "import collections\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import itertools\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "\n",
    "class MRinvertedIndex(MRJob):\n",
    "    \n",
    "    #START SUDENT CODE531_INV_INDEX\n",
    "  \n",
    "    def mapper(self, _, line):\n",
    "        key, stripeJson = line.strip().split('\\t')\n",
    "        key = key.strip(\"\\\"\")\n",
    "        stripe = json.loads(stripeJson)\n",
    "        \n",
    "        for k, v in stripe.iteritems():\n",
    "            yield k, [key, len(stripe)]\n",
    "        \n",
    "    def reducer(self, key, values):\n",
    "\n",
    "        table = {}\n",
    "        for value in values:\n",
    "            table[value[0]] = value[1]\n",
    "            \n",
    "        yield key, table\n",
    "        \n",
    "    #END SUDENT CODE531_INV_INDEX\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    MRinvertedIndex.run() \n",
    "    elapsed_time = time.time() - start_time\n",
    "    mins = elapsed_time/float(60)\n",
    "    a = \"\"\"Elapsed time: %s seconds\n",
    "    In minutes: %s mins\"\"\" % (str(elapsed_time), str(mins))\n",
    "    logging.warning(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting similarity.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile similarity.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "import collections\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "#import numpy as np\n",
    "import itertools\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import time\n",
    "import logging\n",
    "\n",
    "class MRsimilarity(MRJob):\n",
    "  \n",
    "    #START SUDENT CODE531_SIMILARITY\n",
    "\n",
    "    MRJob.SORT_VALUES = True\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        key, valuesJson = line.strip().split('\\t')\n",
    "        key = key.strip(\"\\\"\")\n",
    "        values = json.loads(valuesJson)\n",
    "\n",
    "        for pair in itertools.combinations(sorted(set(values)), 2):\n",
    "            yield pair, [values[pair[0]], values[pair[1]]]\n",
    "        \n",
    "    def reducer(self, key, values):\n",
    "        intersection = 0\n",
    "        count1 = None\n",
    "        count2 = None\n",
    "        \n",
    "        cosine = 0.0\n",
    "        \n",
    "        # Iterate through the values\n",
    "        for value in values:\n",
    "            # Jaccard, get counts for the intersection, and for each set\n",
    "            intersection += 1\n",
    "            if count1 == None:\n",
    "                count1 = value[0]\n",
    "                count2 = value[1]\n",
    "        \n",
    "            # Cosine\n",
    "            a = 1 / math.sqrt(value[0])\n",
    "            b = 1 / math.sqrt(value[1])\n",
    "            cosine += a * b\n",
    "            \n",
    "        jaccard = float(intersection) / float(count1 + count2 - intersection)\n",
    "        \n",
    "        overlap_coefficient = float(intersection) / min(count1, count2)\n",
    "        \n",
    "        dice_coefficient = float(2 * intersection) / (count1 + count2)\n",
    "        \n",
    "        average = (cosine + jaccard + overlap_coefficient + dice_coefficient) / 4.0\n",
    "        \n",
    "        yield average, [key[0] + ' - ' + key[1], cosine, jaccard, overlap_coefficient, dice_coefficient]\n",
    "            \n",
    "    #END SUDENT CODE531_SIMILARITY\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    MRsimilarity.run()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    mins = elapsed_time/float(60)\n",
    "    a = \"\"\"Elapsed time: %s seconds\n",
    "    In minutes: %s mins\"\"\" % (str(elapsed_time), str(mins))\n",
    "    logging.warning(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt\n",
    "A BILL FOR ESTABLISHING RELIGIOUS\t59\t59\t54\n",
    "A Biography of General George\t92\t90\t74\n",
    "A Case Study in Government\t102\t102\t78\n",
    "A Case Study of Female\t447\t447\t327\n",
    "A Case Study of Limited\t55\t55\t43\n",
    "A Child's Christmas in Wales\t1099\t1061\t866\n",
    "A Circumstantial Narrative of the\t62\t62\t50\n",
    "A City by the Sea\t62\t60\t49\n",
    "A Collection of Fairy Tales\t123\t117\t80\n",
    "A Collection of Forms of\t116\t103\t82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting atlas-boon-systems-test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile atlas-boon-systems-test.txt\n",
    "atlas boon\t50\t50\t50\n",
    "boon cava dipped\t10\t10\t10\n",
    "atlas dipped\t15\t15\t15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build stripes for mini-test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `systems_test_stripes_1': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/buildStripes.carlosscastro.20170619.233332.528121\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/carlosscastro/tmp/mrjob/buildStripes.carlosscastro.20170619.233332.528121/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob4225323262776239379.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497906899862_0065\n",
      "  Submitted application application_1497906899862_0065\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_0065/\n",
      "  Running job: job_1497906899862_0065\n",
      "  Job job_1497906899862_0065 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_0065 completed successfully\n",
      "  Output directory: hdfs:///user/carlosscastro/tmp/mrjob/buildStripes.carlosscastro.20170619.233332.528121/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=563\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2406\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1098\n",
      "\t\tFILE: Number of bytes written=400975\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1039\n",
      "\t\tHDFS: Number of bytes written=2406\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=11042304\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=15388160\n",
      "\t\tTotal time spent by all map tasks (ms)=7189\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=21567\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6011\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=30055\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=7189\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=6011\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2470\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=128\n",
      "\t\tInput split bytes=476\n",
      "\t\tMap input records=10\n",
      "\t\tMap output bytes=3359\n",
      "\t\tMap output materialized bytes=1076\n",
      "\t\tMap output records=49\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1914564608\n",
      "\t\tReduce input groups=49\n",
      "\t\tReduce input records=49\n",
      "\t\tReduce output records=28\n",
      "\t\tReduce shuffle bytes=1076\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=98\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7752548352\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/carlosscastro/tmp/mrjob/buildStripes.carlosscastro.20170619.233332.528121/output...\n",
      "Removing HDFS temp directory hdfs:///user/carlosscastro/tmp/mrjob/buildStripes.carlosscastro.20170619.233332.528121...\n",
      "Removing temp directory /tmp/buildStripes.carlosscastro.20170619.233332.528121...\n",
      "WARNING:root:Elapsed time: 55.1794281006 seconds\n",
      "    In minutes: 0.91965713501 mins\n"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "# Make Stripes from ngrams for systems test 1\n",
    "###########################################################################\n",
    "\n",
    "!hdfs dfs -rm -r systems_test_stripes_1\n",
    "!python buildStripes.py -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt > systems_test_stripes_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=2\n",
    "### Launched reduce tasks=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"a\"\t{\"limited\": 55, \"female\": 447, \"general\": 92, \"sea\": 62, \"in\": 1201, \"religious\": 59, \"george\": 92, \"biography\": 92, \"city\": 62, \"for\": 59, \"tales\": 123, \"child's\": 1099, \"forms\": 116, \"wales\": 1099, \"christmas\": 1099, \"government\": 102, \"collection\": 239, \"by\": 62, \"case\": 604, \"circumstantial\": 62, \"fairy\": 123, \"of\": 895, \"study\": 604, \"bill\": 59, \"establishing\": 59, \"narrative\": 62, \"the\": 124}\r\n",
      "\"bill\"\t{\"a\": 59, \"religious\": 59, \"for\": 59, \"establishing\": 59}\r\n",
      "\"biography\"\t{\"a\": 92, \"of\": 92, \"george\": 92, \"general\": 92}\r\n",
      "\"by\"\t{\"a\": 62, \"city\": 62, \"the\": 62, \"sea\": 62}\r\n",
      "\"case\"\t{\"a\": 604, \"limited\": 55, \"government\": 102, \"of\": 502, \"study\": 604, \"female\": 447, \"in\": 102}\r\n",
      "\"child's\"\t{\"a\": 1099, \"wales\": 1099, \"christmas\": 1099, \"in\": 1099}\r\n",
      "\"christmas\"\t{\"a\": 1099, \"wales\": 1099, \"in\": 1099, \"child's\": 1099}\r\n",
      "\"circumstantial\"\t{\"a\": 62, \"of\": 62, \"the\": 62, \"narrative\": 62}\r\n",
      "\"city\"\t{\"a\": 62, \"the\": 62, \"by\": 62, \"sea\": 62}\r\n",
      "\"collection\"\t{\"a\": 239, \"forms\": 116, \"fairy\": 123, \"tales\": 123, \"of\": 239}\r\n",
      "\"establishing\"\t{\"a\": 59, \"bill\": 59, \"religious\": 59, \"for\": 59}\r\n",
      "\"fairy\"\t{\"a\": 123, \"of\": 123, \"tales\": 123, \"collection\": 123}\r\n",
      "\"female\"\t{\"a\": 447, \"case\": 447, \"study\": 447, \"of\": 447}\r\n",
      "\"for\"\t{\"a\": 59, \"bill\": 59, \"religious\": 59, \"establishing\": 59}\r\n",
      "\"forms\"\t{\"a\": 116, \"of\": 116, \"collection\": 116}\r\n",
      "\"general\"\t{\"a\": 92, \"of\": 92, \"george\": 92, \"biography\": 92}\r\n",
      "\"george\"\t{\"a\": 92, \"of\": 92, \"biography\": 92, \"general\": 92}\r\n",
      "\"government\"\t{\"a\": 102, \"case\": 102, \"study\": 102, \"in\": 102}\r\n",
      "\"in\"\t{\"a\": 1201, \"case\": 102, \"government\": 102, \"study\": 102, \"child's\": 1099, \"wales\": 1099, \"christmas\": 1099}\r\n",
      "\"limited\"\t{\"a\": 55, \"case\": 55, \"study\": 55, \"of\": 55}\r\n",
      "\"narrative\"\t{\"a\": 62, \"of\": 62, \"the\": 62, \"circumstantial\": 62}\r\n",
      "\"of\"\t{\"a\": 895, \"case\": 502, \"circumstantial\": 62, \"george\": 92, \"limited\": 55, \"tales\": 123, \"collection\": 239, \"the\": 62, \"forms\": 116, \"female\": 447, \"narrative\": 62, \"fairy\": 123, \"general\": 92, \"study\": 502, \"biography\": 92}\r\n",
      "\"religious\"\t{\"a\": 59, \"bill\": 59, \"for\": 59, \"establishing\": 59}\r\n",
      "\"sea\"\t{\"a\": 62, \"city\": 62, \"the\": 62, \"by\": 62}\r\n",
      "\"study\"\t{\"a\": 604, \"case\": 604, \"limited\": 55, \"government\": 102, \"of\": 502, \"female\": 447, \"in\": 102}\r\n",
      "\"tales\"\t{\"a\": 123, \"of\": 123, \"fairy\": 123, \"collection\": 123}\r\n",
      "\"the\"\t{\"a\": 124, \"city\": 62, \"circumstantial\": 62, \"of\": 62, \"sea\": 62, \"narrative\": 62, \"by\": 62}\r\n",
      "\"wales\"\t{\"a\": 1099, \"in\": 1099, \"christmas\": 1099, \"child's\": 1099}\r\n"
     ]
    }
   ],
   "source": [
    "!cat systems_test_stripes_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `systems_test_stripes_2': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/buildStripes.carlosscastro.20170619.233430.467494\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/carlosscastro/tmp/mrjob/buildStripes.carlosscastro.20170619.233430.467494/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob582798525239557719.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497906899862_0066\n",
      "  Submitted application application_1497906899862_0066\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_0066/\n",
      "  Running job: job_1497906899862_0066\n",
      "  Job job_1497906899862_0066 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_0066 completed successfully\n",
      "  Output directory: hdfs:///user/carlosscastro/tmp/mrjob/buildStripes.carlosscastro.20170619.233430.467494/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=101\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=163\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=138\n",
      "\t\tFILE: Number of bytes written=398998\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=503\n",
      "\t\tHDFS: Number of bytes written=163\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=18269184\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=9687040\n",
      "\t\tTotal time spent by all map tasks (ms)=11894\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=35682\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3784\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=18920\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=11894\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3784\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2440\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=61\n",
      "\t\tInput split bytes=402\n",
      "\t\tMap input records=3\n",
      "\t\tMap output bytes=217\n",
      "\t\tMap output materialized bytes=173\n",
      "\t\tMap output records=7\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1912156160\n",
      "\t\tReduce input groups=7\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=4\n",
      "\t\tReduce shuffle bytes=173\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=14\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7744491520\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/carlosscastro/tmp/mrjob/buildStripes.carlosscastro.20170619.233430.467494/output...\n",
      "Removing HDFS temp directory hdfs:///user/carlosscastro/tmp/mrjob/buildStripes.carlosscastro.20170619.233430.467494...\n",
      "Removing temp directory /tmp/buildStripes.carlosscastro.20170619.233430.467494...\n",
      "WARNING:root:Elapsed time: 53.581441164 seconds\n",
      "    In minutes: 0.8930240194 mins\n"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "# Make Stripes from ngrams for systems test 2\n",
    "###########################################################################\n",
    "\n",
    "!hdfs dfs -rm -r systems_test_stripes_2\n",
    "!python buildStripes.py -r hadoop atlas-boon-systems-test.txt > systems_test_stripes_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=2\n",
    "### Launched reduce tasks=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"atlas\"\t{\"dipped\": 15, \"boon\": 50}\r\n",
      "\"boon\"\t{\"atlas\": 50, \"dipped\": 10, \"cava\": 10}\r\n",
      "\"cava\"\t{\"dipped\": 10, \"boon\": 10}\r\n",
      "\"dipped\"\t{\"atlas\": 15, \"boon\": 10, \"cava\": 10}\r\n"
     ]
    }
   ],
   "source": [
    "!cat systems_test_stripes_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"DocA\"\t{\"X\":20, \"Y\":30, \"Z\":5}\r\n",
      "\"DocB\"\t{\"X\":100, \"Y\":20}\r\n",
      "\"DocC\"\t{\"M\":5, \"N\":20, \"Z\":5, \"Y\":1}\r\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "# Stripes for systems test 3 (given, no need to build stripes)\n",
    "########################################################################\n",
    "\n",
    "with open(\"systems_test_stripes_3\", \"w\") as f:\n",
    "    f.writelines([\n",
    "        '\"DocA\"\\t{\"X\":20, \"Y\":30, \"Z\":5}\\n',\n",
    "        '\"DocB\"\\t{\"X\":100, \"Y\":20}\\n',  \n",
    "        '\"DocC\"\\t{\"M\":5, \"N\":20, \"Z\":5, \"Y\":1}\\n'\n",
    "    ])\n",
    "!cat systems_test_stripes_3  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted indices for mini-test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/invertedIndex.carlosscastro.20170619.233524.719276\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/carlosscastro/tmp/mrjob/invertedIndex.carlosscastro.20170619.233524.719276/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob953207769058660204.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497906899862_0067\n",
      "  Submitted application application_1497906899862_0067\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_0067/\n",
      "  Running job: job_1497906899862_0067\n",
      "  Job job_1497906899862_0067 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_0067 completed successfully\n",
      "  Output directory: hdfs:///user/carlosscastro/tmp/mrjob/invertedIndex.carlosscastro.20170619.233524.719276/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3609\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2192\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1402\n",
      "\t\tFILE: Number of bytes written=400293\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=4003\n",
      "\t\tHDFS: Number of bytes written=2192\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=10718208\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=16058880\n",
      "\t\tTotal time spent by all map tasks (ms)=6978\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=20934\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6273\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=31365\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=6978\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=6273\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2420\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=118\n",
      "\t\tInput split bytes=394\n",
      "\t\tMap input records=28\n",
      "\t\tMap output bytes=3308\n",
      "\t\tMap output materialized bytes=1623\n",
      "\t\tMap output records=158\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1908445184\n",
      "\t\tReduce input groups=28\n",
      "\t\tReduce input records=158\n",
      "\t\tReduce output records=28\n",
      "\t\tReduce shuffle bytes=1623\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=316\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7744876544\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/carlosscastro/tmp/mrjob/invertedIndex.carlosscastro.20170619.233524.719276/output...\n",
      "Removing HDFS temp directory hdfs:///user/carlosscastro/tmp/mrjob/invertedIndex.carlosscastro.20170619.233524.719276...\n",
      "Removing temp directory /tmp/invertedIndex.carlosscastro.20170619.233524.719276...\n",
      "WARNING:root:Elapsed time: 54.9282670021 seconds\n",
      "    In minutes: 0.915471116702 mins\n"
     ]
    }
   ],
   "source": [
    "!python invertedIndex.py -r hadoop systems_test_stripes_1 > systems_test_index_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=2\n",
    "### Launched reduce tasks=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/invertedIndex.carlosscastro.20170619.233619.931554\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/carlosscastro/tmp/mrjob/invertedIndex.carlosscastro.20170619.233619.931554/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob8013270647447558844.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497906899862_0068\n",
      "  Submitted application application_1497906899862_0068\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_0068/\n",
      "  Running job: job_1497906899862_0068\n",
      "  Job job_1497906899862_0068 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_0068 completed successfully\n",
      "  Output directory: hdfs:///user/carlosscastro/tmp/mrjob/invertedIndex.carlosscastro.20170619.233619.931554/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=245\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=153\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=144\n",
      "\t\tFILE: Number of bytes written=397605\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=639\n",
      "\t\tHDFS: Number of bytes written=153\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=18691584\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=11456000\n",
      "\t\tTotal time spent by all map tasks (ms)=12169\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=36507\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4475\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=22375\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=12169\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=4475\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2680\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=84\n",
      "\t\tInput split bytes=394\n",
      "\t\tMap input records=4\n",
      "\t\tMap output bytes=206\n",
      "\t\tMap output materialized bytes=190\n",
      "\t\tMap output records=10\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1910685696\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce input records=10\n",
      "\t\tReduce output records=4\n",
      "\t\tReduce shuffle bytes=190\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=20\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7761653760\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/carlosscastro/tmp/mrjob/invertedIndex.carlosscastro.20170619.233619.931554/output...\n",
      "Removing HDFS temp directory hdfs:///user/carlosscastro/tmp/mrjob/invertedIndex.carlosscastro.20170619.233619.931554...\n",
      "Removing temp directory /tmp/invertedIndex.carlosscastro.20170619.233619.931554...\n",
      "WARNING:root:Elapsed time: 57.4641230106 seconds\n",
      "    In minutes: 0.957735383511 mins\n"
     ]
    }
   ],
   "source": [
    "!python invertedIndex.py -r hadoop systems_test_stripes_2 > systems_test_index_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=2\n",
    "### Launched reduce tasks=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/invertedIndex.carlosscastro.20170619.233717.683486\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/carlosscastro/tmp/mrjob/invertedIndex.carlosscastro.20170619.233717.683486/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob147506958633468906.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497906899862_0069\n",
      "  Submitted application application_1497906899862_0069\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_0069/\n",
      "  Running job: job_1497906899862_0069\n",
      "  Job job_1497906899862_0069 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_0069 completed successfully\n",
      "  Output directory: hdfs:///user/carlosscastro/tmp/mrjob/invertedIndex.carlosscastro.20170619.233717.683486/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=140\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=124\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=98\n",
      "\t\tFILE: Number of bytes written=397496\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=534\n",
      "\t\tHDFS: Number of bytes written=124\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=11409408\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=10127360\n",
      "\t\tTotal time spent by all map tasks (ms)=7428\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=22284\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3956\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=19780\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=7428\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3956\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2560\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=92\n",
      "\t\tInput split bytes=394\n",
      "\t\tMap input records=3\n",
      "\t\tMap output bytes=144\n",
      "\t\tMap output materialized bytes=130\n",
      "\t\tMap output records=9\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1905901568\n",
      "\t\tReduce input groups=5\n",
      "\t\tReduce input records=9\n",
      "\t\tReduce output records=5\n",
      "\t\tReduce shuffle bytes=130\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=18\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7758643200\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/carlosscastro/tmp/mrjob/invertedIndex.carlosscastro.20170619.233717.683486/output...\n",
      "Removing HDFS temp directory hdfs:///user/carlosscastro/tmp/mrjob/invertedIndex.carlosscastro.20170619.233717.683486...\n",
      "Removing temp directory /tmp/invertedIndex.carlosscastro.20170619.233717.683486...\n",
      "WARNING:root:Elapsed time: 53.7165219784 seconds\n",
      "    In minutes: 0.895275366306 mins\n"
     ]
    }
   ],
   "source": [
    "!python invertedIndex.py -r hadoop systems_test_stripes_3 > systems_test_index_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=2\n",
    "### Launched reduce tasks=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Systems test  1  - Inverted Index\n",
      "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "             \"a\" |          bill 4 |     biography 4 |            by 4\n",
      "          \"bill\" |            a 27 |  establishing 4 |           for 4\n",
      "     \"biography\" |            a 27 |       general 4 |        george 4\n",
      "            \"by\" |            a 27 |          city 4 |           sea 4\n",
      "          \"case\" |            a 27 |        female 4 |    government 4\n",
      "       \"child's\" |            a 27 |     christmas 4 |            in 7\n",
      "     \"christmas\" |            a 27 |       child's 4 |            in 7\n",
      "\"circumstantial\" |            a 27 |     narrative 4 |           of 15\n",
      "          \"city\" |            a 27 |            by 4 |           sea 4\n",
      "    \"collection\" |            a 27 |         fairy 4 |         forms 3\n",
      "  \"establishing\" |            a 27 |          bill 4 |           for 4\n",
      "         \"fairy\" |            a 27 |    collection 5 |           of 15\n",
      "        \"female\" |            a 27 |          case 7 |           of 15\n",
      "           \"for\" |            a 27 |          bill 4 |  establishing 4\n",
      "         \"forms\" |            a 27 |    collection 5 |           of 15\n",
      "       \"general\" |            a 27 |     biography 4 |        george 4\n",
      "        \"george\" |            a 27 |     biography 4 |       general 4\n",
      "    \"government\" |            a 27 |          case 7 |            in 7\n",
      "            \"in\" |            a 27 |          case 7 |       child's 4\n",
      "       \"limited\" |            a 27 |          case 7 |           of 15\n",
      "     \"narrative\" |            a 27 |circumstantial 4 |           of 15\n",
      "            \"of\" |            a 27 |     biography 4 |          case 7\n",
      "     \"religious\" |            a 27 |          bill 4 |  establishing 4\n",
      "           \"sea\" |            a 27 |            by 4 |          city 4\n",
      "         \"study\" |            a 27 |          case 7 |        female 4\n",
      "         \"tales\" |            a 27 |    collection 5 |         fairy 4\n",
      "           \"the\" |            a 27 |            by 4 |circumstantial 4\n",
      "         \"wales\" |            a 27 |       child's 4 |     christmas 4\n",
      "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Systems test  2  - Inverted Index\n",
      "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "         \"atlas\" |          boon 3 |        dipped 3 |                \n",
      "          \"boon\" |         atlas 2 |          cava 2 |        dipped 3\n",
      "          \"cava\" |          boon 3 |        dipped 3 |                \n",
      "        \"dipped\" |         atlas 2 |          boon 3 |          cava 2\n",
      "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Systems test  3  - Inverted Index\n",
      "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "             \"M\" |          DocC 4 |                 |                \n",
      "             \"N\" |          DocC 4 |                 |                \n",
      "             \"X\" |          DocA 3 |          DocB 2 |                \n",
      "             \"Y\" |          DocA 3 |          DocB 2 |          DocC 4\n",
      "             \"Z\" |          DocA 3 |          DocC 4 |                \n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "# Pretty print systems tests for generating Inverted Index\n",
    "##########################################################\n",
    "import json\n",
    "\n",
    "for i in range(1,4):\n",
    "    print \"—\"*100\n",
    "    print \"Systems test \",i,\" - Inverted Index\"\n",
    "    print \"—\"*100  \n",
    "    with open(\"systems_test_index_\"+str(i),\"r\") as f:\n",
    "        lines = sorted(f.readlines())\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            word, doc_list = line.split(\"\\t\")\n",
    "            doc_dict = json.loads(doc_list)\n",
    "            stripe=[]\n",
    "            for doc in doc_dict:\n",
    "                stripe.append([doc, doc_dict[doc]])\n",
    "            stripe=sorted(stripe)\n",
    "            stripe.extend([[\"\",\"\"] for _ in xrange(3 - len(stripe))])\n",
    "\n",
    "            print \"{0:>16} |{1:>16} |{2:>16} |{3:>16}\".format(\n",
    "              (word), stripe[0][0]+\" \"+str(stripe[0][1]), stripe[1][0]+\" \"+str(stripe[1][1]), stripe[2][0]+\" \"+str(stripe[2][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarities for mini-test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/similarity.carlosscastro.20170619.233811.680672\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/carlosscastro/tmp/mrjob/similarity.carlosscastro.20170619.233811.680672/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob60442221680766306.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497906899862_0070\n",
      "  Submitted application application_1497906899862_0070\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_0070/\n",
      "  Running job: job_1497906899862_0070\n",
      "  Job job_1497906899862_0070 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_0070 completed successfully\n",
      "  Output directory: hdfs:///user/carlosscastro/tmp/mrjob/similarity.carlosscastro.20170619.233811.680672/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3288\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=35050\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=3756\n",
      "\t\tFILE: Number of bytes written=407269\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3672\n",
      "\t\tHDFS: Number of bytes written=35050\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=18564096\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=10140160\n",
      "\t\tTotal time spent by all map tasks (ms)=12086\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=36258\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3961\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=19805\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=12086\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3961\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2800\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=118\n",
      "\t\tInput split bytes=384\n",
      "\t\tMap input records=28\n",
      "\t\tMap output bytes=19239\n",
      "\t\tMap output materialized bytes=4940\n",
      "\t\tMap output records=673\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1899835392\n",
      "\t\tReduce input groups=378\n",
      "\t\tReduce input records=673\n",
      "\t\tReduce output records=378\n",
      "\t\tReduce shuffle bytes=4940\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=1346\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7762612224\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/carlosscastro/tmp/mrjob/similarity.carlosscastro.20170619.233811.680672/output...\n",
      "Removing HDFS temp directory hdfs:///user/carlosscastro/tmp/mrjob/similarity.carlosscastro.20170619.233811.680672...\n",
      "Removing temp directory /tmp/similarity.carlosscastro.20170619.233811.680672...\n",
      "WARNING:root:Elapsed time: 55.9401051998 seconds\n",
      "    In minutes: 0.932335086664 mins\n"
     ]
    }
   ],
   "source": [
    "!python similarity.py -r hadoop systems_test_index_1 > systems_test_similarities_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=2\n",
    "### Launched reduce tasks=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/similarity.carlosscastro.20170619.233907.863045\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/carlosscastro/tmp/mrjob/similarity.carlosscastro.20170619.233907.863045/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob2880718829264400707.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497906899862_0071\n",
      "  Submitted application application_1497906899862_0071\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_0071/\n",
      "  Running job: job_1497906899862_0071\n",
      "  Job job_1497906899862_0071 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_0071 completed successfully\n",
      "  Output directory: hdfs:///user/carlosscastro/tmp/mrjob/similarity.carlosscastro.20170619.233907.863045/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=230\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=511\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=130\n",
      "\t\tFILE: Number of bytes written=398900\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=614\n",
      "\t\tHDFS: Number of bytes written=511\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=10630656\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=14773760\n",
      "\t\tTotal time spent by all map tasks (ms)=6921\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=20763\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5771\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=28855\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=6921\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=5771\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2360\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=94\n",
      "\t\tInput split bytes=384\n",
      "\t\tMap input records=4\n",
      "\t\tMap output bytes=212\n",
      "\t\tMap output materialized bytes=191\n",
      "\t\tMap output records=8\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1907519488\n",
      "\t\tReduce input groups=6\n",
      "\t\tReduce input records=8\n",
      "\t\tReduce output records=6\n",
      "\t\tReduce shuffle bytes=191\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=16\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7784771584\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/carlosscastro/tmp/mrjob/similarity.carlosscastro.20170619.233907.863045/output...\n",
      "Removing HDFS temp directory hdfs:///user/carlosscastro/tmp/mrjob/similarity.carlosscastro.20170619.233907.863045...\n",
      "Removing temp directory /tmp/similarity.carlosscastro.20170619.233907.863045...\n",
      "WARNING:root:Elapsed time: 53.6822800636 seconds\n",
      "    In minutes: 0.894704667727 mins\n"
     ]
    }
   ],
   "source": [
    "!python similarity.py -r hadoop systems_test_index_2 > systems_test_similarities_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/similarity.carlosscastro.20170619.234001.781795\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/carlosscastro/tmp/mrjob/similarity.carlosscastro.20170619.234001.781795/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob7292955171073824495.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497906899862_0072\n",
      "  Submitted application application_1497906899862_0072\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_0072/\n",
      "  Running job: job_1497906899862_0072\n",
      "  Job job_1497906899862_0072 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_0072 completed successfully\n",
      "  Output directory: hdfs:///user/carlosscastro/tmp/mrjob/similarity.carlosscastro.20170619.234001.781795/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=186\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=327\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=80\n",
      "\t\tFILE: Number of bytes written=398770\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=570\n",
      "\t\tHDFS: Number of bytes written=327\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=17995776\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=10818560\n",
      "\t\tTotal time spent by all map tasks (ms)=11716\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=35148\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4226\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=21130\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=11716\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=4226\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2670\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=73\n",
      "\t\tInput split bytes=384\n",
      "\t\tMap input records=5\n",
      "\t\tMap output bytes=125\n",
      "\t\tMap output materialized bytes=111\n",
      "\t\tMap output records=5\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1894973440\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce input records=5\n",
      "\t\tReduce output records=3\n",
      "\t\tReduce shuffle bytes=111\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=10\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7781769216\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/carlosscastro/tmp/mrjob/similarity.carlosscastro.20170619.234001.781795/output...\n",
      "Removing HDFS temp directory hdfs:///user/carlosscastro/tmp/mrjob/similarity.carlosscastro.20170619.234001.781795...\n",
      "Removing temp directory /tmp/similarity.carlosscastro.20170619.234001.781795...\n",
      "WARNING:root:Elapsed time: 55.7582540512 seconds\n",
      "    In minutes: 0.929304234187 mins\n"
     ]
    }
   ],
   "source": [
    "!python similarity.py -r hadoop systems_test_index_3 > systems_test_similarities_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=2\n",
    "### Launched reduce tasks=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Systems test  1  - Similarity measures\n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "        average |           pair |         cosine |        jaccard |        overlap |           dice\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "       0.334842 |       a - bill |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.334842 |  a - biography |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.334842 |         a - by |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.465201 |       a - case |       0.436436 |       0.214286 |       0.857143 |       0.352941\n",
      "       0.334842 |    a - child's |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.334842 |  a - christmas |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.334842 |a - circumstantial |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.334842 |       a - city |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.384281 | a - collection |       0.344265 |       0.142857 |       0.800000 |       0.250000\n",
      "       0.334842 |a - establishing |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.334842 |      a - fairy |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.334842 |     a - female |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.334842 |        a - for |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.273413 |      a - forms |       0.222222 |       0.071429 |       0.666667 |       0.133333\n",
      "       0.334842 |    a - general |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.334842 |     a - george |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.334842 | a - government |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.465201 |         a - in |       0.436436 |       0.214286 |       0.857143 |       0.352941\n",
      "       0.334842 |    a - limited |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.334842 |  a - narrative |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.698916 |         a - of |       0.695666 |       0.500000 |       0.933333 |       0.666667\n",
      "       0.334842 |  a - religious |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.334842 |        a - sea |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.465201 |      a - study |       0.436436 |       0.214286 |       0.857143 |       0.352941\n",
      "       0.334842 |      a - tales |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.465201 |        a - the |       0.436436 |       0.214286 |       0.857143 |       0.352941\n",
      "       0.334842 |      a - wales |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.223214 |bill - biography |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |      bill - by |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |    bill - case |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 | bill - child's |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |bill - christmas |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |bill - circumstantial |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |    bill - city |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.205207 |bill - collection |       0.223607 |       0.125000 |       0.250000 |       0.222222\n",
      "       0.712500 |bill - establishing |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.223214 |   bill - fairy |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |  bill - female |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.712500 |     bill - for |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.268597 |   bill - forms |       0.288675 |       0.166667 |       0.333333 |       0.285714\n",
      "       0.223214 | bill - general |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |  bill - george |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |bill - government |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |      bill - in |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 | bill - limited |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |bill - narrative |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.134980 |      bill - of |       0.129099 |       0.055556 |       0.250000 |       0.105263\n",
      "       0.712500 |bill - religious |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.223214 |     bill - sea |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |   bill - study |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |   bill - tales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |     bill - the |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |   bill - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 | biography - by |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.365956 |biography - case |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.223214 |biography - child's |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |biography - christmas |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.458333 |biography - circumstantial |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.223214 |biography - city |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.419343 |biography - collection |       0.447214 |       0.285714 |       0.500000 |       0.444444\n",
      "       0.223214 |biography - establishing |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.458333 |biography - fairy |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.458333 |biography - female |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.223214 |biography - for |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.553861 |biography - forms |       0.577350 |       0.400000 |       0.666667 |       0.571429\n",
      "       0.712500 |biography - general |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.712500 |biography - george |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.223214 |biography - government |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 | biography - in |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.458333 |biography - limited |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.458333 |biography - narrative |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.410147 | biography - of |       0.387298 |       0.187500 |       0.750000 |       0.315789\n",
      "       0.223214 |biography - religious |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |biography - sea |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.365956 |biography - study |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.458333 |biography - tales |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.365956 |biography - the |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.223214 |biography - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |      by - case |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |   by - child's |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 | by - christmas |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.458333 |by - circumstantial |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.712500 |      by - city |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.205207 |by - collection |       0.223607 |       0.125000 |       0.250000 |       0.222222\n",
      "       0.223214 |by - establishing |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |     by - fairy |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |    by - female |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |       by - for |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.268597 |     by - forms |       0.288675 |       0.166667 |       0.333333 |       0.285714\n",
      "       0.223214 |   by - general |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |    by - george |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |by - government |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |        by - in |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |   by - limited |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.458333 | by - narrative |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.271593 |        by - of |       0.258199 |       0.117647 |       0.500000 |       0.210526\n",
      "       0.223214 | by - religious |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.712500 |       by - sea |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.180200 |     by - study |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |     by - tales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.559350 |       by - the |       0.566947 |       0.375000 |       0.750000 |       0.545455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0.223214 |     by - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.365956 | case - child's |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.365956 |case - christmas |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.365956 |case - circumstantial |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.180200 |    case - city |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.317849 |case - collection |       0.338062 |       0.200000 |       0.400000 |       0.333333\n",
      "       0.180200 |case - establishing |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.365956 |   case - fairy |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.559350 |  case - female |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.180200 |     case - for |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.438276 |   case - forms |       0.436436 |       0.250000 |       0.666667 |       0.400000\n",
      "       0.365956 | case - general |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.365956 |  case - george |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.559350 |case - government |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.389610 |      case - in |       0.428571 |       0.272727 |       0.428571 |       0.428571\n",
      "       0.559350 | case - limited |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.365956 |case - narrative |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.386912 |      case - of |       0.390360 |       0.222222 |       0.571429 |       0.363636\n",
      "       0.180200 |case - religious |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.180200 |     case - sea |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.830357 |   case - study |       0.857143 |       0.750000 |       0.857143 |       0.857143\n",
      "       0.365956 |   case - tales |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.255952 |     case - the |       0.285714 |       0.166667 |       0.285714 |       0.285714\n",
      "       0.365956 |   case - wales |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.712500 |child's - christmas |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.223214 |child's - circumstantial |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 | child's - city |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.205207 |child's - collection |       0.223607 |       0.125000 |       0.250000 |       0.222222\n",
      "       0.223214 |child's - establishing |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |child's - fairy |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |child's - female |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |  child's - for |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.268597 |child's - forms |       0.288675 |       0.166667 |       0.333333 |       0.285714\n",
      "       0.223214 |child's - general |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |child's - george |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.458333 |child's - government |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.559350 |   child's - in |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.223214 |child's - limited |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |child's - narrative |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.134980 |   child's - of |       0.129099 |       0.055556 |       0.250000 |       0.105263\n",
      "       0.223214 |child's - religious |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |  child's - sea |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.365956 |child's - study |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.223214 |child's - tales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |  child's - the |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.712500 |child's - wales |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.223214 |christmas - circumstantial |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |christmas - city |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.205207 |christmas - collection |       0.223607 |       0.125000 |       0.250000 |       0.222222\n",
      "       0.223214 |christmas - establishing |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |christmas - fairy |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |christmas - female |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |christmas - for |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.268597 |christmas - forms |       0.288675 |       0.166667 |       0.333333 |       0.285714\n",
      "       0.223214 |christmas - general |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |christmas - george |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.458333 |christmas - government |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.559350 | christmas - in |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.223214 |christmas - limited |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |christmas - narrative |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.134980 | christmas - of |       0.129099 |       0.055556 |       0.250000 |       0.105263\n",
      "       0.223214 |christmas - religious |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |christmas - sea |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.365956 |christmas - study |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.223214 |christmas - tales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |christmas - the |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.712500 |christmas - wales |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.458333 |circumstantial - city |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.419343 |circumstantial - collection |       0.447214 |       0.285714 |       0.500000 |       0.444444\n",
      "       0.223214 |circumstantial - establishing |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.458333 |circumstantial - fairy |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.458333 |circumstantial - female |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.223214 |circumstantial - for |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.553861 |circumstantial - forms |       0.577350 |       0.400000 |       0.666667 |       0.571429\n",
      "       0.458333 |circumstantial - general |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.458333 |circumstantial - george |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.223214 |circumstantial - government |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |circumstantial - in |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.458333 |circumstantial - limited |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.712500 |circumstantial - narrative |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.410147 |circumstantial - of |       0.387298 |       0.187500 |       0.750000 |       0.315789\n",
      "       0.223214 |circumstantial - religious |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.458333 |circumstantial - sea |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.365956 |circumstantial - study |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.458333 |circumstantial - tales |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.559350 |circumstantial - the |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.223214 |circumstantial - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.205207 |city - collection |       0.223607 |       0.125000 |       0.250000 |       0.222222\n",
      "       0.223214 |city - establishing |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |   city - fairy |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |  city - female |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |     city - for |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.268597 |   city - forms |       0.288675 |       0.166667 |       0.333333 |       0.285714\n",
      "       0.223214 | city - general |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |  city - george |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |city - government |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |      city - in |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 | city - limited |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.458333 |city - narrative |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.271593 |      city - of |       0.258199 |       0.117647 |       0.500000 |       0.210526\n",
      "       0.223214 |city - religious |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.712500 |     city - sea |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.180200 |   city - study |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |   city - tales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.559350 |     city - the |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.223214 |   city - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.205207 |collection - establishing |       0.223607 |       0.125000 |       0.250000 |       0.222222\n",
      "       0.646872 |collection - fairy |       0.670820 |       0.500000 |       0.750000 |       0.666667\n",
      "       0.419343 |collection - female |       0.447214 |       0.285714 |       0.500000 |       0.444444\n",
      "       0.205207 |collection - for |       0.223607 |       0.125000 |       0.250000 |       0.222222\n",
      "       0.504099 |collection - forms |       0.516398 |       0.333333 |       0.666667 |       0.500000\n",
      "       0.419343 |collection - general |       0.447214 |       0.285714 |       0.500000 |       0.444444\n",
      "       0.419343 |collection - george |       0.447214 |       0.285714 |       0.500000 |       0.444444\n",
      "       0.205207 |collection - government |       0.223607 |       0.125000 |       0.250000 |       0.222222\n",
      "       0.156652 |collection - in |       0.169031 |       0.090909 |       0.200000 |       0.166667\n",
      "       0.419343 |collection - limited |       0.447214 |       0.285714 |       0.500000 |       0.444444\n",
      "       0.419343 |collection - narrative |       0.447214 |       0.285714 |       0.500000 |       0.444444\n",
      "       0.477970 |collection - of |       0.461880 |       0.250000 |       0.800000 |       0.400000\n",
      "       0.205207 |collection - religious |       0.223607 |       0.125000 |       0.250000 |       0.222222\n",
      "       0.205207 |collection - sea |       0.223607 |       0.125000 |       0.250000 |       0.222222\n",
      "       0.317849 |collection - study |       0.338062 |       0.200000 |       0.400000 |       0.333333\n",
      "       0.646872 |collection - tales |       0.670820 |       0.500000 |       0.750000 |       0.666667\n",
      "       0.317849 |collection - the |       0.338062 |       0.200000 |       0.400000 |       0.333333\n",
      "       0.205207 |collection - wales |       0.223607 |       0.125000 |       0.250000 |       0.222222\n",
      "       0.223214 |establishing - fairy |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |establishing - female |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.712500 |establishing - for |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.268597 |establishing - forms |       0.288675 |       0.166667 |       0.333333 |       0.285714\n",
      "       0.223214 |establishing - general |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |establishing - george |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |establishing - government |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |establishing - in |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |establishing - limited |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |establishing - narrative |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.134980 |establishing - of |       0.129099 |       0.055556 |       0.250000 |       0.105263\n",
      "       0.712500 |establishing - religious |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.223214 |establishing - sea |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |establishing - study |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |establishing - tales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |establishing - the |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |establishing - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.458333 | fairy - female |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.223214 |    fairy - for |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.868292 |  fairy - forms |       0.866025 |       0.750000 |       1.000000 |       0.857143\n",
      "       0.458333 |fairy - general |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.458333 | fairy - george |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.223214 |fairy - government |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |     fairy - in |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.458333 |fairy - limited |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.458333 |fairy - narrative |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.410147 |     fairy - of |       0.387298 |       0.187500 |       0.750000 |       0.315789\n",
      "       0.223214 |fairy - religious |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |    fairy - sea |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.365956 |  fairy - study |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.712500 |  fairy - tales |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.365956 |    fairy - the |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.223214 |  fairy - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |   female - for |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.553861 | female - forms |       0.577350 |       0.400000 |       0.666667 |       0.571429\n",
      "       0.458333 |female - general |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.458333 |female - george |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.712500 |female - government |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.559350 |    female - in |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       1.000000 |female - limited |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "       0.458333 |female - narrative |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.410147 |    female - of |       0.387298 |       0.187500 |       0.750000 |       0.315789\n",
      "       0.223214 |female - religious |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |   female - sea |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.559350 | female - study |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.458333 | female - tales |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.365956 |   female - the |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.223214 | female - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.268597 |    for - forms |       0.288675 |       0.166667 |       0.333333 |       0.285714\n",
      "       0.223214 |  for - general |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |   for - george |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |for - government |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |       for - in |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |  for - limited |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |for - narrative |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.134980 |       for - of |       0.129099 |       0.055556 |       0.250000 |       0.105263\n",
      "       0.712500 |for - religious |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.223214 |      for - sea |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |    for - study |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |    for - tales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |      for - the |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |    for - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.553861 |forms - general |       0.577350 |       0.400000 |       0.666667 |       0.571429\n",
      "       0.553861 | forms - george |       0.577350 |       0.400000 |       0.666667 |       0.571429\n",
      "       0.268597 |forms - government |       0.288675 |       0.166667 |       0.333333 |       0.285714\n",
      "       0.215666 |     forms - in |       0.218218 |       0.111111 |       0.333333 |       0.200000\n",
      "       0.553861 |forms - limited |       0.577350 |       0.400000 |       0.666667 |       0.571429\n",
      "       0.553861 |forms - narrative |       0.577350 |       0.400000 |       0.666667 |       0.571429\n",
      "       0.328008 |     forms - of |       0.298142 |       0.125000 |       0.666667 |       0.222222\n",
      "       0.268597 |forms - religious |       0.288675 |       0.166667 |       0.333333 |       0.285714\n",
      "       0.268597 |    forms - sea |       0.288675 |       0.166667 |       0.333333 |       0.285714\n",
      "       0.438276 |  forms - study |       0.436436 |       0.250000 |       0.666667 |       0.400000\n",
      "       0.868292 |  forms - tales |       0.866025 |       0.750000 |       1.000000 |       0.857143\n",
      "       0.438276 |    forms - the |       0.436436 |       0.250000 |       0.666667 |       0.400000\n",
      "       0.268597 |  forms - wales |       0.288675 |       0.166667 |       0.333333 |       0.285714\n",
      "       0.712500 |general - george |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.223214 |general - government |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |   general - in |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.458333 |general - limited |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.458333 |general - narrative |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.410147 |   general - of |       0.387298 |       0.187500 |       0.750000 |       0.315789\n",
      "       0.223214 |general - religious |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |  general - sea |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.365956 |general - study |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.458333 |general - tales |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.365956 |  general - the |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.223214 |general - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |george - government |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |    george - in |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.458333 |george - limited |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.458333 |george - narrative |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.410147 |    george - of |       0.387298 |       0.187500 |       0.750000 |       0.315789\n",
      "       0.223214 |george - religious |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |   george - sea |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.365956 | george - study |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.458333 | george - tales |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.365956 |   george - the |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.223214 | george - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.559350 |government - in |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.712500 |government - limited |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.223214 |government - narrative |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.410147 |government - of |       0.387298 |       0.187500 |       0.750000 |       0.315789\n",
      "       0.223214 |government - religious |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |government - sea |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.559350 |government - study |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.223214 |government - tales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |government - the |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.458333 |government - wales |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.559350 |   in - limited |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.180200 | in - narrative |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.287991 |        in - of |       0.292770 |       0.157895 |       0.428571 |       0.272727\n",
      "       0.180200 | in - religious |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.180200 |       in - sea |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.389610 |     in - study |       0.428571 |       0.272727 |       0.428571 |       0.428571\n",
      "       0.180200 |     in - tales |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.126374 |       in - the |       0.142857 |       0.076923 |       0.142857 |       0.142857\n",
      "       0.559350 |     in - wales |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.458333 |limited - narrative |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.410147 |   limited - of |       0.387298 |       0.187500 |       0.750000 |       0.315789\n",
      "       0.223214 |limited - religious |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |  limited - sea |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.559350 |limited - study |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.458333 |limited - tales |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.365956 |  limited - the |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.223214 |limited - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.410147 | narrative - of |       0.387298 |       0.187500 |       0.750000 |       0.315789\n",
      "       0.223214 |narrative - religious |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.458333 |narrative - sea |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.365956 |narrative - study |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.458333 |narrative - tales |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.559350 |narrative - the |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.223214 |narrative - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.134980 | of - religious |       0.129099 |       0.055556 |       0.250000 |       0.105263\n",
      "       0.271593 |       of - sea |       0.258199 |       0.117647 |       0.500000 |       0.210526\n",
      "       0.386912 |     of - study |       0.390360 |       0.222222 |       0.571429 |       0.363636\n",
      "       0.410147 |     of - tales |       0.387298 |       0.187500 |       0.750000 |       0.315789\n",
      "       0.287991 |       of - the |       0.292770 |       0.157895 |       0.428571 |       0.272727\n",
      "       0.134980 |     of - wales |       0.129099 |       0.055556 |       0.250000 |       0.105263\n",
      "       0.223214 |religious - sea |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |religious - study |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |religious - tales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |religious - the |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |religious - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |    sea - study |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |    sea - tales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.559350 |      sea - the |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.223214 |    sea - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.365956 |  study - tales |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.255952 |    study - the |       0.285714 |       0.166667 |       0.285714 |       0.285714\n",
      "       0.365956 |  study - wales |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.365956 |    tales - the |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.223214 |  tales - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |    the - wales |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Systems test  2  - Similarity measures\n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "        average |           pair |         cosine |        jaccard |        overlap |           dice\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "       0.389562 |   atlas - boon |       0.408248 |       0.250000 |       0.500000 |       0.400000\n",
      "       1.000000 |   atlas - cava |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "       0.389562 | atlas - dipped |       0.408248 |       0.250000 |       0.500000 |       0.400000\n",
      "       0.389562 |    boon - cava |       0.408248 |       0.250000 |       0.500000 |       0.400000\n",
      "       0.625000 |  boon - dipped |       0.666667 |       0.500000 |       0.666667 |       0.666667\n",
      "       0.389562 |  cava - dipped |       0.408248 |       0.250000 |       0.500000 |       0.400000\n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Systems test  3  - Similarity measures\n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "        average |           pair |         cosine |        jaccard |        overlap |           dice\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "       0.820791 |    DocA - DocB |       0.816497 |       0.666667 |       1.000000 |       0.800000\n",
      "       0.553861 |    DocA - DocC |       0.577350 |       0.400000 |       0.666667 |       0.571429\n",
      "       0.346722 |    DocB - DocC |       0.353553 |       0.200000 |       0.500000 |       0.333333\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "# Pretty print systems tests\n",
    "############################################\n",
    "\n",
    "import json\n",
    "for i in range(1,4):\n",
    "  print '—'*110\n",
    "  print \"Systems test \",i,\" - Similarity measures\"\n",
    "  print '—'*110\n",
    "  print \"{0:>15} |{1:>15} |{2:>15} |{3:>15} |{4:>15} |{5:>15}\".format(\n",
    "          \"average\", \"pair\", \"cosine\", \"jaccard\", \"overlap\", \"dice\")\n",
    "  print '-'*110\n",
    "\n",
    "  with open(\"systems_test_similarities_\"+str(i),\"r\") as f:\n",
    "      lines = f.readlines()\n",
    "      for line in lines:\n",
    "          line = line.strip()\n",
    "          avg,stripe = line.split(\"\\t\")\n",
    "          stripe = json.loads(stripe)\n",
    "\n",
    "          print \"{0:>15f} |{1:>15} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "              float(avg), stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.4.1 <a name=\"5.4.1\"></a>Full-scale experiment: EDA of Google n-grams dataset (PHASE 2)\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Do some EDA on this dataset using mrjob, e.g., \n",
    "\n",
    "- A. Longest 5-gram (number of characters)\n",
    "- B. Top 10 most frequent words (please use the count information), i.e., unigrams\n",
    "- C. 20 Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency \n",
    "- D. Distribution of 5-gram sizes (character length).  E.g., count (using the count field) up how many times a 5-gram of 50 characters shows up. Plot the data graphically using a histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.4.1 - A. Longest 5-gram (number of characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting longest5gram.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile longest5gram.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import re\n",
    "\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import logging\n",
    "import time\n",
    "\n",
    "class longest5gram(MRJob):\n",
    "    \n",
    "    # START STUDENT CODE 5.4.1.A\n",
    "    \n",
    "    MRJob.SORT_VALUES = True\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super(longest5gram, self).__init__(args)\n",
    "        self.max_count = 0\n",
    "        self.max_grams = []\n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # Split line\n",
    "        splits = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "        words = splits[0].lower().split()\n",
    "        \n",
    "        char_count = 0\n",
    "        \n",
    "        # Count characters\n",
    "        for word in words:\n",
    "            char_count += len(word)\n",
    "        \n",
    "        # Optimization: we track the max count local to the current mapper instance. If records\n",
    "        # have higher count than the max, we output them and update the max. We don't yield\n",
    "        # records that are smaller than the local max. \n",
    "        # Even some non-max records are passed on, the good thing about this is that it is extremely memory efficient\n",
    "        # in that it uses constant memory.\n",
    "        if char_count > self.max_count:\n",
    "            self.max_count = char_count\n",
    "            yield (words), char_count\n",
    "        elif char_count == self.max_count:\n",
    "            yield (words), char_count\n",
    "            \n",
    "    \n",
    "    def combiner(self, ngram, char_counts):\n",
    "        current_max = max(char_counts)\n",
    "        \n",
    "        # Optimization: we track the max count local to the current combiner instance. If records\n",
    "        # have higher count than the max, we output them and update the max. We don't yield\n",
    "        # records that are smaller than the local max, drastically reducing work on shuffling and sorting\n",
    "        # Even some non-max or local max records are passed on, the good thing about this is that it is extremely \n",
    "        # memory efficient in that it uses constant memory (just 1 integer :)\n",
    "        if current_max > self.max_count:\n",
    "            self.max_count = current_max\n",
    "            yield ngram, current_max\n",
    "        elif current_max == self.max_count:\n",
    "            yield ngram, current_max\n",
    "    \n",
    "    def reducer(self, ngram, char_counts):\n",
    "            \n",
    "        current_count = max(char_counts)\n",
    "\n",
    "        # Track in max_grams the n-grams with the max count of words\n",
    "        if current_count > self.max_count:\n",
    "            self.max_count = current_count\n",
    "            self.max_grams = [(current_count, ngram)]\n",
    "        elif current_count == self.max_count:\n",
    "            self.max_grams.append((current_count, ngram))\n",
    "\n",
    "    def reducer_final(self):\n",
    "        # Once\n",
    "        for gram in self.max_grams:\n",
    "            yield gram[0], gram[1]\n",
    "\n",
    "\n",
    "    def steps(self):\n",
    "        \n",
    "        # We need 1 reducer for this approach. However the optimizations in the mappers and combiners\n",
    "        # help us ensure that a small percentage of records get to the reducer\n",
    "        custom_jobconf = {\n",
    "            'stream.num.map.output.key.fields':'1',\n",
    "            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapred.text.key.comparator.options': '-k2,2nr',\n",
    "            'mapred.reduce.tasks': '1'\n",
    "        }\n",
    "\n",
    "        return [\n",
    "                MRStep(jobconf=custom_jobconf,\n",
    "                    mapper=self.mapper,\n",
    "                    reducer=self.reducer,\n",
    "                    combiner = self.combiner,\n",
    "                    reducer_final = self.reducer_final\n",
    "                      )\n",
    "                 ]\n",
    "\n",
    "    # END STUDENT CODE 5.4.1.A\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    longest5gram.run()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    mins = elapsed_time/float(60)\n",
    "    a = \"\"\"Elapsed time: %s seconds\n",
    "    In minutes: %s mins\"\"\" % (str(elapsed_time), str(mins))\n",
    "    logging.warning(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On test data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `systems_test_stripes_5.4.1.a_1': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/longest5gram.carlosscastro.20170619.234100.231616\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/carlosscastro/tmp/mrjob/longest5gram.carlosscastro.20170619.234100.231616/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob9120542938215890326.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497906899862_0073\n",
      "  Submitted application application_1497906899862_0073\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_0073/\n",
      "  Running job: job_1497906899862_0073\n",
      "  Job job_1497906899862_0073 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_0073 completed successfully\n",
      "  Output directory: hdfs:///user/carlosscastro/tmp/mrjob/longest5gram.carlosscastro.20170619.234100.231616/output\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=563\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=106\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=160\n",
      "\t\tFILE: Number of bytes written=401350\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1039\n",
      "\t\tHDFS: Number of bytes written=106\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=10954752\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=15088640\n",
      "\t\tTotal time spent by all map tasks (ms)=7132\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=21396\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5894\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=29470\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=7132\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=5894\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2490\n",
      "\t\tCombine input records=3\n",
      "\t\tCombine output records=3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=76\n",
      "\t\tInput split bytes=476\n",
      "\t\tMap input records=10\n",
      "\t\tMap output bytes=154\n",
      "\t\tMap output materialized bytes=176\n",
      "\t\tMap output records=3\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1908621312\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce input records=3\n",
      "\t\tReduce output records=2\n",
      "\t\tReduce shuffle bytes=176\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=6\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7755288576\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/carlosscastro/tmp/mrjob/longest5gram.carlosscastro.20170619.234100.231616/output...\n",
      "Removing HDFS temp directory hdfs:///user/carlosscastro/tmp/mrjob/longest5gram.carlosscastro.20170619.234100.231616...\n",
      "Removing temp directory /tmp/longest5gram.carlosscastro.20170619.234100.231616...\n",
      "WARNING:root:Elapsed time: 53.6010410786 seconds\n",
      "    In minutes: 0.893350684643 mins\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r systems_test_stripes_5.4.1.a_1\n",
    "!python longest5gram.py -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt > systems_test_stripes_5.4.1.a_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=2\n",
    "### Launched reduce tasks=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\t[\"a\", \"bill\", \"for\", \"establishing\", \"religious\"]\r\n",
      "29\t[\"a\", \"circumstantial\", \"narrative\", \"of\", \"the\"]\r\n"
     ]
    }
   ],
   "source": [
    "!cat systems_test_stripes_5.4.1.a_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On full data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `full_stripes_5.4.1.a': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Creating temp directory /tmp/longest5gram.carlosscastro.20170619.234156.545172\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/carlosscastro/tmp/mrjob/longest5gram.carlosscastro.20170619.234156.545172/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob9088797564860260219.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 190\n",
      "  number of splits:190\n",
      "  Submitting tokens for job: job_1497906899862_0074\n",
      "  Submitted application application_1497906899862_0074\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_0074/\n",
      "  Running job: job_1497906899862_0074\n",
      "  Job job_1497906899862_0074 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 1% reduce 0%\n",
      "   map 4% reduce 0%\n",
      "   map 11% reduce 0%\n",
      "   map 13% reduce 0%\n",
      "   map 14% reduce 0%\n",
      "   map 16% reduce 0%\n",
      "   map 18% reduce 0%\n",
      "   map 21% reduce 0%\n",
      "   map 26% reduce 0%\n",
      "   map 34% reduce 0%\n",
      "   map 42% reduce 0%\n",
      "   map 48% reduce 0%\n",
      "   map 57% reduce 0%\n",
      "   map 66% reduce 0%\n",
      "   map 82% reduce 0%\n",
      "   map 94% reduce 0%\n",
      "   map 97% reduce 0%\n",
      "   map 99% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_0074 completed successfully\n",
      "  Output directory: hdfs:///user/carlosscastro/tmp/mrjob/longest5gram.carlosscastro.20170619.234156.545172/output\n",
      "Counters: 51\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2156069116\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=360\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=37703\n",
      "\t\tFILE: Number of bytes written=25613806\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2156101116\n",
      "\t\tHDFS: Number of bytes written=360\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=573\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=191\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tOther local map tasks=2\n",
      "\t\tRack-local map tasks=189\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=8751909888\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=9937920\n",
      "\t\tTotal time spent by all map tasks (ms)=5697858\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=17093574\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3882\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=19410\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=5697858\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3882\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=563780\n",
      "\t\tCombine input records=2699\n",
      "\t\tCombine output records=1108\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=40201\n",
      "\t\tInput split bytes=32000\n",
      "\t\tMap input records=58682266\n",
      "\t\tMap output bytes=172790\n",
      "\t\tMap output materialized bytes=63827\n",
      "\t\tMap output records=2699\n",
      "\t\tMerged Map outputs=190\n",
      "\t\tPhysical memory (bytes) snapshot=153958842368\n",
      "\t\tReduce input groups=1108\n",
      "\t\tReduce input records=1108\n",
      "\t\tReduce output records=2\n",
      "\t\tReduce shuffle bytes=63827\n",
      "\t\tShuffled Maps =190\n",
      "\t\tSpilled Records=2216\n",
      "\t\tTotal committed heap usage (bytes)=299992875008\n",
      "\t\tVirtual memory (bytes) snapshot=420906733568\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/carlosscastro/tmp/mrjob/longest5gram.carlosscastro.20170619.234156.545172/output...\n",
      "Removing HDFS temp directory hdfs:///user/carlosscastro/tmp/mrjob/longest5gram.carlosscastro.20170619.234156.545172...\n",
      "Removing temp directory /tmp/longest5gram.carlosscastro.20170619.234156.545172...\n",
      "WARNING:root:Elapsed time: 91.2285780907 seconds\n",
      "    In minutes: 1.52047630151 mins\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r full_stripes_5.4.1.a\n",
    "!python longest5gram.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > full_stripes_5.4.1.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=2\n",
    "### Launched reduce tasks=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155\t[\"roplezimpredastrodonbraslpklson\", \"yhroaclmparcheyxmmioudavesaurus\", \"piofpilocowersuruasogetsesnegcp\", \"tyravopsifengoquapialloboskenuo\", \"owinfuyaiokenecksasxhyilpoynuat\"]\r\n",
      "155\t[\"aiopjumrxuyvaslyhypsibemapodikr\", \"ufrydiuuolbigasuaurusrexlisnaye\", \"rnoondqsrunsubunougrabberyairtc\", \"utahraptoredileipmilbdummyuveri\", \"syevrahvelocyallosauruslinrotsr\"]\r\n"
     ]
    }
   ],
   "source": [
    "!cat full_stripes_5.4.1.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report Stats: \n",
    "## example: \n",
    "## Longest 5grams MR stats\n",
    "\n",
    "    ec2_instance_type: m3.xlarge\n",
    "    num_ec2_instances: 15\n",
    "\n",
    "__Step 1:__  \n",
    "\n",
    "    RUNNING for 107.0s ~= 2 minutes  \n",
    "    Reduce tasks = 16 \n",
    "    \n",
    "__Step 2:__   \n",
    "\n",
    "    RUNNING for 108.8s ~= 2 minutes\n",
    "    Reduce tasks = 1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.4.1 - B. Top 10 most frequent words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mostFrequentWords.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mostFrequentWords.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import re\n",
    "\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import logging\n",
    "import time\n",
    "\n",
    "class mostFrequentWords(MRJob):\n",
    "\n",
    "    # START STUDENT CODE 5.4.1.B\n",
    "            \n",
    "    MRJob.SORT_VALUES = True\n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # Split line\n",
    "        splits = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "        words = splits[0].lower().split()\n",
    "        count = int(splits[1])\n",
    "        \n",
    "        for word in words:\n",
    "            yield word, count\n",
    "            \n",
    "    \n",
    "    def combiner(self, word, counts):\n",
    "        total = sum(count for count in counts)\n",
    "        yield word, total\n",
    "    \n",
    "    def reducer(self, word, counts):\n",
    "        total = sum(count for count in counts)\n",
    "        yield total, word\n",
    "    \n",
    "    def max_reducer(self, count, words):\n",
    "        for word in words:\n",
    "            yield word, count\n",
    "\n",
    "    def steps(self):\n",
    "        \n",
    "        custom_jobconf = {\n",
    "            'stream.num.map.output.key.fields':'2',\n",
    "            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapred.text.key.comparator.options': '-k1,1nr',\n",
    "            'mapred.reduce.tasks': '1'\n",
    "        }\n",
    "\n",
    "        return [\n",
    "                MRStep(mapper=self.mapper,\n",
    "                    reducer=self.reducer,\n",
    "                    combiner = self.combiner),\n",
    "                MRStep(jobconf=custom_jobconf,\n",
    "                       reducer=self.max_reducer)\n",
    "                 ]\n",
    "\n",
    "    # END STUDENT CODE 5.4.1.B\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    mostFrequentWords.run()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    mins = elapsed_time/float(60)\n",
    "    a = \"\"\"Elapsed time: %s seconds\n",
    "    In minutes: %s mins\"\"\" % (str(elapsed_time), str(mins))\n",
    "    logging.warning(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On the test data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `systems_test_stripes_5.4.1.b_1': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/mostFrequentWords.carlosscastro.20170619.234330.489579\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/carlosscastro/tmp/mrjob/mostFrequentWords.carlosscastro.20170619.234330.489579/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob8750166049497529789.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497906899862_0075\n",
      "  Submitted application application_1497906899862_0075\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_0075/\n",
      "  Running job: job_1497906899862_0075\n",
      "  Job job_1497906899862_0075 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_0075 completed successfully\n",
      "  Output directory: hdfs:///user/carlosscastro/tmp/mrjob/mostFrequentWords.carlosscastro.20170619.234330.489579/step-output/0000\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=563\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=357\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=430\n",
      "\t\tFILE: Number of bytes written=401096\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1049\n",
      "\t\tHDFS: Number of bytes written=357\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=11344896\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=14635520\n",
      "\t\tTotal time spent by all map tasks (ms)=7386\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=22158\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5717\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=28585\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=7386\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=5717\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2560\n",
      "\t\tCombine input records=50\n",
      "\t\tCombine output records=31\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=98\n",
      "\t\tInput split bytes=486\n",
      "\t\tMap input records=10\n",
      "\t\tMap output bytes=602\n",
      "\t\tMap output materialized bytes=458\n",
      "\t\tMap output records=50\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1908690944\n",
      "\t\tReduce input groups=28\n",
      "\t\tReduce input records=31\n",
      "\t\tReduce output records=28\n",
      "\t\tReduce shuffle bytes=458\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=62\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7728197632\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob90661395402709.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497906899862_0076\n",
      "  Submitted application application_1497906899862_0076\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_0076/\n",
      "  Running job: job_1497906899862_0076\n",
      "  Job job_1497906899862_0076 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_0076 completed successfully\n",
      "  Output directory: hdfs:///user/carlosscastro/tmp/mrjob/mostFrequentWords.carlosscastro.20170619.234330.489579/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=536\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=357\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=396\n",
      "\t\tFILE: Number of bytes written=400546\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=936\n",
      "\t\tHDFS: Number of bytes written=357\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=18290688\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=9233920\n",
      "\t\tTotal time spent by all map tasks (ms)=11908\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=35724\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3607\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=18035\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=11908\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3607\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2410\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=104\n",
      "\t\tInput split bytes=400\n",
      "\t\tMap input records=28\n",
      "\t\tMap output bytes=385\n",
      "\t\tMap output materialized bytes=437\n",
      "\t\tMap output records=28\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1912434688\n",
      "\t\tReduce input groups=28\n",
      "\t\tReduce input records=28\n",
      "\t\tReduce output records=28\n",
      "\t\tReduce shuffle bytes=437\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=56\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7757537280\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/carlosscastro/tmp/mrjob/mostFrequentWords.carlosscastro.20170619.234330.489579/output...\n",
      "Removing HDFS temp directory hdfs:///user/carlosscastro/tmp/mrjob/mostFrequentWords.carlosscastro.20170619.234330.489579...\n",
      "Removing temp directory /tmp/mostFrequentWords.carlosscastro.20170619.234330.489579...\n",
      "WARNING:root:Elapsed time: 85.9819910526 seconds\n",
      "    In minutes: 1.43303318421 mins\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r systems_test_stripes_5.4.1.b_1\n",
    "!python mostFrequentWords.py -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt > systems_test_stripes_5.4.1.b_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=2\n",
    "### Launched reduce tasks=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"a\"\t2217\r\n",
      "\"in\"\t1201\r\n",
      "\"wales\"\t1099\r\n",
      "\"christmas\"\t1099\r\n",
      "\"child's\"\t1099\r\n"
     ]
    }
   ],
   "source": [
    "!head -5 systems_test_stripes_5.4.1.b_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On the full data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `full_mostFrequentWords_5.4.1.b': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Creating temp directory /tmp/mostFrequentWords.carlosscastro.20170619.234459.174974\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/carlosscastro/tmp/mrjob/mostFrequentWords.carlosscastro.20170619.234459.174974/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob3546579141812141352.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 190\n",
      "  number of splits:190\n",
      "  Submitting tokens for job: job_1497906899862_0077\n",
      "  Submitted application application_1497906899862_0077\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_0077/\n",
      "  Running job: job_1497906899862_0077\n",
      "  Job job_1497906899862_0077 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 1% reduce 0%\n",
      "   map 2% reduce 0%\n",
      "   map 3% reduce 0%\n",
      "   map 4% reduce 0%\n",
      "   map 5% reduce 0%\n",
      "   map 6% reduce 0%\n",
      "   map 7% reduce 0%\n",
      "   map 8% reduce 0%\n",
      "   map 9% reduce 0%\n",
      "   map 10% reduce 0%\n",
      "   map 11% reduce 0%\n",
      "   map 12% reduce 0%\n",
      "   map 13% reduce 0%\n",
      "   map 14% reduce 0%\n",
      "   map 15% reduce 0%\n",
      "   map 16% reduce 0%\n",
      "   map 17% reduce 0%\n",
      "   map 19% reduce 0%\n",
      "   map 20% reduce 0%\n",
      "   map 21% reduce 0%\n",
      "   map 22% reduce 0%\n",
      "   map 23% reduce 0%\n",
      "   map 24% reduce 0%\n",
      "   map 25% reduce 0%\n",
      "   map 26% reduce 0%\n",
      "   map 27% reduce 0%\n",
      "   map 28% reduce 0%\n",
      "   map 29% reduce 0%\n",
      "   map 30% reduce 0%\n",
      "   map 31% reduce 0%\n",
      "   map 32% reduce 0%\n",
      "   map 33% reduce 0%\n",
      "   map 34% reduce 0%\n",
      "   map 35% reduce 0%\n",
      "   map 36% reduce 0%\n",
      "   map 37% reduce 0%\n",
      "   map 38% reduce 0%\n",
      "   map 39% reduce 0%\n",
      "   map 40% reduce 0%\n",
      "   map 41% reduce 0%\n",
      "   map 42% reduce 0%\n",
      "   map 43% reduce 0%\n",
      "   map 44% reduce 0%\n",
      "   map 45% reduce 0%\n",
      "   map 46% reduce 0%\n",
      "   map 47% reduce 0%\n",
      "   map 48% reduce 0%\n",
      "   map 49% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 51% reduce 0%\n",
      "   map 52% reduce 0%\n",
      "   map 53% reduce 0%\n",
      "   map 54% reduce 0%\n",
      "   map 55% reduce 0%\n",
      "   map 56% reduce 0%\n",
      "   map 57% reduce 0%\n",
      "   map 58% reduce 0%\n",
      "   map 59% reduce 0%\n",
      "   map 60% reduce 0%\n",
      "   map 61% reduce 0%\n",
      "   map 62% reduce 0%\n",
      "   map 63% reduce 0%\n",
      "   map 64% reduce 0%\n",
      "   map 65% reduce 0%\n",
      "   map 66% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 68% reduce 0%\n",
      "   map 69% reduce 0%\n",
      "   map 70% reduce 0%\n",
      "   map 71% reduce 0%\n",
      "   map 73% reduce 0%\n",
      "   map 74% reduce 0%\n",
      "   map 75% reduce 0%\n",
      "   map 76% reduce 0%\n",
      "   map 78% reduce 0%\n",
      "   map 79% reduce 0%\n",
      "   map 80% reduce 0%\n",
      "   map 82% reduce 0%\n",
      "   map 84% reduce 0%\n",
      "   map 85% reduce 0%\n",
      "   map 87% reduce 0%\n",
      "   map 88% reduce 0%\n",
      "   map 90% reduce 0%\n",
      "   map 91% reduce 0%\n",
      "   map 92% reduce 0%\n",
      "   map 93% reduce 0%\n",
      "   map 94% reduce 0%\n",
      "   map 95% reduce 0%\n",
      "   map 96% reduce 0%\n",
      "   map 97% reduce 0%\n",
      "   map 98% reduce 0%\n",
      "   map 99% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 67%\n",
      "   map 100% reduce 68%\n",
      "   map 100% reduce 69%\n",
      "   map 100% reduce 70%\n",
      "   map 100% reduce 71%\n",
      "   map 100% reduce 72%\n",
      "   map 100% reduce 73%\n",
      "   map 100% reduce 74%\n",
      "   map 100% reduce 75%\n",
      "   map 100% reduce 76%\n",
      "   map 100% reduce 77%\n",
      "   map 100% reduce 78%\n",
      "   map 100% reduce 79%\n",
      "   map 100% reduce 80%\n",
      "   map 100% reduce 81%\n",
      "   map 100% reduce 82%\n",
      "   map 100% reduce 83%\n",
      "   map 100% reduce 84%\n",
      "   map 100% reduce 85%\n",
      "   map 100% reduce 86%\n",
      "   map 100% reduce 87%\n",
      "   map 100% reduce 88%\n",
      "   map 100% reduce 89%\n",
      "   map 100% reduce 90%\n",
      "   map 100% reduce 91%\n",
      "   map 100% reduce 92%\n",
      "   map 100% reduce 93%\n",
      "   map 100% reduce 94%\n",
      "   map 100% reduce 95%\n",
      "   map 100% reduce 96%\n",
      "   map 100% reduce 97%\n",
      "   map 100% reduce 98%\n",
      "   map 100% reduce 99%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_0077 completed successfully\n",
      "  Output directory: hdfs:///user/carlosscastro/tmp/mrjob/mostFrequentWords.carlosscastro.20170619.234459.174974/step-output/0000\n",
      "Counters: 51\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2156069116\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=4158739\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=39014765\n",
      "\t\tFILE: Number of bytes written=138276024\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2156101116\n",
      "\t\tHDFS: Number of bytes written=4158739\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=573\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=2\n",
      "\t\tLaunched map tasks=192\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tOther local map tasks=2\n",
      "\t\tRack-local map tasks=190\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=44362337280\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=322554880\n",
      "\t\tTotal time spent by all map tasks (ms)=28881730\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=86645190\n",
      "\t\tTotal time spent by all reduce tasks (ms)=125998\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=629990\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=28881730\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=125998\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=15375340\n",
      "\t\tCombine input records=293411330\n",
      "\t\tCombine output records=6822745\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=121092\n",
      "\t\tInput split bytes=32000\n",
      "\t\tMap input records=58682266\n",
      "\t\tMap output bytes=3430141090\n",
      "\t\tMap output materialized bytes=73800744\n",
      "\t\tMap output records=293411330\n",
      "\t\tMerged Map outputs=190\n",
      "\t\tPhysical memory (bytes) snapshot=154684665856\n",
      "\t\tReduce input groups=269339\n",
      "\t\tReduce input records=6822745\n",
      "\t\tReduce output records=269339\n",
      "\t\tReduce shuffle bytes=73800744\n",
      "\t\tShuffled Maps =190\n",
      "\t\tSpilled Records=13645490\n",
      "\t\tTotal committed heap usage (bytes)=298020503552\n",
      "\t\tVirtual memory (bytes) snapshot=421431128064\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob4339919534488822237.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497906899862_0079\n",
      "  Submitted application application_1497906899862_0079\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_0079/\n",
      "  Running job: job_1497906899862_0079\n",
      "  Job job_1497906899862_0079 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 88%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_0079 completed successfully\n",
      "  Output directory: hdfs:///user/carlosscastro/tmp/mrjob/mostFrequentWords.carlosscastro.20170619.234459.174974/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=4176522\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=4158739\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2953963\n",
      "\t\tFILE: Number of bytes written=6322951\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=4176922\n",
      "\t\tHDFS: Number of bytes written=4158739\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=20367360\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=31055360\n",
      "\t\tTotal time spent by all map tasks (ms)=13260\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=39780\n",
      "\t\tTotal time spent by all reduce tasks (ms)=12131\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=60655\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=13260\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=12131\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=13720\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=194\n",
      "\t\tInput split bytes=400\n",
      "\t\tMap input records=269339\n",
      "\t\tMap output bytes=4428078\n",
      "\t\tMap output materialized bytes=2969260\n",
      "\t\tMap output records=269339\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1952821248\n",
      "\t\tReduce input groups=269339\n",
      "\t\tReduce input records=269339\n",
      "\t\tReduce output records=269339\n",
      "\t\tReduce shuffle bytes=2969260\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=538678\n",
      "\t\tTotal committed heap usage (bytes)=5280104448\n",
      "\t\tVirtual memory (bytes) snapshot=7753207808\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/carlosscastro/tmp/mrjob/mostFrequentWords.carlosscastro.20170619.234459.174974/output...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing HDFS temp directory hdfs:///user/carlosscastro/tmp/mrjob/mostFrequentWords.carlosscastro.20170619.234459.174974...\n",
      "Removing temp directory /tmp/mostFrequentWords.carlosscastro.20170619.234459.174974...\n",
      "WARNING:root:Elapsed time: 445.191097021 seconds\n",
      "    In minutes: 7.41985161702 mins\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r full_mostFrequentWords_5.4.1.b\n",
    "!python mostFrequentWords.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > full_mostFrequentWords_5.4.1.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=2\n",
    "### Launched reduce tasks=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"the\"\t5490815394\r\n",
      "\"of\"\t3698583299\r\n",
      "\"to\"\t2227866570\r\n",
      "\"in\"\t1421312776\r\n",
      "\"a\"\t1361123022\r\n"
     ]
    }
   ],
   "source": [
    "!head -5 full_mostFrequentWords_5.4.1.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most frequent words MR stats\n",
    "    \n",
    "    ec2_instance_type: m3.xlarge\n",
    "    num_ec2_instances: 15\n",
    "    \n",
    "__Step 1:__   \n",
    "\n",
    "    RUNNING for 590.7s ~= 10 minutes   \n",
    "    Launched map tasks=191  \n",
    "    Launched reduce tasks=57   \n",
    "\n",
    "__Step 2:__  \n",
    "\n",
    "    RUNNING for 76.6s   \n",
    "    Launched map tasks=110\n",
    "    Launched reduce tasks=16  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.4.1 - C. 20 Most/Least densely appearing words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mostLeastDenseWords.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mostLeastDenseWords.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import division\n",
    "import re\n",
    "import numpy as np\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import time\n",
    "import logging\n",
    "\n",
    "class mostLeastDenseWords(MRJob):\n",
    "    \n",
    "    # START STUDENT CODE 5.4.1.C\n",
    "           \n",
    "    MRJob.SORT_VALUES = True\n",
    "        \n",
    "    def __init__(self, args):\n",
    "        super(mostLeastDenseWords, self).__init__(args)\n",
    "        self.total_word_count = None\n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # Split line\n",
    "        splits = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "        words = splits[0].lower().split()\n",
    "        count = int(splits[1])\n",
    "        \n",
    "        for word in words:\n",
    "            yield \"*\", count\n",
    "            yield word, count\n",
    "            \n",
    "    \n",
    "    def combiner(self, word, counts):\n",
    "        total = sum(count for count in counts)\n",
    "        yield word, total\n",
    "    \n",
    "    def reducer(self, word, counts):\n",
    "    \n",
    "        total = sum(count for count in counts)\n",
    "        \n",
    "        if word == \"*\":\n",
    "            self.total_word_count = total\n",
    "        else:\n",
    "            yield float(total) / float(self.total_word_count), word\n",
    "    \n",
    "    def max_reducer(self, count, words):\n",
    "        for word in words:\n",
    "            yield word, count\n",
    "\n",
    "    def steps(self):\n",
    "        \n",
    "        custom_jobconf = {\n",
    "            'stream.num.map.output.key.fields':'2',\n",
    "            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapred.text.key.comparator.options': '-g -k1,1nr',\n",
    "            'mapred.reduce.tasks': '1'\n",
    "        }\n",
    "\n",
    "        return [\n",
    "                MRStep(\n",
    "                    mapper=self.mapper,\n",
    "                    reducer=self.reducer,\n",
    "                    combiner = self.combiner),\n",
    "                MRStep(jobconf=custom_jobconf,\n",
    "                       reducer=self.max_reducer)\n",
    "                 ]\n",
    "    \n",
    "    # END STUDENT CODE 5.4.1.C\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    mostLeastDenseWords.run()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    mins = elapsed_time/float(60)\n",
    "    a = \"\"\"Elapsed time: %s seconds\n",
    "    In minutes: %s mins\"\"\" % (str(elapsed_time), str(mins))\n",
    "    logging.warning(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On the test data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `systems_test_stripes_5.4.1.c_1': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/mostLeastDenseWords.carlosscastro.20170619.235227.146750\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/carlosscastro/tmp/mrjob/mostLeastDenseWords.carlosscastro.20170619.235227.146750/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob6441761054186289522.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497906899862_0080\n",
      "  Submitted application application_1497906899862_0080\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_0080/\n",
      "  Running job: job_1497906899862_0080\n",
      "  Job job_1497906899862_0080 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_0080 completed successfully\n",
      "  Output directory: hdfs:///user/carlosscastro/tmp/mrjob/mostLeastDenseWords.carlosscastro.20170619.235227.146750/step-output/0000\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=563\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=850\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=447\n",
      "\t\tFILE: Number of bytes written=401228\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1053\n",
      "\t\tHDFS: Number of bytes written=850\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=19226112\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=17840640\n",
      "\t\tTotal time spent by all map tasks (ms)=12517\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=37551\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6969\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=34845\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=12517\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=6969\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2930\n",
      "\t\tCombine input records=100\n",
      "\t\tCombine output records=33\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=168\n",
      "\t\tInput split bytes=490\n",
      "\t\tMap input records=10\n",
      "\t\tMap output bytes=1032\n",
      "\t\tMap output materialized bytes=477\n",
      "\t\tMap output records=100\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1899401216\n",
      "\t\tReduce input groups=29\n",
      "\t\tReduce input records=33\n",
      "\t\tReduce output records=28\n",
      "\t\tReduce shuffle bytes=477\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=66\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7728975872\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob6390422704013569564.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497906899862_0081\n",
      "  Submitted application application_1497906899862_0081\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_0081/\n",
      "  Running job: job_1497906899862_0081\n",
      "  Job job_1497906899862_0081 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_0081 completed successfully\n",
      "  Output directory: hdfs:///user/carlosscastro/tmp/mrjob/mostLeastDenseWords.carlosscastro.20170619.235227.146750/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1275\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=850\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=633\n",
      "\t\tFILE: Number of bytes written=401234\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1679\n",
      "\t\tHDFS: Number of bytes written=850\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=17255424\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=17226240\n",
      "\t\tTotal time spent by all map tasks (ms)=11234\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=33702\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6729\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=33645\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=11234\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=6729\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2640\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=93\n",
      "\t\tInput split bytes=404\n",
      "\t\tMap input records=28\n",
      "\t\tMap output bytes=878\n",
      "\t\tMap output materialized bytes=780\n",
      "\t\tMap output records=28\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1896321024\n",
      "\t\tReduce input groups=28\n",
      "\t\tReduce input records=28\n",
      "\t\tReduce output records=28\n",
      "\t\tReduce shuffle bytes=780\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=56\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7708250112\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/carlosscastro/tmp/mrjob/mostLeastDenseWords.carlosscastro.20170619.235227.146750/output...\n",
      "Removing HDFS temp directory hdfs:///user/carlosscastro/tmp/mrjob/mostLeastDenseWords.carlosscastro.20170619.235227.146750...\n",
      "Removing temp directory /tmp/mostLeastDenseWords.carlosscastro.20170619.235227.146750...\n",
      "WARNING:root:Elapsed time: 92.5123889446 seconds\n",
      "    In minutes: 1.54187314908 mins\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r systems_test_stripes_5.4.1.c_1\n",
    "!python mostLeastDenseWords.py -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt > systems_test_stripes_5.4.1.c_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=2\n",
    "### Launched reduce tasks=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"a\"\t0.20000000000000001\r\n",
      "\"in\"\t0.1083446098331078\r\n",
      "\"wales\"\t0.099142986017140278\r\n",
      "\"christmas\"\t0.099142986017140278\r\n",
      "\"child's\"\t0.099142986017140278\r\n",
      "\"of\"\t0.091204330175913395\r\n",
      "\"study\"\t0.054488046910239063\r\n",
      "\"case\"\t0.054488046910239063\r\n",
      "\"female\"\t0.04032476319350474\r\n",
      "\"collection\"\t0.021560667568786648\r\n",
      "\"the\"\t0.011186287776274244\r\n",
      "\"tales\"\t0.011096075778078484\r\n",
      "\"fairy\"\t0.011096075778078484\r\n",
      "\"forms\"\t0.010464591790708164\r\n",
      "\"government\"\t0.0092016238159675235\r\n",
      "\"george\"\t0.0082995038340099234\r\n",
      "\"general\"\t0.0082995038340099234\r\n",
      "\"biography\"\t0.0082995038340099234\r\n",
      "\"city\"\t0.0055931438881371221\r\n",
      "\"circumstantial\"\t0.0055931438881371221\r\n",
      "\"by\"\t0.0055931438881371221\r\n",
      "\"sea\"\t0.0055931438881371221\r\n",
      "\"narrative\"\t0.0055931438881371221\r\n",
      "\"religious\"\t0.0053225078935498424\r\n",
      "\"establishing\"\t0.0053225078935498424\r\n",
      "\"for\"\t0.0053225078935498424\r\n",
      "\"bill\"\t0.0053225078935498424\r\n",
      "\"limited\"\t0.0049616599007668016\r\n"
     ]
    }
   ],
   "source": [
    "!cat systems_test_stripes_5.4.1.c_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On the full data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `full_mostLeastDenseWords_5.4.1.c': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Creating temp directory /tmp/mostLeastDenseWords.carlosscastro.20170619.235402.437440\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/carlosscastro/tmp/mrjob/mostLeastDenseWords.carlosscastro.20170619.235402.437440/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob3588233408732778077.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 190\n",
      "  number of splits:190\n",
      "  Submitting tokens for job: job_1497906899862_0083\n",
      "  Submitted application application_1497906899862_0083\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_0083/\n",
      "  Running job: job_1497906899862_0083\n",
      "  Job job_1497906899862_0083 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 1% reduce 0%\n",
      "   map 2% reduce 0%\n",
      "   map 3% reduce 0%\n",
      "   map 4% reduce 0%\n",
      "   map 5% reduce 0%\n",
      "   map 6% reduce 0%\n",
      "   map 7% reduce 0%\n",
      "   map 8% reduce 0%\n",
      "   map 9% reduce 0%\n",
      "   map 10% reduce 0%\n",
      "   map 11% reduce 0%\n",
      "   map 12% reduce 0%\n",
      "   map 13% reduce 0%\n",
      "   map 14% reduce 0%\n",
      "   map 15% reduce 0%\n",
      "   map 16% reduce 0%\n",
      "   map 17% reduce 0%\n",
      "   map 18% reduce 0%\n",
      "   map 19% reduce 0%\n",
      "   map 20% reduce 0%\n",
      "   map 21% reduce 0%\n",
      "   map 22% reduce 0%\n",
      "   map 23% reduce 0%\n",
      "   map 24% reduce 0%\n",
      "   map 25% reduce 0%\n",
      "   map 26% reduce 0%\n",
      "   map 27% reduce 0%\n",
      "   map 28% reduce 0%\n",
      "   map 29% reduce 0%\n",
      "   map 30% reduce 0%\n",
      "   map 31% reduce 0%\n",
      "   map 32% reduce 0%\n",
      "   map 33% reduce 0%\n",
      "   map 34% reduce 0%\n",
      "   map 35% reduce 0%\n",
      "   map 36% reduce 0%\n",
      "   map 37% reduce 0%\n",
      "   map 38% reduce 0%\n",
      "   map 39% reduce 0%\n",
      "   map 40% reduce 0%\n",
      "   map 41% reduce 0%\n",
      "   map 42% reduce 0%\n",
      "   map 43% reduce 0%\n",
      "   map 44% reduce 0%\n",
      "   map 45% reduce 0%\n",
      "   map 46% reduce 0%\n",
      "   map 47% reduce 0%\n",
      "   map 48% reduce 0%\n",
      "   map 49% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 51% reduce 0%\n",
      "   map 52% reduce 0%\n",
      "   map 53% reduce 0%\n",
      "   map 54% reduce 0%\n",
      "   map 55% reduce 0%\n",
      "   map 56% reduce 0%\n",
      "   map 57% reduce 0%\n",
      "   map 58% reduce 0%\n",
      "   map 59% reduce 0%\n",
      "   map 60% reduce 0%\n",
      "   map 61% reduce 0%\n",
      "   map 62% reduce 0%\n",
      "   map 63% reduce 0%\n",
      "   map 64% reduce 0%\n",
      "   map 65% reduce 0%\n",
      "   map 66% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 68% reduce 0%\n",
      "   map 69% reduce 0%\n",
      "   map 70% reduce 0%\n",
      "   map 71% reduce 0%\n",
      "   map 72% reduce 0%\n",
      "   map 73% reduce 0%\n",
      "   map 74% reduce 0%\n",
      "   map 75% reduce 0%\n",
      "   map 76% reduce 0%\n",
      "   map 78% reduce 0%\n",
      "   map 80% reduce 0%\n",
      "   map 81% reduce 0%\n",
      "   map 84% reduce 0%\n",
      "   map 86% reduce 0%\n",
      "   map 88% reduce 0%\n",
      "   map 89% reduce 0%\n",
      "   map 90% reduce 0%\n",
      "   map 91% reduce 0%\n",
      "   map 92% reduce 0%\n",
      "   map 93% reduce 0%\n",
      "   map 94% reduce 0%\n",
      "   map 95% reduce 0%\n",
      "   map 96% reduce 0%\n",
      "   map 97% reduce 0%\n",
      "   map 98% reduce 0%\n",
      "   map 99% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 65%\n",
      "   map 100% reduce 67%\n",
      "   map 100% reduce 68%\n",
      "   map 100% reduce 69%\n",
      "   map 100% reduce 70%\n",
      "   map 100% reduce 71%\n",
      "   map 100% reduce 72%\n",
      "   map 100% reduce 73%\n",
      "   map 100% reduce 74%\n",
      "   map 100% reduce 75%\n",
      "   map 100% reduce 76%\n",
      "   map 100% reduce 77%\n",
      "   map 100% reduce 78%\n",
      "   map 100% reduce 79%\n",
      "   map 100% reduce 80%\n",
      "   map 100% reduce 81%\n",
      "   map 100% reduce 82%\n",
      "   map 100% reduce 83%\n",
      "   map 100% reduce 84%\n",
      "   map 100% reduce 85%\n",
      "   map 100% reduce 86%\n",
      "   map 100% reduce 87%\n",
      "   map 100% reduce 88%\n",
      "   map 100% reduce 89%\n",
      "   map 100% reduce 90%\n",
      "   map 100% reduce 91%\n",
      "   map 100% reduce 92%\n",
      "   map 100% reduce 93%\n",
      "   map 100% reduce 94%\n",
      "   map 100% reduce 95%\n",
      "   map 100% reduce 96%\n",
      "   map 100% reduce 97%\n",
      "   map 100% reduce 98%\n",
      "   map 100% reduce 99%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_0083 completed successfully\n",
      "  Output directory: hdfs:///user/carlosscastro/tmp/mrjob/mostLeastDenseWords.carlosscastro.20170619.235402.437440/step-output/0000\n",
      "Counters: 51\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2156069116\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=9190412\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=39018054\n",
      "\t\tFILE: Number of bytes written=138288416\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2156101116\n",
      "\t\tHDFS: Number of bytes written=9190412\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=573\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=191\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tOther local map tasks=2\n",
      "\t\tRack-local map tasks=189\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=79031497728\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=320286720\n",
      "\t\tTotal time spent by all map tasks (ms)=51452798\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=154358394\n",
      "\t\tTotal time spent by all reduce tasks (ms)=125112\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=625560\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=51452798\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=125112\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=28829620\n",
      "\t\tCombine input records=586822660\n",
      "\t\tCombine output records=6822933\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=177854\n",
      "\t\tInput split bytes=32000\n",
      "\t\tMap input records=58682266\n",
      "\t\tMap output bytes=5878243690\n",
      "\t\tMap output materialized bytes=73804117\n",
      "\t\tMap output records=586822660\n",
      "\t\tMerged Map outputs=190\n",
      "\t\tPhysical memory (bytes) snapshot=154456424448\n",
      "\t\tReduce input groups=269340\n",
      "\t\tReduce input records=6822933\n",
      "\t\tReduce output records=269339\n",
      "\t\tReduce shuffle bytes=73804117\n",
      "\t\tShuffled Maps =190\n",
      "\t\tSpilled Records=13645866\n",
      "\t\tTotal committed heap usage (bytes)=295949041664\n",
      "\t\tVirtual memory (bytes) snapshot=421382959104\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob226850176681672799.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497906899862_0085\n",
      "  Submitted application application_1497906899862_0085\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_0085/\n",
      "  Running job: job_1497906899862_0085\n",
      "  Job job_1497906899862_0085 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 85%\n",
      "   map 100% reduce 95%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_0085 completed successfully\n",
      "  Output directory: hdfs:///user/carlosscastro/tmp/mrjob/mostLeastDenseWords.carlosscastro.20170619.235402.437440/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=9313798\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=9190412\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=3749377\n",
      "\t\tFILE: Number of bytes written=8033214\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=9314202\n",
      "\t\tHDFS: Number of bytes written=9190412\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=20904960\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=34186240\n",
      "\t\tTotal time spent by all map tasks (ms)=13610\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=40830\n",
      "\t\tTotal time spent by all reduce tasks (ms)=13354\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=66770\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=13610\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=13354\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=17040\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=153\n",
      "\t\tInput split bytes=404\n",
      "\t\tMap input records=269339\n",
      "\t\tMap output bytes=9459751\n",
      "\t\tMap output materialized bytes=3884019\n",
      "\t\tMap output records=269339\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1949487104\n",
      "\t\tReduce input groups=269339\n",
      "\t\tReduce input records=269339\n",
      "\t\tReduce output records=269339\n",
      "\t\tReduce shuffle bytes=3884019\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=538678\n",
      "\t\tTotal committed heap usage (bytes)=5280104448\n",
      "\t\tVirtual memory (bytes) snapshot=7751405568\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/carlosscastro/tmp/mrjob/mostLeastDenseWords.carlosscastro.20170619.235402.437440/output...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing HDFS temp directory hdfs:///user/carlosscastro/tmp/mrjob/mostLeastDenseWords.carlosscastro.20170619.235402.437440...\n",
      "Removing temp directory /tmp/mostLeastDenseWords.carlosscastro.20170619.235402.437440...\n",
      "WARNING:root:Elapsed time: 635.000291109 seconds\n",
      "    In minutes: 10.5833381852 mins\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r full_mostLeastDenseWords_5.4.1.c\n",
    "!python mostLeastDenseWords.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > full_mostLeastDenseWords_5.4.1.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=192\n",
    "### Launched reduce tasks=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Highest frequency words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"particulars\"\t9.999991202155478e-06\r\n",
      "\"venn\"\t9.9998902747203268e-08\r\n",
      "\"shortage\"\t9.9992645246223945e-06\r\n",
      "\"melville's\"\t9.9978717260173162e-08\r\n",
      "\"heine\"\t9.9978717260173162e-08\r\n",
      "\"gloat\"\t9.9978717260173162e-08\r\n",
      "\"exxon\"\t9.9978717260173162e-08\r\n",
      "\"anachronistic\"\t9.9978717260173162e-08\r\n",
      "\"trail\"\t9.9970239355620498e-06\r\n",
      "\"letzten\"\t9.9958531773143042e-08\r\n",
      "\"playgrounds\"\t9.9958531773143042e-08\r\n",
      "\"breathings\"\t9.9958531773143042e-08\r\n",
      "\"exceptionable\"\t9.9958531773143042e-08\r\n",
      "\"formalize\"\t9.9958531773143042e-08\r\n",
      "\"hates\"\t9.9942383383518938e-07\r\n",
      "\"coastwise\"\t9.9938346286112922e-08\r\n",
      "\"dividers\"\t9.9938346286112922e-08\r\n",
      "\"assents\"\t9.9938346286112922e-08\r\n",
      "\"residents\"\t9.9936529592280203e-06\r\n",
      "\"smiths\"\t9.9918160799082802e-08\r\n"
     ]
    }
   ],
   "source": [
    "!head -20 full_mostLeastDenseWords_5.4.1.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowest frequency words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"continued\"\t0.00010249356653279443\r\n",
      "\"twenty\"\t0.0001022841017338829\r\n",
      "\"heat\"\t0.00010214243998590554\r\n",
      "\"test\"\t0.00010213036906466152\r\n",
      "\"lines\"\t0.00010208735379180034\r\n",
      "\"towards\"\t0.00010163723761651575\r\n",
      "\"difficulty\"\t0.00010160754476509445\r\n",
      "\"income\"\t0.00010148736037531712\r\n",
      "\"mass\"\t0.00010128009579449188\r\n",
      "\"especially\"\t0.00010121792449443911\r\n",
      "\"kingdom\"\t0.00010088728621688579\r\n",
      "\"religion\"\t0.00010079972157414914\r\n",
      "\"former\"\t0.00010066784978738138\r\n",
      "\"obtain\"\t0.00010047905492718869\r\n",
      "\"aspects\"\t0.00010047713730592082\r\n",
      "\"parties\"\t0.0001004460920268685\r\n",
      "\"outside\"\t0.00010036181761851775\r\n",
      "\"mouth\"\t0.00010033606093706733\r\n",
      "\"remember\"\t0.00010031351374805469\r\n",
      "\"treated\"\t0.00010018121806605929\r\n"
     ]
    }
   ],
   "source": [
    "!tail -20 full_mostLeastDenseWords_5.4.1.c\n",
    "#TODO revert order probably"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word density MR stats\n",
    "\n",
    "    ec2_instance_type: m3.xlarge\n",
    "    num_ec2_instances: 15\n",
    "    \n",
    "__Step 1:__ \n",
    "\n",
    "    RUNNING for 649.2s  ~= 10 minutes      \n",
    "    Launched map tasks=190   \n",
    "    Launched reduce tasks=57     \n",
    "\n",
    "__Step 2:__  \n",
    "\n",
    "    RUNNING for 74.4s  ~= 1 minute    \n",
    "    Launched map tasks=110   \n",
    "    Launched reduce tasks=20   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.4.1 - D. Distribution of 5-gram sizes (character length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting distribution.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile distribution.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import time\n",
    "import logging\n",
    "\n",
    "class distribution(MRJob):\n",
    "    \n",
    "    # START STUDENT CODE 5.4.1.D\n",
    "    \n",
    "    MRJob.SORT_VALUES = True\n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # Split line\n",
    "        splits = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "        words = splits[0].lower().split()\n",
    "        \n",
    "        char_count = 0\n",
    "        \n",
    "        # Count characters\n",
    "        for word in words:\n",
    "            char_count += len(word)\n",
    "        \n",
    "        yield char_count, 1\n",
    "            \n",
    "    \n",
    "    def combiner(self, ngram_size, counts):\n",
    "        yield ngram_size, sum(count for count in counts)\n",
    "    \n",
    "    def reducer(self, ngram_size, counts):\n",
    "        yield ngram_size, sum(count for count in counts)\n",
    "\n",
    "    def steps(self):\n",
    "        \n",
    "        custom_jobconf = {\n",
    "            'stream.num.map.output.key.fields':'2',\n",
    "            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapred.text.key.comparator.options': '-k1,1n',\n",
    "            'mapred.reduce.tasks': '1'\n",
    "        }\n",
    "\n",
    "        return [\n",
    "                MRStep(\n",
    "                    jobconf=custom_jobconf,\n",
    "                    mapper=self.mapper,\n",
    "                    reducer=self.reducer,\n",
    "                    combiner = self.combiner)\n",
    "        ]\n",
    "\n",
    "    # END STUDENT CODE 5.4.1.D\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    distribution.run()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    mins = elapsed_time/float(60)\n",
    "    a = \"\"\"Elapsed time: %s seconds\n",
    "    In minutes: %s mins\"\"\" % (str(elapsed_time), str(mins))\n",
    "    logging.warning(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__On the test data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `5.3distributions_test/part-00000': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/distribution.carlosscastro.20170620.000440.297675\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/carlosscastro/tmp/mrjob/distribution.carlosscastro.20170620.000440.297675/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob8427844021077115963.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497906899862_0087\n",
      "  Submitted application application_1497906899862_0087\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_0087/\n",
      "  Running job: job_1497906899862_0087\n",
      "  Job job_1497906899862_0087 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_0087 completed successfully\n",
      "  Output directory: hdfs:///user/carlosscastro/tmp/mrjob/distribution.carlosscastro.20170620.000440.297675/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=563\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=45\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=68\n",
      "\t\tFILE: Number of bytes written=401167\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1039\n",
      "\t\tHDFS: Number of bytes written=45\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=18441216\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=10629120\n",
      "\t\tTotal time spent by all map tasks (ms)=12006\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=36018\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4152\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=20760\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=12006\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=4152\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2890\n",
      "\t\tCombine input records=10\n",
      "\t\tCombine output records=10\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=88\n",
      "\t\tInput split bytes=476\n",
      "\t\tMap input records=10\n",
      "\t\tMap output bytes=60\n",
      "\t\tMap output materialized bytes=88\n",
      "\t\tMap output records=10\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1896161280\n",
      "\t\tReduce input groups=9\n",
      "\t\tReduce input records=10\n",
      "\t\tReduce output records=9\n",
      "\t\tReduce shuffle bytes=88\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=20\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7728373760\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/carlosscastro/tmp/mrjob/distribution.carlosscastro.20170620.000440.297675/output...\n",
      "Removing HDFS temp directory hdfs:///user/carlosscastro/tmp/mrjob/distribution.carlosscastro.20170620.000440.297675...\n",
      "Removing temp directory /tmp/distribution.carlosscastro.20170620.000440.297675...\n",
      "WARNING:root:Elapsed time: 55.916836977 seconds\n",
      "    In minutes: 0.93194728295 mins\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r 5.3distributions_test/part-00000\n",
    "!python distribution.py -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt > 5.3distributions_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=2\n",
    "### Launched reduce tasks=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\t1\r\n",
      "18\t1\r\n",
      "19\t1\r\n",
      "20\t1\r\n",
      "22\t1\r\n",
      "23\t1\r\n",
      "24\t1\r\n",
      "25\t1\r\n",
      "29\t2\r\n"
     ]
    }
   ],
   "source": [
    "!cat 5.3distributions_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Histogram 10-line test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+MAAAGuCAYAAAD73ddLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGEwYBwAAgMGEcQAAABjs/wfyhAYSyQRBIAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2a67bcd7d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "\n",
    "results_A = []\n",
    "for line in open(\"5.3distributions_test\").readlines():\n",
    "    line = line.strip()\n",
    "    X,Y = line.split(\"\\t\")\n",
    "    results_A.append([int(X),int(Y)])\n",
    "\n",
    "items = (np.array(results_A)[::-1].T)\n",
    "fig = pl.figure(figsize=(17,7))\n",
    "ax = pl.subplot(111)\n",
    "width=0.8\n",
    "ax.bar(range(len(items[0])), items[1], width=width)\n",
    "ax.set_xticks(np.arange(len(items[0])) + width/2)\n",
    "ax.set_xticklabels(items[0], rotation=90)\n",
    "ax.invert_xaxis()\n",
    "\n",
    "\n",
    "pl.title(\"Distributions of 5 Gram lengths in 10-line test\")\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On the full data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `full_distribution_5.4.1.d': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Creating temp directory /tmp/distribution.carlosscastro.20170620.000539.737484\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/carlosscastro/tmp/mrjob/distribution.carlosscastro.20170620.000539.737484/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob5840998109717419389.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 190\n",
      "  number of splits:190\n",
      "  Submitting tokens for job: job_1497906899862_0089\n",
      "  Submitted application application_1497906899862_0089\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_0089/\n",
      "  Running job: job_1497906899862_0089\n",
      "  Job job_1497906899862_0089 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 1% reduce 0%\n",
      "   map 2% reduce 0%\n",
      "   map 3% reduce 0%\n",
      "   map 4% reduce 0%\n",
      "   map 5% reduce 0%\n",
      "   map 6% reduce 0%\n",
      "   map 7% reduce 0%\n",
      "   map 8% reduce 0%\n",
      "   map 9% reduce 0%\n",
      "   map 10% reduce 0%\n",
      "   map 12% reduce 0%\n",
      "   map 15% reduce 0%\n",
      "   map 17% reduce 0%\n",
      "   map 18% reduce 0%\n",
      "   map 19% reduce 0%\n",
      "   map 22% reduce 0%\n",
      "   map 24% reduce 0%\n",
      "   map 25% reduce 0%\n",
      "   map 28% reduce 0%\n",
      "   map 30% reduce 0%\n",
      "   map 32% reduce 0%\n",
      "   map 35% reduce 0%\n",
      "   map 38% reduce 0%\n",
      "   map 39% reduce 0%\n",
      "   map 42% reduce 0%\n",
      "   map 45% reduce 0%\n",
      "   map 47% reduce 0%\n",
      "   map 49% reduce 0%\n",
      "   map 53% reduce 0%\n",
      "   map 56% reduce 0%\n",
      "   map 57% reduce 0%\n",
      "   map 59% reduce 0%\n",
      "   map 61% reduce 0%\n",
      "   map 62% reduce 0%\n",
      "   map 63% reduce 0%\n",
      "   map 64% reduce 0%\n",
      "   map 65% reduce 0%\n",
      "   map 66% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 68% reduce 0%\n",
      "   map 69% reduce 0%\n",
      "   map 71% reduce 0%\n",
      "   map 73% reduce 0%\n",
      "   map 74% reduce 0%\n",
      "   map 76% reduce 0%\n",
      "   map 79% reduce 0%\n",
      "   map 82% reduce 0%\n",
      "   map 87% reduce 0%\n",
      "   map 90% reduce 0%\n",
      "   map 92% reduce 0%\n",
      "   map 94% reduce 0%\n",
      "   map 95% reduce 0%\n",
      "   map 96% reduce 0%\n",
      "   map 97% reduce 0%\n",
      "   map 98% reduce 0%\n",
      "   map 99% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_0089 completed successfully\n",
      "  Output directory: hdfs:///user/carlosscastro/tmp/mrjob/distribution.carlosscastro.20170620.000539.737484/output\n",
      "Counters: 51\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2156069116\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=619\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=29316\n",
      "\t\tFILE: Number of bytes written=25620621\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2156101116\n",
      "\t\tHDFS: Number of bytes written=619\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=573\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=191\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tOther local map tasks=2\n",
      "\t\tRack-local map tasks=189\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=17351967744\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=10168320\n",
      "\t\tTotal time spent by all map tasks (ms)=11296854\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=33890562\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3972\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=19860\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=11296854\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3972\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=3889810\n",
      "\t\tCombine input records=58682266\n",
      "\t\tCombine output records=9172\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=81032\n",
      "\t\tInput split bytes=32000\n",
      "\t\tMap input records=58682266\n",
      "\t\tMap output bytes=352088828\n",
      "\t\tMap output materialized bytes=79220\n",
      "\t\tMap output records=58682266\n",
      "\t\tMerged Map outputs=190\n",
      "\t\tPhysical memory (bytes) snapshot=154537222144\n",
      "\t\tReduce input groups=80\n",
      "\t\tReduce input records=9172\n",
      "\t\tReduce output records=80\n",
      "\t\tReduce shuffle bytes=79220\n",
      "\t\tShuffled Maps =190\n",
      "\t\tSpilled Records=18344\n",
      "\t\tTotal committed heap usage (bytes)=299992875008\n",
      "\t\tVirtual memory (bytes) snapshot=421227868160\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/carlosscastro/tmp/mrjob/distribution.carlosscastro.20170620.000539.737484/output...\n",
      "Removing HDFS temp directory hdfs:///user/carlosscastro/tmp/mrjob/distribution.carlosscastro.20170620.000539.737484...\n",
      "Removing temp directory /tmp/distribution.carlosscastro.20170620.000539.737484...\n",
      "WARNING:root:Elapsed time: 136.871945858 seconds\n",
      "    In minutes: 2.28119909763 mins\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r full_distribution_5.4.1.d\n",
    "!python distribution.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > full_distribution_5.4.1.d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=2\n",
    "### Launched reduce tasks=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution MRJob stats\n",
    "\n",
    "__Step 1:__ \n",
    "\n",
    "    RUNNING for 157.8s ~= 2.6 minutes  \n",
    "    Launched map tasks=191  \n",
    "    Launched reduce tasks=16   \n",
    "    \n",
    "__Step 2:__  \n",
    "\n",
    "    RUNNING for 115.0s ~= 2 minutes   \n",
    "    Launched map tasks=139\n",
    "\tLaunched reduce tasks=1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA/oAAAG1CAYAAABJd48xAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAACAEfn/AYLyy+6w1VfAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2a6522de10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "\n",
    "results_A = []\n",
    "for line in open(\"full_distribution_5.4.1.d\").readlines():\n",
    "    line = line.strip()\n",
    "    X,Y = line.split(\"\\t\")\n",
    "    results_A.append([int(X),int(Y)])\n",
    "\n",
    "items = (np.array(results_A)[::-1].T)\n",
    "fig = pl.figure(figsize=(17,7))\n",
    "ax = pl.subplot(111)\n",
    "width=0.8\n",
    "ax.bar(range(len(items[0])), items[1], width=width)\n",
    "ax.set_xticks(np.arange(len(items[0])) + width/2)\n",
    "ax.set_xticklabels(items[0], rotation=90)\n",
    "ax.invert_xaxis()\n",
    "\n",
    "\n",
    "pl.title(\"Distributions of 5 Gram lengths\")\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.4.2 <a name=\"5.4.2\"></a>OPTIONAL Question: log-log plots (PHASE 2)\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Plot the log-log plot of the frequency distributuion of unigrams. Does it follow power law distribution?\n",
    "\n",
    "For more background see:\n",
    "- https://en.wikipedia.org/wiki/Log%E2%80%93log_plot\n",
    "- https://en.wikipedia.org/wiki/Power_law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.5  <a name=\"5.5\"></a> Synonym detection over 2Gig of Data with extra Preprocessing steps (HW5.3 plus some preprocessing)   (Phase 2)\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "For the remainder of this assignment please feel free to eliminate stop words from your analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">There is also a corpus of stopwords, that is, high-frequency words like \"the\", \"to\" and \"also\" that we sometimes want to filter out of a document before further processing. Stopwords usually have little lexical content, and their presence in a text fails to distinguish it from other texts. Python's nltk comes with a prebuilt list of stopwords (see below). Using this stopword list filter out these tokens from your analysis and rerun the experiments in 5.5 and disucuss the results of using a stopword list and without using a stopword list.\n",
    "\n",
    "> from nltk.corpus import stopwords\n",
    " stopwords.words('english')\n",
    "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: A large subset of the Google n-grams dataset as was described above\n",
    "\n",
    "For each HW 5.4 -5.5.1 Please unit test and system test your code with respect \n",
    "to SYSTEMS TEST DATASET and show the results. \n",
    "Please compute the expected answer by hand and show your hand calculations for the \n",
    "SYSTEMS TEST DATASET. Then show the results you get with your system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the assignment we will focus on developing methods for detecting synonyms, using the Google 5-grams dataset. At a high level:\n",
    "\n",
    "\n",
    "1. remove stopwords\n",
    "2. get 10,0000 most frequent\n",
    "3. get 1000 (9001-10000) features\n",
    "3. build stripes\n",
    "\n",
    "To accomplish this you must script two main tasks using MRJob:\n",
    "\n",
    "\n",
    "__TASK (1)__ Build stripes for the most frequent 10,000 words using cooccurence information based on\n",
    "the words ranked from 9001,-10,000 as a basis/vocabulary (drop stopword-like terms),\n",
    "and output to a file in your bucket on s3 (bigram analysis, though the words are non-contiguous).\n",
    "\n",
    "\n",
    "__TASK (2)__ Using two (symmetric) comparison methods of your choice \n",
    "(e.g., correlations, distances, similarities), pairwise compare \n",
    "all stripes (vectors), and output to a file in your bucket on s3.\n",
    "\n",
    "#### Design notes for TASK (1)\n",
    "For this task you will be able to modify the pattern we used in HW 3.2\n",
    "(feel free to use the solution as reference). To total the word counts \n",
    "across the 5-grams, output the support from the mappers using the total \n",
    "order inversion pattern:\n",
    "\n",
    "<*word,count>\n",
    "\n",
    "to ensure that the support arrives before the cooccurrences.\n",
    "\n",
    "In addition to ensuring the determination of the total word counts,\n",
    "the mapper must also output co-occurrence counts for the pairs of\n",
    "words inside of each 5-gram. Treat these words as a basket,\n",
    "as we have in HW 3, but count all stripes or pairs in both orders,\n",
    "i.e., count both orderings: (word1,word2), and (word2,word1), to preserve\n",
    "symmetry in our output for TASK (2).\n",
    "\n",
    "#### Design notes for _TASK (2)_\n",
    "For this task you will have to determine a method of comparison.\n",
    "Here are a few that you might consider:\n",
    "\n",
    "- Jaccard\n",
    "- Cosine similarity\n",
    "- Spearman correlation\n",
    "- Euclidean distance\n",
    "- Taxicab (Manhattan) distance\n",
    "- Shortest path graph distance (a graph, because our data is symmetric!)\n",
    "- Pearson correlation\n",
    "- Kendall correlation\n",
    "\n",
    "However, be cautioned that some comparison methods are more difficult to\n",
    "parallelize than others, and do not perform more associations than is necessary, \n",
    "since your choice of association will be symmetric.\n",
    "\n",
    "Please use the inverted index (discussed in live session #5) based pattern to compute the pairwise (term-by-term) similarity matrix. \n",
    "\n",
    "Please report the size of the cluster used and the amount of time it takes to run for the index construction task and for the synonym calculation task. How many pairs need to be processed (HINT: use the posting list length to calculate directly)? Report your  Cluster configuration!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example MR stats: (report times!)\n",
    "    took ~11 minutes on 5 m3.xlarge nodes\n",
    "    Data-local map tasks=188\n",
    "\tLaunched map tasks=190\n",
    "\tLaunched reduce tasks=15\n",
    "\tOther local map tasks=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# START STUDENT CODE 5.5\n",
    "# ADD OR REMOVE CELLS AS NEEDED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Frequency ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting frequencies5_5.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile frequencies5_5.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import re\n",
    "\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import time\n",
    "import logging\n",
    "\n",
    "class frequencies(MRJob):\n",
    "\n",
    "    # START STUDENT CODE 5.4.1.B\n",
    "    \n",
    "    MRJob.SORT_VALUES = True\n",
    "        \n",
    "    def __init__(self, args):\n",
    "        super(frequencies, self).__init__(args)\n",
    "        #self.min_rank = 9001\n",
    "        #self.max_rank = 10000 \n",
    "        self.current_rank = 0\n",
    "\n",
    "    def configure_options(self): \n",
    "        super(frequencies, self).configure_options() \n",
    "        self.add_passthrough_option('--min_rank', dest='min_rank', type='int', default=9001) \n",
    "        self.add_passthrough_option('--max_rank', dest='max_rank', type='int', default=10000) \n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # Split line\n",
    "        splits = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "        words = splits[0].lower().split()\n",
    "        count = int(splits[1])\n",
    "        \n",
    "        for word in words:\n",
    "            yield word, count\n",
    "            \n",
    "    \n",
    "    def combiner(self, word, counts):\n",
    "        total = sum(count for count in counts)\n",
    "        yield word, total\n",
    "    \n",
    "    def reducer(self, word, counts):\n",
    "        total = sum(count for count in counts)\n",
    "        yield total, word\n",
    "    \n",
    "    def max_reducer(self, count, words):\n",
    "        \n",
    "        # Words come in frequency descending order here\n",
    "        # Only yield the words that are within the min and max frequency ranking desired\n",
    "        \n",
    "        for word in words:\n",
    "            self.current_rank += 1\n",
    "            \n",
    "            if self.current_rank >= self.options.min_rank and self.current_rank <= self.options.max_rank:\n",
    "                yield word, count\n",
    "\n",
    "    def steps(self):\n",
    "        \n",
    "        custom_jobconf = {\n",
    "            'stream.num.map.output.key.fields':'2',\n",
    "            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapred.text.key.comparator.options': '-k1,1nr',\n",
    "            'mapred.reduce.tasks': '1'\n",
    "        }\n",
    "\n",
    "        return [\n",
    "                MRStep(mapper=self.mapper,\n",
    "                    reducer=self.reducer,\n",
    "                    combiner = self.combiner),\n",
    "                MRStep(jobconf=custom_jobconf,\n",
    "                       reducer=self.max_reducer)\n",
    "                 ]\n",
    "\n",
    "    # END STUDENT CODE 5.4.1.B\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    frequencies.run()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    mins = elapsed_time/float(60)\n",
    "    a = \"\"\"Elapsed time: %s seconds\n",
    "    In minutes: %s mins\"\"\" % (str(elapsed_time), str(mins))\n",
    "    logging.warning(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency ranking on 10-line test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `frequencies_test5.5': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/frequencies5_5.carlosscastro.20170620.000800.186818\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/carlosscastro/tmp/mrjob/frequencies5_5.carlosscastro.20170620.000800.186818/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob4303987330107755731.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497906899862_0091\n",
      "  Submitted application application_1497906899862_0091\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_0091/\n",
      "  Running job: job_1497906899862_0091\n",
      "  Job job_1497906899862_0091 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_0091 completed successfully\n",
      "  Output directory: hdfs:///user/carlosscastro/tmp/mrjob/frequencies5_5.carlosscastro.20170620.000800.186818/step-output/0000\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=563\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=357\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=430\n",
      "\t\tFILE: Number of bytes written=401186\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1043\n",
      "\t\tHDFS: Number of bytes written=357\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=62148096\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=15165440\n",
      "\t\tTotal time spent by all map tasks (ms)=40461\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=121383\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5924\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=29620\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=40461\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=5924\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=3270\n",
      "\t\tCombine input records=50\n",
      "\t\tCombine output records=31\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=141\n",
      "\t\tInput split bytes=480\n",
      "\t\tMap input records=10\n",
      "\t\tMap output bytes=602\n",
      "\t\tMap output materialized bytes=458\n",
      "\t\tMap output records=50\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1905745920\n",
      "\t\tReduce input groups=28\n",
      "\t\tReduce input records=31\n",
      "\t\tReduce output records=28\n",
      "\t\tReduce shuffle bytes=458\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=62\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7746318336\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob6131404608627086079.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497906899862_0092\n",
      "  Submitted application application_1497906899862_0092\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_0092/\n",
      "  Running job: job_1497906899862_0092\n",
      "  Job job_1497906899862_0092 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_0092 completed successfully\n",
      "  Output directory: hdfs:///user/carlosscastro/tmp/mrjob/frequencies5_5.carlosscastro.20170620.000800.186818/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=536\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=40\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=396\n",
      "\t\tFILE: Number of bytes written=400513\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=930\n",
      "\t\tHDFS: Number of bytes written=40\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=35612160\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=17646080\n",
      "\t\tTotal time spent by all map tasks (ms)=23185\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=69555\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6893\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=34465\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=23185\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=6893\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2970\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=201\n",
      "\t\tInput split bytes=394\n",
      "\t\tMap input records=28\n",
      "\t\tMap output bytes=385\n",
      "\t\tMap output materialized bytes=437\n",
      "\t\tMap output records=28\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1897476096\n",
      "\t\tReduce input groups=28\n",
      "\t\tReduce input records=28\n",
      "\t\tReduce output records=3\n",
      "\t\tReduce shuffle bytes=437\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=56\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7784591360\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/carlosscastro/tmp/mrjob/frequencies5_5.carlosscastro.20170620.000800.186818/output...\n",
      "Removing HDFS temp directory hdfs:///user/carlosscastro/tmp/mrjob/frequencies5_5.carlosscastro.20170620.000800.186818...\n",
      "Removing temp directory /tmp/frequencies5_5.carlosscastro.20170620.000800.186818...\n",
      "WARNING:root:Elapsed time: 150.88996911 seconds\n",
      "    In minutes: 2.51483281851 mins\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r frequencies_test5.5\n",
    "!python frequencies5_5.py --min_rank 2 --max_rank 4 -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt > frequencies_test5.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"in\"\t1201\r\n",
      "\"wales\"\t1099\r\n",
      "\"christmas\"\t1099\r\n"
     ]
    }
   ],
   "source": [
    "!cat frequencies_test5.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency ranking on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `frequencies5.5': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Creating temp directory /tmp/frequencies5_5.carlosscastro.20170620.002241.204239\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/carlosscastro/tmp/mrjob/frequencies5_5.carlosscastro.20170620.002241.204239/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 2...\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r frequencies5.5\n",
    "!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat frequencies5.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stripes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile stripes5_5.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "import re\n",
    "import mrjob\n",
    "import json\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import itertools\n",
    "import collections\n",
    "import time\n",
    "import logging\n",
    "\n",
    "class stripes(MRJob):\n",
    "  \n",
    "    #START SUDENT CODE531_STRIPES\n",
    "  \n",
    "    MRJob.SORT_VALUES = True\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        self.valid_words = set()\n",
    "        super(stripes, self).__init__(args)\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        # Load the left table in the init so all mappers get this info\n",
    "        with open(\"frequencies5.5\", 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                x = line.strip().split(\"\\t\")\n",
    "                self.valid_words.add(x[0].strip(\"\\\"\"))\n",
    "            \n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        splits = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "\n",
    "        words = splits[0].lower().split()\n",
    "        count = splits[1]\n",
    "\n",
    "        H = {}\n",
    "        for subset in itertools.combinations(sorted(set(words)), 2):\n",
    "            \n",
    "            # Logic from slack discussion, with Sharmila and Kyle: the stripes are assymentric\n",
    "            # If a given word is a valid word, then we build a stripe for the other word and add\n",
    "            # its co-ocurrence to the valid word. If both words are valid, only then we add\n",
    "            # to stripes in a symmetric way\n",
    "            \n",
    "            if subset[1] in self.valid_words:\n",
    "\n",
    "                # Process combinations in sorted order, i.e. \"hello\",\"tomorrow\"\n",
    "                if subset[0] not in H.keys():\n",
    "                    H[subset[0]] = {}\n",
    "                    H[subset[0]][subset[1]] = count \n",
    "                elif subset[1] not in H[subset[0]]:\n",
    "                    H[subset[0]][subset[1]] = count\n",
    "                else:\n",
    "                    H[subset[0]][subset[1]] += count\n",
    "\n",
    "            if subset[0] in self.valid_words:\n",
    "                # Obtain combinations in reverse order\n",
    "                if subset[1] not in H.keys():\n",
    "                    H[subset[1]] = {}\n",
    "                    H[subset[1]][subset[0]] = count \n",
    "                elif subset[0] not in H[subset[1]]:\n",
    "                    H[subset[1]][subset[0]] = count\n",
    "                else:\n",
    "                    H[subset[1]][subset[0]] += count\n",
    "                    \n",
    "        for key in H.keys():\n",
    "            yield key, H[key]\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        \n",
    "        counter = {}\n",
    "\n",
    "        for value in values:\n",
    "            \n",
    "            for k, v in value.iteritems():\n",
    "                if k in counter:\n",
    "                    counter[k] += int(v)\n",
    "                else:\n",
    "                    counter[k] = int(v)\n",
    "        \n",
    "        yield key, counter\n",
    "\n",
    "  #END SUDENT CODE531_STRIPES\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    stripes.run()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    mins = elapsed_time/float(60)\n",
    "    a = \"\"\"Elapsed time: %s seconds\n",
    "    In minutes: %s mins\"\"\" % (str(elapsed_time), str(mins))\n",
    "    logging.warning(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r stripes5.5\n",
    "!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat stripes5.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile index5_5.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "from __future__ import division\n",
    "import collections\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import itertools\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "\n",
    "class index(MRJob):\n",
    "    \n",
    "    #START SUDENT CODE531_INV_INDEX\n",
    "  \n",
    "    def mapper(self, _, line):\n",
    "        key, stripeJson = line.strip().split('\\t')\n",
    "        key = key.strip(\"\\\"\")\n",
    "        stripe = json.loads(stripeJson)\n",
    "        \n",
    "        for k, v in stripe.iteritems():\n",
    "            yield k, [key, len(stripe)]\n",
    "        \n",
    "    def reducer(self, key, values):\n",
    "\n",
    "        table = {}\n",
    "        for value in values:\n",
    "            table[value[0]] = value[1]\n",
    "            \n",
    "        yield key, table\n",
    "        \n",
    "    #END SUDENT CODE531_INV_INDEX\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    index.run() \n",
    "    elapsed_time = time.time() - start_time\n",
    "    mins = elapsed_time/float(60)\n",
    "    a = \"\"\"Elapsed time: %s seconds\n",
    "    In minutes: %s mins\"\"\" % (str(elapsed_time), str(mins))\n",
    "    logging.warning(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r index5.5\n",
    "!python index5_5.py -r hadoop stripes5.5 > index5.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat index5.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile similarity5_5.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "import collections\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "#import numpy as np\n",
    "import itertools\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import time\n",
    "import logging\n",
    "\n",
    "class similarity(MRJob):\n",
    "  \n",
    "    #START SUDENT CODE531_SIMILARITY\n",
    "\n",
    "    MRJob.SORT_VALUES = True\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        key, valuesJson = line.strip().split('\\t')\n",
    "        key = key.strip(\"\\\"\")\n",
    "        values = json.loads(valuesJson)\n",
    "\n",
    "        for pair in itertools.combinations(sorted(set(values)), 2):\n",
    "            yield pair, [values[pair[0]], values[pair[1]]]\n",
    "        \n",
    "    def reducer(self, key, values):\n",
    "        intersection = 0\n",
    "        count1 = None\n",
    "        count2 = None\n",
    "        \n",
    "        cosine = 0.0\n",
    "        \n",
    "        # Iterate through the values\n",
    "        for value in values:\n",
    "            # Jaccard, get counts for the intersection, and for each set\n",
    "            intersection += 1\n",
    "            if count1 == None:\n",
    "                count1 = value[0]\n",
    "                count2 = value[1]\n",
    "        \n",
    "            # Cosine\n",
    "            a = 1 / math.sqrt(value[0])\n",
    "            b = 1 / math.sqrt(value[1])\n",
    "            cosine += a * b\n",
    "            \n",
    "        jaccard = float(intersection) / float(count1 + count2 - intersection)\n",
    "        \n",
    "        overlap_coefficient = float(intersection) / min(count1, count2)\n",
    "        \n",
    "        dice_coefficient = float(2 * intersection) / (count1 + count2)\n",
    "        \n",
    "        average = (cosine + jaccard + overlap_coefficient + dice_coefficient) / 4.0\n",
    "        \n",
    "        yield average, [key[0] + ' - ' + key[1], cosine, jaccard, overlap_coefficient, dice_coefficient, average]\n",
    "    \n",
    "    \n",
    "    def max_reducer(self, average, records):\n",
    "        for record in records:\n",
    "            yield average, record\n",
    "\n",
    "    def steps(self):\n",
    "        \n",
    "        custom_jobconf = {\n",
    "            'stream.num.map.output.key.fields':'2',\n",
    "            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapred.text.key.comparator.options': '-k1,1nr',\n",
    "            'mapred.reduce.tasks': '1'\n",
    "        }\n",
    "\n",
    "        return [\n",
    "                MRStep(mapper=self.mapper,\n",
    "                    reducer=self.reducer),\n",
    "                MRStep(jobconf=custom_jobconf,\n",
    "                       reducer=self.max_reducer)\n",
    "                 ]\n",
    "    \n",
    "    #END SUDENT CODE531_SIMILARITY\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    similarity.run()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    mins = elapsed_time/float(60)\n",
    "    a = \"\"\"Elapsed time: %s seconds\n",
    "    In minutes: %s mins\"\"\" % (str(elapsed_time), str(mins))\n",
    "    logging.warning(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r similarity5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python similarity5_5.py -r hadoop index5.5 > similarity5.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat similarity5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "sortedSims = []\n",
    "with open(\"similarity5.5\",\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "\n",
    "        line = line.strip()\n",
    "        avg,lisst = line.split(\"\\t\")\n",
    "        lisst = json.loads(lisst)\n",
    "        lisst.append(avg)\n",
    "        sortedSims.append(lisst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir sims2\n",
    "!head -1000 similarity5.5 > sims2/top1000sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# END STUDENT CODE 5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"\\nTop/Bottom 20 results - Similarity measures - sorted by cosine\"\n",
    "print \"(From the entire data set)\"\n",
    "print '—'*117\n",
    "print \"{0:>30} |{1:>15} |{2:>15} |{3:>15} |{4:>15} |{5:>15}\".format(\n",
    "        \"pair\", \"cosine\", \"jaccard\", \"overlap\", \"dice\", \"average\")\n",
    "print '-'*117\n",
    "\n",
    "for stripe in sortedSims[:20]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )\n",
    "\n",
    "print '—'*117\n",
    "\n",
    "for stripe in sortedSims[-20:]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Top/Bottom 20 results - Similarity measures - sorted by cosine\n",
    "(From the entire data set)\n",
    "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "                          pair |         cosine |        jaccard |        overlap |           dice |        average\n",
    "---------------------------------------------------------------------------------------------------------------------\n",
    "                   cons - pros |       0.894427 |       0.800000 |       1.000000 |       0.888889 |       0.895829\n",
    "            forties - twenties |       0.816497 |       0.666667 |       1.000000 |       0.800000 |       0.820791\n",
    "                    own - time |       0.809510 |       0.670563 |       0.921168 |       0.802799 |       0.801010\n",
    "                 little - time |       0.784197 |       0.630621 |       0.926101 |       0.773473 |       0.778598\n",
    "                  found - time |       0.783434 |       0.636364 |       0.883788 |       0.777778 |       0.770341\n",
    "                 nova - scotia |       0.774597 |       0.600000 |       1.000000 |       0.750000 |       0.781149\n",
    "                   hong - kong |       0.769800 |       0.615385 |       0.888889 |       0.761905 |       0.758995\n",
    "                   life - time |       0.769666 |       0.608789 |       0.925081 |       0.756829 |       0.765091\n",
    "                  time - world |       0.755476 |       0.585049 |       0.937500 |       0.738209 |       0.754058\n",
    "                  means - time |       0.752181 |       0.587117 |       0.902597 |       0.739854 |       0.745437\n",
    "                   form - time |       0.749943 |       0.588418 |       0.876733 |       0.740885 |       0.738995\n",
    "       infarction - myocardial |       0.748331 |       0.560000 |       1.000000 |       0.717949 |       0.756570\n",
    "                 people - time |       0.745788 |       0.573577 |       0.923875 |       0.729010 |       0.743063\n",
    "                 angeles - los |       0.745499 |       0.586207 |       0.850000 |       0.739130 |       0.730209\n",
    "                  little - own |       0.739343 |       0.585834 |       0.767296 |       0.738834 |       0.707827\n",
    "                    life - own |       0.737053 |       0.582217 |       0.778502 |       0.735951 |       0.708430\n",
    "          anterior - posterior |       0.733388 |       0.576471 |       0.790323 |       0.731343 |       0.707881\n",
    "                  power - time |       0.719611 |       0.533623 |       0.933586 |       0.695898 |       0.720680\n",
    "              dearly - install |       0.707107 |       0.500000 |       1.000000 |       0.666667 |       0.718443\n",
    "                   found - own |       0.704802 |       0.544134 |       0.710949 |       0.704776 |       0.666165\n",
    "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "           arrival - essential |       0.008258 |       0.004098 |       0.009615 |       0.008163 |       0.007534\n",
    "         governments - surface |       0.008251 |       0.003534 |       0.014706 |       0.007042 |       0.008383\n",
    "                king - lesions |       0.008178 |       0.003106 |       0.017857 |       0.006192 |       0.008833\n",
    "              clinical - stood |       0.008178 |       0.003831 |       0.011905 |       0.007634 |       0.007887\n",
    "               till - validity |       0.008172 |       0.003367 |       0.015625 |       0.006711 |       0.008469\n",
    "            evidence - started |       0.008159 |       0.003802 |       0.012048 |       0.007576 |       0.007896\n",
    "               forces - record |       0.008152 |       0.003876 |       0.011364 |       0.007722 |       0.007778\n",
    "               primary - stone |       0.008146 |       0.004065 |       0.009091 |       0.008097 |       0.007350\n",
    "             beneath - federal |       0.008134 |       0.004082 |       0.008403 |       0.008130 |       0.007187\n",
    "                factors - rose |       0.008113 |       0.004032 |       0.009346 |       0.008032 |       0.007381\n",
    "           evening - functions |       0.008069 |       0.004049 |       0.008333 |       0.008065 |       0.007129\n",
    "                   bone - told |       0.008061 |       0.003704 |       0.012346 |       0.007380 |       0.007873\n",
    "             building - occurs |       0.008002 |       0.003891 |       0.010309 |       0.007752 |       0.007489\n",
    "                 company - fig |       0.007913 |       0.003257 |       0.015152 |       0.006494 |       0.008204\n",
    "               chronic - north |       0.007803 |       0.003268 |       0.014493 |       0.006515 |       0.008020\n",
    "             evaluation - king |       0.007650 |       0.003030 |       0.015625 |       0.006042 |       0.008087\n",
    "             resulting - stood |       0.007650 |       0.003663 |       0.010417 |       0.007299 |       0.007257\n",
    "                 agent - round |       0.007515 |       0.003289 |       0.012821 |       0.006557 |       0.007546\n",
    "         afterwards - analysis |       0.007387 |       0.003521 |       0.010204 |       0.007018 |       0.007032\n",
    "            posterior - spirit |       0.007156 |       0.002660 |       0.016129 |       0.005305 |       0.007812"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.6  <a name=\"5.6\"></a> Evaluation of synonyms that your discovered\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "\n",
    "In this part of the assignment you will evaluate the success of you synonym detector (developed in response to HW5.4).\n",
    "Take the top 1,000 closest/most similar/correlative pairs of words as determined by your measure in HW5.4, and use the synonyms function in the accompanying python code:\n",
    "\n",
    "nltk_synonyms.py\n",
    "\n",
    "Note: This will require installing the python nltk package:\n",
    "\n",
    "http://www.nltk.org/install.html\n",
    "\n",
    "and downloading its data with nltk.download().\n",
    "\n",
    "For each (word1,word2) pair, check to see if word1 is in the list, \n",
    "synonyms(word2), and vice-versa. If one of the two is a synonym of the other, \n",
    "then consider this pair a 'hit', and then report the precision, recall, and F1 measure  of \n",
    "your detector across your 1,000 best guesses. Report the macro averages of these measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Calculate performance measures:\n",
    "$$Precision (P) = \\frac{TP}{TP + FP} $$  \n",
    "$$Recall (R) = \\frac{TP}{TP + FN} $$  \n",
    "$$F1 = \\frac{2 * ( precision * recall )}{precision + recall}$$\n",
    "\n",
    "\n",
    "We calculate Precision by counting the number of hits and dividing by the number of occurances in our top1000 (opportunities)   \n",
    "We calculate Recall by counting the number of hits, and dividing by the number of synonyms in wordnet (syns)\n",
    "\n",
    "\n",
    "Other diagnostic measures not implemented here:  https://en.wikipedia.org/wiki/F1_score#Diagnostic_Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Performance measures '''\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "\n",
    "#print all the synset element of an element\n",
    "def synonyms(string):\n",
    "    syndict = {}\n",
    "    for i,j in enumerate(wn.synsets(string)):\n",
    "        syns = j.lemma_names()\n",
    "        for syn in syns:\n",
    "            syndict.setdefault(syn,1)\n",
    "    return syndict.keys()\n",
    "hits = []\n",
    "\n",
    "TP = 0\n",
    "FP = 0\n",
    "\n",
    "TOTAL = 0\n",
    "flag = False # so we don't double count, but at the same time don't miss hits\n",
    "start_time = time.time()\n",
    "top1000sims = []\n",
    "with open(\"sims2/top1000sims\",\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "\n",
    "        line = line.strip()\n",
    "        avg,lisst = line.split(\"\\t\")\n",
    "        lisst = json.loads(lisst)\n",
    "        lisst.append(avg)\n",
    "        top1000sims.append(lisst)\n",
    "    \n",
    "\n",
    "measures = {}\n",
    "not_in_wordnet = []\n",
    "\n",
    "for line in top1000sims:\n",
    "    TOTAL += 1\n",
    "\n",
    "    pair = line[0]\n",
    "    words = pair.split(\" - \")\n",
    "    \n",
    "    for word in words:\n",
    "        if word not in measures:\n",
    "            measures[word] = {\"syns\":0,\"opps\": 0,\"hits\":0}\n",
    "        measures[word][\"opps\"] += 1 \n",
    "    \n",
    "    syns0 = synonyms(words[0])\n",
    "    measures[words[1]][\"syns\"] = len(syns0)\n",
    "    if len(syns0) == 0:\n",
    "        not_in_wordnet.append(words[0])\n",
    "        \n",
    "    if words[1] in syns0:\n",
    "        TP += 1\n",
    "        hits.append(line)\n",
    "        flag = True\n",
    "        measures[words[1]][\"hits\"] += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "    syns1 = synonyms(words[1]) \n",
    "    measures[words[0]][\"syns\"] = len(syns1)\n",
    "    if len(syns1) == 0:\n",
    "        not_in_wordnet.append(words[1])\n",
    "\n",
    "    if words[0] in syns1:\n",
    "        if flag == False:\n",
    "            TP += 1\n",
    "            hits.append(line)\n",
    "            measures[words[0]][\"hits\"] += 1\n",
    "            \n",
    "    flag = False    \n",
    "\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for key in measures:\n",
    "    p,r,f = 0,0,0\n",
    "    if measures[key][\"hits\"] > 0 and measures[key][\"syns\"] > 0:\n",
    "        p = measures[key][\"hits\"]/measures[key][\"opps\"]\n",
    "        r = measures[key][\"hits\"]/measures[key][\"syns\"]\n",
    "        f = 2 * (p*r)/(p+r)\n",
    "    \n",
    "    # For calculating measures, only take into account words that have synonyms in wordnet\n",
    "    if measures[key][\"syns\"] > 0:\n",
    "        precision.append(p)\n",
    "        recall.append(r)\n",
    "        f1.append(f)\n",
    "\n",
    "    \n",
    "# Take the mean of each measure    \n",
    "print \"—\"*110    \n",
    "print \"Number of Hits:\",TP, \"out of top\",TOTAL\n",
    "print \"Number of words without synonyms:\",len(not_in_wordnet)\n",
    "print \"—\"*110 \n",
    "print \"Precision\\t\", np.mean(precision)\n",
    "print \"Recall\\t\\t\", np.mean(recall)\n",
    "print \"F1\\t\\t\", np.mean(f1)\n",
    "print \"—\"*110  \n",
    "\n",
    "print \"Words without synonyms:\"\n",
    "print \"-\"*100\n",
    "\n",
    "for word in not_in_wordnet:\n",
    "    print synonyms(word),word\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "mins = elapsed_time/float(60)\n",
    "a = \"\"\"Elapsed time: %s seconds\n",
    "In minutes: %s mins\"\"\" % (str(elapsed_time), str(mins))\n",
    "logging.warning(a)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "Number of Hits: 31 out of top 1000\n",
    "Number of words without synonyms: 67\n",
    "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "Precision\t0.0280214404967\n",
    "Recall\t\t0.0178598869579\n",
    "F1\t\t0.013965517619\n",
    "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "Words without synonyms:\n",
    "----------------------------------------------------------------------------------------------------\n",
    "[] scotia\n",
    "[] hong\n",
    "[] kong\n",
    "[] angeles\n",
    "[] los\n",
    "[] nor\n",
    "[] themselves\n",
    "[] \n",
    "......."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.7  <a name=\"5.7\"></a> OPTIONAL: using different vocabulary subsets\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "\n",
    "Repeat HW5 using vocabulary words ranked from 8001,-10,000;  7001,-10,000; 6001,-10,000; 5001,-10,000; 3001,-10,000; and 1001,-10,000;\n",
    "Dont forget to report you Cluster configuration.\n",
    "\n",
    "Generate the following graphs:\n",
    "-- vocabulary size (X-Axis) versus CPU time for indexing\n",
    "-- vocabulary size (X-Axis) versus number of pairs processed\n",
    "-- vocabulary size (X-Axis) versus F1 measure, Precision, Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7: Vocabulary subset 8001-10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r frequencies5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5\n",
    "!python frequencies5_5.py --min_rank 8001 --max_rank 10000 -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered.txt > frequencies5.5\n",
    "\n",
    "!hdfs dfs -rm -r stripes5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python stripes5_5.py --file=frequencies5.5 -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered.txt > stripes5.5\n",
    "\n",
    "!hdfs dfs -rm -r index5.5\n",
    "!python index5_5.py -r hadoop stripes5.5 > index5.5\n",
    "\n",
    "!hdfs dfs -rm -r similarity5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python similarity5_5.py -r hadoop index5.5 > similarity5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "sortedSims = []\n",
    "with open(\"similarity5.5\",\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "\n",
    "        line = line.strip()\n",
    "        avg,lisst = line.split(\"\\t\")\n",
    "        lisst = json.loads(lisst)\n",
    "        lisst.append(avg)\n",
    "        sortedSims.append(lisst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir sims2\n",
    "!head -1000 similarity5.5 > sims2/top1000sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"\\nTop/Bottom 20 results - Similarity measures - sorted by cosine\"\n",
    "print \"(From the entire data set)\"\n",
    "print '—'*117\n",
    "print \"{0:>30} |{1:>15} |{2:>15} |{3:>15} |{4:>15} |{5:>15}\".format(\n",
    "        \"pair\", \"cosine\", \"jaccard\", \"overlap\", \"dice\", \"average\")\n",
    "print '-'*117\n",
    "\n",
    "for stripe in sortedSims[:20]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )\n",
    "\n",
    "print '—'*117\n",
    "\n",
    "for stripe in sortedSims[-20:]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7: Vocabulary subset 7001-10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r frequencies5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5\n",
    "!python frequencies5_5.py --min_rank 7001 --max_rank 10000 -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered.txt > frequencies5.5\n",
    "\n",
    "!hdfs dfs -rm -r stripes5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python stripes5_5.py --file=frequencies5.5 -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered.txt > stripes5.5\n",
    "\n",
    "!hdfs dfs -rm -r index5.5\n",
    "!python index5_5.py -r hadoop stripes5.5 > index5.5\n",
    "\n",
    "!hdfs dfs -rm -r similarity5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python similarity5_5.py -r hadoop index5.5 > similarity5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "sortedSims = []\n",
    "with open(\"similarity5.5\",\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "\n",
    "        line = line.strip()\n",
    "        avg,lisst = line.split(\"\\t\")\n",
    "        lisst = json.loads(lisst)\n",
    "        lisst.append(avg)\n",
    "        sortedSims.append(lisst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir sims2\n",
    "!head -1000 similarity5.5 > sims2/top1000sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"\\nTop/Bottom 20 results - Similarity measures - sorted by cosine\"\n",
    "print \"(From the entire data set)\"\n",
    "print '—'*117\n",
    "print \"{0:>30} |{1:>15} |{2:>15} |{3:>15} |{4:>15} |{5:>15}\".format(\n",
    "        \"pair\", \"cosine\", \"jaccard\", \"overlap\", \"dice\", \"average\")\n",
    "print '-'*117\n",
    "\n",
    "for stripe in sortedSims[:20]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )\n",
    "\n",
    "print '—'*117\n",
    "\n",
    "for stripe in sortedSims[-20:]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7: Vocabulary subset 6001-10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r frequencies5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5\n",
    "!python frequencies5_5.py --min_rank 6001 --max_rank 10000 -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered.txt > frequencies5.5\n",
    "\n",
    "!hdfs dfs -rm -r stripes5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python stripes5_5.py --file=frequencies5.5 -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered.txt > stripes5.5\n",
    "\n",
    "!hdfs dfs -rm -r index5.5\n",
    "!python index5_5.py -r hadoop stripes5.5 > index5.5\n",
    "\n",
    "!hdfs dfs -rm -r similarity5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python similarity5_5.py -r hadoop index5.5 > similarity5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "sortedSims = []\n",
    "with open(\"similarity5.5\",\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "\n",
    "        line = line.strip()\n",
    "        avg,lisst = line.split(\"\\t\")\n",
    "        lisst = json.loads(lisst)\n",
    "        lisst.append(avg)\n",
    "        sortedSims.append(lisst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir sims2\n",
    "!head -1000 similarity5.5 > sims2/top1000sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"\\nTop/Bottom 20 results - Similarity measures - sorted by cosine\"\n",
    "print \"(From the entire data set)\"\n",
    "print '—'*117\n",
    "print \"{0:>30} |{1:>15} |{2:>15} |{3:>15} |{4:>15} |{5:>15}\".format(\n",
    "        \"pair\", \"cosine\", \"jaccard\", \"overlap\", \"dice\", \"average\")\n",
    "print '-'*117\n",
    "\n",
    "for stripe in sortedSims[:20]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )\n",
    "\n",
    "print '—'*117\n",
    "\n",
    "for stripe in sortedSims[-20:]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7: Vocabulary subset 5001-10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r frequencies5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5\n",
    "!python frequencies5_5.py --min_rank 5001 --max_rank 10000 -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered.txt > frequencies5.5\n",
    "\n",
    "!hdfs dfs -rm -r stripes5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python stripes5_5.py --file=frequencies5.5 -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered.txt > stripes5.5\n",
    "\n",
    "!hdfs dfs -rm -r index5.5\n",
    "!python index5_5.py -r hadoop stripes5.5 > index5.5\n",
    "\n",
    "!hdfs dfs -rm -r similarity5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python similarity5_5.py -r hadoop index5.5 > similarity5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "sortedSims = []\n",
    "with open(\"similarity5.5\",\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "\n",
    "        line = line.strip()\n",
    "        avg,lisst = line.split(\"\\t\")\n",
    "        lisst = json.loads(lisst)\n",
    "        lisst.append(avg)\n",
    "        sortedSims.append(lisst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir sims2\n",
    "!head -1000 similarity5.5 > sims2/top1000sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"\\nTop/Bottom 20 results - Similarity measures - sorted by cosine\"\n",
    "print \"(From the entire data set)\"\n",
    "print '—'*117\n",
    "print \"{0:>30} |{1:>15} |{2:>15} |{3:>15} |{4:>15} |{5:>15}\".format(\n",
    "        \"pair\", \"cosine\", \"jaccard\", \"overlap\", \"dice\", \"average\")\n",
    "print '-'*117\n",
    "\n",
    "for stripe in sortedSims[:20]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )\n",
    "\n",
    "print '—'*117\n",
    "\n",
    "for stripe in sortedSims[-20:]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7: Vocabulary subset 4001-10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r frequencies5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5\n",
    "!python frequencies5_5.py --min_rank 4001 --max_rank 10000 -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered.txt > frequencies5.5\n",
    "\n",
    "!hdfs dfs -rm -r stripes5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python stripes5_5.py --file=frequencies5.5 -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered.txt > stripes5.5\n",
    "\n",
    "!hdfs dfs -rm -r index5.5\n",
    "!python index5_5.py -r hadoop stripes5.5 > index5.5\n",
    "\n",
    "!hdfs dfs -rm -r similarity5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python similarity5_5.py -r hadoop index5.5 > similarity5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "sortedSims = []\n",
    "with open(\"similarity5.5\",\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "\n",
    "        line = line.strip()\n",
    "        avg,lisst = line.split(\"\\t\")\n",
    "        lisst = json.loads(lisst)\n",
    "        lisst.append(avg)\n",
    "        sortedSims.append(lisst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir sims2\n",
    "!head -1000 similarity5.5 > sims2/top1000sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"\\nTop/Bottom 20 results - Similarity measures - sorted by cosine\"\n",
    "print \"(From the entire data set)\"\n",
    "print '—'*117\n",
    "print \"{0:>30} |{1:>15} |{2:>15} |{3:>15} |{4:>15} |{5:>15}\".format(\n",
    "        \"pair\", \"cosine\", \"jaccard\", \"overlap\", \"dice\", \"average\")\n",
    "print '-'*117\n",
    "\n",
    "for stripe in sortedSims[:20]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )\n",
    "\n",
    "print '—'*117\n",
    "\n",
    "for stripe in sortedSims[-20:]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7: Vocabulary subset 3001-10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r frequencies5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5\n",
    "!python frequencies5_5.py --min_rank 3001 --max_rank 10000 -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered.txt > frequencies5.5\n",
    "\n",
    "!hdfs dfs -rm -r stripes5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python stripes5_5.py --file=frequencies5.5 -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered.txt > stripes5.5\n",
    "\n",
    "!hdfs dfs -rm -r index5.5\n",
    "!python index5_5.py -r hadoop stripes5.5 > index5.5\n",
    "\n",
    "!hdfs dfs -rm -r similarity5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python similarity5_5.py -r hadoop index5.5 > similarity5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "sortedSims = []\n",
    "with open(\"similarity5.5\",\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "\n",
    "        line = line.strip()\n",
    "        avg,lisst = line.split(\"\\t\")\n",
    "        lisst = json.loads(lisst)\n",
    "        lisst.append(avg)\n",
    "        sortedSims.append(lisst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir sims2\n",
    "!head -1000 similarity5.5 > sims2/top1000sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"\\nTop/Bottom 20 results - Similarity measures - sorted by cosine\"\n",
    "print \"(From the entire data set)\"\n",
    "print '—'*117\n",
    "print \"{0:>30} |{1:>15} |{2:>15} |{3:>15} |{4:>15} |{5:>15}\".format(\n",
    "        \"pair\", \"cosine\", \"jaccard\", \"overlap\", \"dice\", \"average\")\n",
    "print '-'*117\n",
    "\n",
    "for stripe in sortedSims[:20]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )\n",
    "\n",
    "print '—'*117\n",
    "\n",
    "for stripe in sortedSims[-20:]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7: Vocabulary subset 2001-10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r frequencies5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5\n",
    "!python frequencies5_5.py --min_rank 2001 --max_rank 10000 -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered.txt > frequencies5.5\n",
    "\n",
    "!hdfs dfs -rm -r stripes5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python stripes5_5.py --file=frequencies5.5 -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered.txt > stripes5.5\n",
    "\n",
    "!hdfs dfs -rm -r index5.5\n",
    "!python index5_5.py -r hadoop stripes5.5 > index5.5\n",
    "\n",
    "!hdfs dfs -rm -r similarity5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python similarity5_5.py -r hadoop index5.5 > similarity5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "sortedSims = []\n",
    "with open(\"similarity5.5\",\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "\n",
    "        line = line.strip()\n",
    "        avg,lisst = line.split(\"\\t\")\n",
    "        lisst = json.loads(lisst)\n",
    "        lisst.append(avg)\n",
    "        sortedSims.append(lisst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir sims2\n",
    "!head -1000 similarity5.5 > sims2/top1000sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"\\nTop/Bottom 20 results - Similarity measures - sorted by cosine\"\n",
    "print \"(From the entire data set)\"\n",
    "print '—'*117\n",
    "print \"{0:>30} |{1:>15} |{2:>15} |{3:>15} |{4:>15} |{5:>15}\".format(\n",
    "        \"pair\", \"cosine\", \"jaccard\", \"overlap\", \"dice\", \"average\")\n",
    "print '-'*117\n",
    "\n",
    "for stripe in sortedSims[:20]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )\n",
    "\n",
    "print '—'*117\n",
    "\n",
    "for stripe in sortedSims[-20:]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7: Vocabulary subset 1001-10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r frequencies5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5\n",
    "!python frequencies5_5.py --min_rank 1001 --max_rank 10000 -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered.txt > frequencies5.5\n",
    "\n",
    "!hdfs dfs -rm -r stripes5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python stripes5_5.py --file=frequencies5.5 -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered.txt > stripes5.5\n",
    "\n",
    "!hdfs dfs -rm -r index5.5\n",
    "!python index5_5.py -r hadoop stripes5.5 > index5.5\n",
    "\n",
    "!hdfs dfs -rm -r similarity5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python similarity5_5.py -r hadoop index5.5 > similarity5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "sortedSims = []\n",
    "with open(\"similarity5.5\",\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "\n",
    "        line = line.strip()\n",
    "        avg,lisst = line.split(\"\\t\")\n",
    "        lisst = json.loads(lisst)\n",
    "        lisst.append(avg)\n",
    "        sortedSims.append(lisst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir sims2\n",
    "!head -1000 similarity5.5 > sims2/top1000sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"\\nTop/Bottom 20 results - Similarity measures - sorted by cosine\"\n",
    "print \"(From the entire data set)\"\n",
    "print '—'*117\n",
    "print \"{0:>30} |{1:>15} |{2:>15} |{3:>15} |{4:>15} |{5:>15}\".format(\n",
    "        \"pair\", \"cosine\", \"jaccard\", \"overlap\", \"dice\", \"average\")\n",
    "print '-'*117\n",
    "\n",
    "for stripe in sortedSims[:20]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )\n",
    "\n",
    "print '—'*117\n",
    "\n",
    "for stripe in sortedSims[-20:]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.8  <a name=\"5.8\"></a> OPTIONAL: filter stopwords\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "There is also a corpus of stopwords, that is, high-frequency words like \"the\", \"to\" and \"also\" that we sometimes want to filter out of a document before further processing. Stopwords usually have little lexical content, and their presence in a text fails to distinguish it from other texts. Python's nltk comes with a prebuilt list of stopwords (see below). Using this stopword list filter out these tokens from your analysis and rerun the experiments in 5.5 and disucuss the results of using a stopword list and without using a stopword list.\n",
    "\n",
    "> from nltk.corpus import stopwords\n",
    ">> stopwords.words('english')\n",
    "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.9 <a name=\"5.9\"></a> OPTIONAL \n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "There are many good ways to build our synonym detectors, so for this optional homework, \n",
    "measure co-occurrence by (left/right/all) consecutive words only, \n",
    "or make stripes according to word co-occurrences with the accompanying \n",
    "2-, 3-, or 4-grams (note here that your output will no longer \n",
    "be interpretable as a network) inside of the 5-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.10 <a name=\"5.10\"></a> OPTIONAL \n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Once again, benchmark your top 10,000 associations (as in 5.5), this time for your\n",
    "results from 5.6. Has your detector improved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "511px",
    "width": "251px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
